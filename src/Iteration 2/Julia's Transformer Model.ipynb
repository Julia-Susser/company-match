{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0eaf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "secDataPath = \"../../inputs/SEC-CompanyTicker.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba116d",
   "metadata": {},
   "source": [
    "To Do <br>\n",
    "-Make GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e758e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "def sdp_attention(query, key, value):\n",
    "    dim_k = query.size(-1) # dimension component\n",
    "    sfact = sqrt(dim_k)     \n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sfact\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "'''\n",
    "\n",
    "Attention Class\n",
    "\n",
    "# nn.linear : apply linear transformation to incoming data\n",
    "#             y = x * A^T + b\n",
    "# Ax = b where x is input, b is output, A is weight\n",
    "\n",
    "# calculate scaled dot product attention matrix\n",
    "# Requires embedding dimension \n",
    "# Each attention head is made of different q,k,v vectors\n",
    "\n",
    "'''\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # initalisation \n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the three vectors\n",
    "        # input - embed_dim, output - head_dim\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # main class operation\n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # calculate scaled dot product given a \n",
    "        attn_outputs = sdp_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state))\n",
    "        \n",
    "        return attn_outputs\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "Multihead attention class\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class multiHeadAttention(nn.Module):\n",
    "    \n",
    "    # Config during initalisation\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # model params, read from config file\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # attention head (define only w/o hidden state)\n",
    "        # each attention head is initialised with embedd/heads head dimension\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # output uses whole embedding dimension for output\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # Given a hidden state (embeddings)\n",
    "    # Apply operation for multihead attention\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # for each head embed_size/heads, calculate attention\n",
    "        heads = [head(hidden_state) for head in self.heads] \n",
    "        x = torch.cat(heads, dim=-1) # merge/concat head data together\n",
    "    \n",
    "        # apply linear transformation to multihead attension scalar product\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class feedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # define layer operations input x\n",
    "        \n",
    "    def forward(self, x):    # note must be forward\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class encoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = multiHeadAttention(config)    # multihead attention layer \n",
    "        self.feed_forward = feedForward(config)        # feed forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer norm. to hidden state, copy input into query, key, value\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "Token + Position Embedding \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class tpEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding layer\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        \n",
    "        # positional embedding layer\n",
    "        # config.max_position_embeddings -> max number of positions in text 512 (tokens)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1) # number of tokens\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)[None,:] # range(0,9)\n",
    "        \n",
    "        # tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931]])\n",
    "        # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "        \n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Add normalisation & dropout layers\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    \n",
    "# full transformer encoder combining the `Embedding` with the ``Embedding` ` layers\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):       \n",
    "        super().__init__()\n",
    "        \n",
    "        # token & positional embedding layer\n",
    "        self.embeddings = tpEmbedding(config)\n",
    "        \n",
    "        # attention & forward feed layer \n",
    "        self.layers = nn.ModuleList([encoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embeddings layer output\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # cycle through all heads\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = x[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0452422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "import torch\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEmbeddings():\n",
    "    def __init__(self):\n",
    "        self.model = TransformerEncoder(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        \n",
    "    def getTensor(self,text):\n",
    "        inputs = self.tokenizer(text, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False,\n",
    "                          padding=True) # don't use pad, sep tokens\n",
    "        return inputs.input_ids\n",
    "    \n",
    "    def train(self,data,epochs=1):\n",
    "        self.model.train()\n",
    "        tensorData = self.getTensor(data)\n",
    "        for epoch in range(epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"epoch %s\" % epoch)\n",
    "            self.model.forward(tensorData)\n",
    "    \n",
    "    def getEmbeddings(self,q):\n",
    "        self.model.eval()\n",
    "        inputs = self.getTensor(q)\n",
    "        with torch.no_grad():\n",
    "            x = self.model(inputs).cpu().detach().numpy()\n",
    "            return x\n",
    "        \n",
    "\n",
    "data = list(pd.read_csv(secDataPath,index_col=0).companyName[:100])\n",
    "train = TransformerEmbeddings()\n",
    "train.train(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 190136.76it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 1486402.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 10\n",
      "epoch 20\n",
      "epoch 30\n",
      "epoch 40\n",
      "epoch 50\n",
      "epoch 60\n",
      "epoch 70\n",
      "epoch 80\n",
      "epoch 90\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "#word2vec is bad for datasets with only one or two words because it doesnt not look for co occurences between different inputs\n",
    "#Since it only looks at the words its surrounded by in its input, it lacks the same self attention features as other models\n",
    "#Indeed, the model looks at the surrounding words to predict the target word (Skip-gram) or predicts surrounding words given the target word (CBOW).\n",
    "# so if the input is one word, it doesn't really work, much better for longer sentences\n",
    "\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = list(pd.read_csv(secDataPath, index_col=0).companyName[:100]) + [\"shell\"]\n",
    "\n",
    "tqdm.pandas()\n",
    "def preprocessing(titles_array):\n",
    "    processed_array = []\n",
    "    for title in tqdm(titles_array):\n",
    "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
    "        processed = re.sub('[^a-zA-Z ]', '', title)\n",
    "        words = processed.split()\n",
    "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
    "        processed_array.append(' '.join([word for word in words if len(word) > 1]))\n",
    "    return processed_array\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'processed' is a list of strings\n",
    "data = list(pd.read_csv(secDataPath, index_col=0).companyName[:100]) + [\"shell\"]\n",
    "processed = preprocessing(data)\n",
    "\n",
    "# Use tqdm's progress_apply with lambda function\n",
    "data = [sublist[0] for sublist in tqdm(pd.DataFrame({\"companyName\": processed}).apply(lambda x: x.str.split()).values)]\n",
    "\n",
    "\n",
    "\n",
    "class Word2VecEmbeddings():\n",
    "    # Required to train on all data and queries because use keys to find embeddings\n",
    "    def train(self, data, epochs=100):\n",
    "#         # Convert data to a list of lists\n",
    "#         data = [[x] for x in data]\n",
    "        \n",
    "        # Initialize Word2Vec model\n",
    "        self.model = Word2Vec(data, \n",
    "                              min_count=1, \n",
    "                              vector_size=768,\n",
    "                              window=5, \n",
    "                              sg=1)\n",
    "        \n",
    "        # Train the model\n",
    "        for epoch in range(epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"epoch %s\" % epoch)\n",
    "            self.model.train(data, total_examples=self.model.corpus_count, epochs=100)\n",
    "            \n",
    "    def getEmbeddings(self, q):\n",
    "        values = []\n",
    "        maxlength = 0\n",
    "\n",
    "        # Find the maximum length of sequences\n",
    "        for sentence in q:\n",
    "            embedding = []\n",
    "            for val in sentence:\n",
    "                embedding += list(self.model.wv.get_vector(val))\n",
    "            values.append(embedding)\n",
    "            maxlength = max(maxlength, len(embedding))\n",
    "\n",
    "        # Pad each sequence individually\n",
    "        padded_values = pad_sequences(values, maxlen=3840, padding='post', dtype='float32')\n",
    "\n",
    "        return np.array(padded_values)\n",
    "\n",
    "    \n",
    "    def getKeys(self):\n",
    "        words = list(self.model.wv.index_to_key)\n",
    "        return words\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of Word2VecEmbeddings\n",
    "train = Word2VecEmbeddings()\n",
    "\n",
    "# Train the model with the data\n",
    "train.train(data)\n",
    "\n",
    "# Get embeddings for the query and the full data\n",
    "xq = train.getEmbeddings([[\"shell\"]])\n",
    "xb = train.getEmbeddings(data)\n",
    "\n",
    "# Now xq should work as expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db3e1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "key = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "class OpenAIEmbeddings:\n",
    "    def __init__(self,api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "    \n",
    "    def getEmbeddings(self, text_list):\n",
    "        data = self.client.embeddings.create(input=text_list, model='text-embedding-ada-002').data\n",
    "        embeddings = [embedding.embedding for embedding in data]\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    \n",
    "train = OpenAIEmbeddings(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d26f1c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.67052944e-02, -2.26962656e-01, -3.52288604e-01,\n",
       "         1.39271274e-01,  3.63313109e-02, -2.48244584e-01,\n",
       "        -7.24242255e-03,  2.84364730e-01, -2.24267125e-01,\n",
       "         2.74040073e-01,  6.68052286e-02, -2.88262486e-01,\n",
       "        -2.35641703e-01,  3.63490671e-01, -6.73132315e-02,\n",
       "        -1.02233335e-01,  1.31339893e-01,  2.46826887e-01,\n",
       "         2.06331387e-01, -1.72131449e-01, -1.58596531e-01,\n",
       "         2.81898826e-01, -7.99801722e-02,  2.85342634e-01,\n",
       "        -2.63511576e-02, -2.79757380e-01,  2.15131685e-01,\n",
       "         4.59036194e-02, -9.49969664e-02,  9.71716344e-02,\n",
       "        -4.19612303e-02, -1.43848658e-02, -8.02190006e-02,\n",
       "        -7.30765983e-02, -1.07166685e-04, -1.66405842e-01,\n",
       "         1.28892303e-01,  5.45171127e-02, -1.84124723e-01,\n",
       "        -1.19634345e-01, -7.17815757e-02, -2.55356371e-01,\n",
       "        -3.24021101e-01, -3.18603784e-01, -2.07002670e-01,\n",
       "        -2.33787283e-01,  1.15620144e-01, -2.81645685e-01,\n",
       "        -2.65673101e-02, -1.69645041e-01,  2.72051960e-01,\n",
       "         4.92678061e-02, -1.36198476e-02, -4.67848741e-02,\n",
       "         1.98244274e-01, -6.20089471e-03, -7.20805824e-02,\n",
       "        -2.05151394e-01,  1.21839486e-01, -8.10572729e-02,\n",
       "         6.99825346e-01,  2.77802080e-01,  4.06696722e-02,\n",
       "        -4.35135067e-01, -1.35701537e-01, -1.50714302e-02,\n",
       "         5.42821825e-01, -1.91447049e-01,  8.71475786e-02,\n",
       "         1.56485051e-01,  3.27457666e-01,  7.49490708e-02,\n",
       "         8.01794082e-02,  2.73294628e-01, -1.06390826e-01,\n",
       "         2.93283254e-01, -2.31957227e-01,  8.95097181e-02,\n",
       "        -1.34768011e-02,  2.70025227e-02,  1.67484239e-01,\n",
       "        -1.49717897e-01, -1.03473119e-01,  1.78339630e-01,\n",
       "         1.34364769e-01,  1.76614076e-01,  3.96789350e-02,\n",
       "         1.14231519e-02, -8.74148011e-02, -2.22473070e-01,\n",
       "        -7.51830280e-01, -4.14764285e-02, -2.40708321e-01,\n",
       "        -1.59915894e-01,  2.31549844e-01, -1.02040485e-01,\n",
       "        -1.00931294e-01,  1.09458119e-01, -2.04621404e-01,\n",
       "        -3.73768866e-01,  3.34479481e-01,  1.00472473e-01,\n",
       "         1.11694135e-01,  2.13551596e-02,  1.65466771e-01,\n",
       "        -2.21411064e-02,  1.16983816e-01, -9.83885229e-02,\n",
       "         7.35410973e-02, -1.14277534e-01, -2.95458704e-01,\n",
       "        -1.11259021e-01,  2.58083522e-01, -1.40133798e-01,\n",
       "         4.49746214e-02,  9.54620391e-02,  1.89113095e-01,\n",
       "         6.74087927e-03, -5.97697832e-02,  5.75506277e-02,\n",
       "        -6.37316585e-01,  3.75245884e-02,  2.36922234e-01,\n",
       "         2.18960375e-01, -4.66642469e-01,  1.60327524e-01,\n",
       "        -2.08496243e-01,  7.91451186e-02,  3.79963517e-02,\n",
       "        -1.62997782e-01, -3.84413600e-01,  1.37426630e-01,\n",
       "        -2.55297095e-01, -1.69554174e-01, -1.88664705e-01,\n",
       "        -5.25342114e-02,  2.27767043e-02, -1.42230168e-01,\n",
       "         1.48461163e-01, -1.86383009e-01,  1.75510928e-01,\n",
       "        -2.00269610e-01,  1.99716777e-01, -1.73727006e-01,\n",
       "        -1.37014017e-01,  8.72480124e-03,  1.86500493e-02,\n",
       "        -3.14480841e-01, -8.68059099e-02,  1.10198207e-01,\n",
       "         2.90966541e-01, -4.84865993e-01,  1.89643294e-01,\n",
       "         2.21101865e-01,  3.07532012e-01, -1.85968235e-01,\n",
       "         7.67785907e-02,  3.33477199e-01, -1.41580522e-01,\n",
       "        -8.95908549e-02, -8.37761238e-02,  7.34572411e-02,\n",
       "         1.47042245e-01, -3.03110093e-01,  3.94960940e-02,\n",
       "        -1.00639328e-01,  1.93896703e-02, -3.68619114e-02,\n",
       "        -2.66592465e-02, -1.66032575e-02,  4.40118611e-01,\n",
       "         2.41677955e-01,  1.01877883e-01,  7.91396052e-02,\n",
       "        -5.18022627e-02,  1.61497518e-01,  3.52659345e-01,\n",
       "        -3.40803087e-01,  1.80374831e-01, -6.00973628e-02,\n",
       "         2.45157734e-01, -9.15476605e-02, -1.44787490e-01,\n",
       "         1.17740631e-01,  4.93402213e-01, -3.05552576e-02,\n",
       "        -4.85131413e-01,  3.44934702e-01,  7.17239231e-02,\n",
       "        -7.23890290e-02,  1.45415254e-02,  1.36146232e-01,\n",
       "        -1.30361065e-01, -1.93914361e-02, -6.37624115e-02,\n",
       "         1.57722682e-01,  1.62305042e-01, -1.90456793e-01,\n",
       "         1.37071267e-01, -2.23016530e-01,  2.69431323e-01,\n",
       "         1.11218214e-01,  3.34793299e-01,  5.19510567e-01,\n",
       "         9.29496586e-02,  1.30683303e-01,  1.83144629e-01,\n",
       "        -2.13431746e-01, -1.22481585e-03,  1.22108988e-01,\n",
       "         8.20283517e-02, -1.56741381e-01,  1.31017417e-02,\n",
       "         5.91183230e-02,  5.46666503e-01,  1.51498988e-03,\n",
       "         2.88280964e-01,  1.19308703e-01, -2.65083522e-01,\n",
       "        -3.01424414e-04, -2.85406440e-01,  4.44024593e-01,\n",
       "         1.52272373e-01, -1.39788732e-01,  2.22930267e-01,\n",
       "         1.33886728e-02,  1.24151736e-01, -1.80119619e-01,\n",
       "        -1.22900270e-01,  3.21016252e-01,  3.44847083e-01,\n",
       "         1.29181147e-01,  2.07503200e-01, -9.86359790e-02,\n",
       "        -1.79033205e-01, -1.86438337e-02,  1.21018581e-01,\n",
       "         1.22235566e-02, -1.05224289e-01, -2.55295753e-01,\n",
       "         3.10072392e-01,  2.96794415e-01,  6.06839359e-02,\n",
       "         5.00157475e-04,  3.58001351e-01,  1.52173176e-01,\n",
       "         2.22199008e-01, -1.09937273e-01, -2.10983083e-02,\n",
       "         1.11186482e-01, -2.26863831e-01, -1.08423762e-01,\n",
       "        -1.85111657e-01, -3.28872114e-01,  1.33008510e-01,\n",
       "        -5.16240418e-01, -3.27274948e-02, -4.71812904e-01,\n",
       "        -4.68140393e-01,  3.11511457e-02,  9.71194431e-02,\n",
       "         1.58131905e-02,  2.81085670e-01,  9.19424593e-02,\n",
       "        -1.63103640e-02,  7.17285722e-02, -3.03778619e-01,\n",
       "        -2.43421644e-02, -4.60295498e-01,  1.94195956e-01,\n",
       "         1.33495688e-01, -6.28722906e-02,  2.28221461e-01,\n",
       "         6.02511838e-02,  3.38503331e-01, -2.38713428e-01,\n",
       "        -1.53827593e-01, -2.38669395e-01, -2.40121827e-01,\n",
       "        -2.43243471e-01, -2.10869044e-01, -2.96773970e-01,\n",
       "        -1.51509792e-01, -4.20028940e-02,  7.30652362e-02,\n",
       "         2.71065161e-04,  1.18145362e-01, -4.84966636e-02,\n",
       "         4.11136359e-01, -5.38925231e-01,  1.77191764e-01,\n",
       "         8.53120461e-02,  4.01937887e-02,  4.66394186e-01,\n",
       "        -2.64127940e-01,  1.61939338e-02,  1.55607015e-01,\n",
       "         5.26889265e-01, -2.87395895e-01, -8.28470103e-03,\n",
       "        -1.13464102e-01, -2.04167038e-01, -2.92278022e-01,\n",
       "         2.30567783e-01, -3.01172674e-01,  1.96092755e-01,\n",
       "        -2.70721763e-01,  2.56885648e-01,  1.85332641e-01,\n",
       "        -2.44825512e-01, -4.19600546e-01, -6.74439073e-02,\n",
       "        -2.27268413e-03, -1.77080482e-01, -7.24389404e-02,\n",
       "         1.19108498e-01, -1.71659693e-01, -7.09671155e-02,\n",
       "        -4.63807173e-02, -2.22905159e-01, -1.17861979e-01,\n",
       "        -1.39193330e-02, -4.13962044e-02, -2.44788945e-01,\n",
       "         3.17875333e-02, -1.97824240e-01, -3.47608000e-01,\n",
       "        -2.01711729e-01,  3.52064252e-01, -5.86014315e-02,\n",
       "         4.30492088e-02, -4.60065603e-02,  6.18813224e-02,\n",
       "        -1.57391131e-01,  1.91762790e-01,  1.84721902e-01,\n",
       "         2.12987959e-02, -7.03232884e-02,  2.15195015e-01,\n",
       "        -8.35271850e-02,  4.06301506e-02, -1.06210619e-01,\n",
       "        -2.83128798e-01, -1.32000163e-01, -6.08419999e-02,\n",
       "        -2.13150114e-01,  5.04592776e-01,  5.29831126e-02,\n",
       "         1.67710274e-01, -5.66192493e-02, -6.51037470e-02,\n",
       "         2.96081364e-01,  2.08033666e-01, -2.58169830e-01,\n",
       "        -1.88771933e-01, -2.20615909e-01,  3.71897779e-02,\n",
       "         2.90124238e-01, -8.11643228e-02, -2.30321884e-01,\n",
       "         3.02018046e-01, -2.33338382e-02, -2.73071766e-01,\n",
       "         1.93860933e-01,  3.76983821e-01,  5.16306996e-01,\n",
       "         3.91358763e-01,  1.52620107e-01,  2.53022164e-01,\n",
       "        -9.78853181e-02, -1.04188032e-01,  6.47482127e-02,\n",
       "        -2.75230587e-01,  1.59282953e-01,  1.64575964e-01,\n",
       "        -5.51896334e-01, -4.57597189e-02, -3.00117016e-01,\n",
       "        -1.15697227e-01,  4.91119586e-02,  8.85215029e-02,\n",
       "        -4.06496823e-02,  4.12716717e-01, -3.79074402e-02,\n",
       "        -2.51502782e-01, -3.23980212e-01, -1.66310906e-01,\n",
       "        -8.38590860e-02,  3.65416557e-02,  1.99315310e-01,\n",
       "        -2.62204558e-02, -3.05942893e-01,  6.21834874e-01,\n",
       "        -2.64938921e-01,  5.36368340e-02,  8.84730667e-02,\n",
       "         6.36535212e-02, -9.26130861e-02, -3.18204284e-01,\n",
       "         3.68412323e-02, -2.57697523e-01,  9.53866839e-01,\n",
       "        -2.46144965e-01,  1.01828270e-01, -2.27812544e-01,\n",
       "        -2.49327213e-01, -1.48103759e-01, -2.59450134e-02,\n",
       "        -1.84820011e-01, -5.12010371e-03,  2.37809286e-01,\n",
       "         7.57219613e-01,  1.17209330e-02,  2.06590444e-01,\n",
       "         7.97266811e-02, -4.93284136e-01, -8.96546394e-02,\n",
       "        -7.66374618e-02, -1.12704761e-01,  7.83435553e-02,\n",
       "         6.16844520e-02,  2.37822562e-01,  8.58181119e-02,\n",
       "         2.69005030e-01,  3.68880391e-01, -8.74714106e-02,\n",
       "        -2.52613574e-01, -9.45166424e-02,  1.49754152e-01,\n",
       "         1.02431789e-01,  1.54147804e-01, -9.28212851e-02,\n",
       "         1.09129176e-01,  7.90148228e-03, -9.82105136e-02,\n",
       "        -3.75313073e-01,  2.00398445e-01,  2.79151857e-01,\n",
       "        -3.65601182e-01, -2.61525422e-01,  2.94829845e-01,\n",
       "         4.81780350e-01,  1.50454864e-01,  1.26477122e-01,\n",
       "         4.30101156e-03,  4.00943905e-02,  2.74219096e-01,\n",
       "        -1.07175991e-01, -4.59849164e-02,  2.91197300e-01,\n",
       "         1.45607904e-01, -3.14987183e-01, -8.99043679e-02,\n",
       "        -6.85250536e-02,  8.47895294e-02, -2.17782781e-01,\n",
       "         5.38590662e-02,  1.41896546e-01, -7.07250163e-02,\n",
       "         1.30216599e-01,  1.95284128e-01,  1.24321318e+00,\n",
       "         2.07477987e-01,  1.01507291e-01, -9.80115905e-02,\n",
       "        -6.79299176e-01, -1.73818856e-01, -7.84694374e-01,\n",
       "         1.82528913e-01, -1.78337380e-01, -4.04718101e-01,\n",
       "        -2.09831484e-02, -1.46438032e-01,  4.11359333e-02,\n",
       "         1.93520799e-01, -2.04519942e-01, -2.76050776e-01,\n",
       "        -2.64150083e-01,  2.62919277e-01,  3.09049878e-02,\n",
       "        -3.33927386e-02,  2.28185326e-01,  2.15992332e-02,\n",
       "         2.56612599e-01,  2.59681404e-01,  1.44230053e-02,\n",
       "        -1.30791143e-02, -1.63196415e-01, -1.05642505e-01,\n",
       "        -6.69914559e-02, -8.26000497e-02, -9.01542231e-02,\n",
       "         3.84536922e-01,  2.93598175e-01, -4.10283729e-02,\n",
       "         1.68878008e-02, -4.36296612e-02, -2.19345823e-01,\n",
       "         3.03515434e-01,  5.86757898e-01,  1.61654428e-01,\n",
       "         1.37319133e-01,  4.38389182e-02,  1.20895028e-01,\n",
       "         2.06671171e-02, -1.54680610e-01,  1.15646899e-01,\n",
       "         4.36759144e-02, -3.50254864e-01, -5.03895938e-01,\n",
       "        -3.65658626e-02,  1.27433106e-01, -4.28537428e-01,\n",
       "        -7.23323345e-01,  1.29155964e-02, -5.55226147e-01,\n",
       "         7.78005421e-02, -6.38609380e-02,  5.79718113e-01,\n",
       "        -6.84905574e-02, -3.48265916e-02,  1.80385232e-01,\n",
       "        -6.46893382e-02, -2.56143302e-01,  3.11605483e-01,\n",
       "         1.05822459e-01, -1.44985199e-01, -2.80678004e-01,\n",
       "         1.29839912e-01,  1.26221240e-01,  4.24672604e-01,\n",
       "         5.94766662e-02, -1.25173673e-01, -2.29692787e-01,\n",
       "        -2.36086488e-01, -2.85015702e-01,  3.42973292e-01,\n",
       "        -1.00967735e-02,  4.21140283e-01, -3.73286575e-01,\n",
       "        -3.06515127e-01, -8.44099894e-02, -1.30670667e-01,\n",
       "        -3.60587388e-02,  3.57556455e-02, -1.29920959e-01,\n",
       "        -2.28186563e-01, -1.79874338e-02,  2.62972772e-01,\n",
       "         6.85954541e-02, -4.40257974e-02, -2.00894833e-01,\n",
       "         1.22594729e-01, -1.59435630e-01,  1.85860649e-01,\n",
       "        -4.14132833e-01,  5.32592386e-02,  3.05239469e-01,\n",
       "         1.00044951e-01,  1.13783292e-01, -3.66840959e-01,\n",
       "         2.20355958e-01,  1.69479340e-01,  2.76633352e-01,\n",
       "         3.79004590e-02,  5.30984923e-02, -3.35347712e-01,\n",
       "        -3.11886631e-02,  6.76513016e-02,  3.85256484e-04,\n",
       "        -1.37095734e-01, -2.23088071e-01, -1.46511197e-01,\n",
       "         3.12624872e-03,  1.23822587e-02, -6.82663918e-02,\n",
       "         2.32585043e-01,  1.82694376e-01,  4.92096134e-02,\n",
       "         1.52679890e-01,  2.18364656e-01,  1.75983980e-02,\n",
       "        -1.41712740e-01,  4.43799525e-01,  6.43835738e-02,\n",
       "        -1.79957420e-01, -1.57113820e-01,  1.69143453e-01,\n",
       "        -1.67379335e-01, -5.44760108e-01, -6.04146197e-02,\n",
       "         5.43774188e-01,  3.39783460e-01, -3.93766686e-02,\n",
       "         1.34145558e-01, -1.10354841e-01, -1.76539108e-01,\n",
       "         2.26430371e-02, -7.91048259e-03,  2.16563627e-01,\n",
       "        -7.27272555e-02,  4.18852597e-01,  2.85397530e-01,\n",
       "        -6.65765256e-02, -2.68435240e-01, -9.24579352e-02,\n",
       "        -1.52573362e-01,  2.43106820e-02,  1.01742573e-01,\n",
       "        -2.84128308e-01,  2.59357750e-01,  7.28507340e-03,\n",
       "        -7.92907625e-02, -6.12357371e-02,  1.05788775e-01,\n",
       "         1.91291958e-01, -1.78998336e-01, -1.36444457e-02,\n",
       "        -7.16195107e-02,  1.93376020e-01,  3.22604656e-01,\n",
       "         4.37910780e-02,  6.30831063e-01,  2.79437631e-01,\n",
       "        -8.32119957e-02,  8.62570882e-01,  2.01182663e-01,\n",
       "         2.48442888e-01,  3.50527585e-01, -6.21538125e-02,\n",
       "         3.65265831e-02,  2.52969444e-01,  1.58022285e-01,\n",
       "         1.27721459e-01, -4.42414284e-01,  2.55726904e-01,\n",
       "        -3.41232896e-01, -1.36830434e-01, -1.90912500e-01,\n",
       "        -2.26111531e-01, -1.38653830e-01, -1.70491412e-01,\n",
       "         9.59496200e-03, -3.98271680e-02, -3.82896774e-02,\n",
       "        -9.48265046e-02, -1.40654877e-01, -4.36626881e-01,\n",
       "         1.35407805e-01,  1.50953650e-01,  1.11770913e-01,\n",
       "        -1.30818605e-01,  1.40862718e-01,  1.77594334e-01,\n",
       "         9.83307511e-02,  1.77152112e-01,  2.53465503e-01,\n",
       "         2.86977768e-01, -3.25608253e-01,  1.49745494e-01,\n",
       "         1.84291437e-01,  1.26229510e-01, -3.57276559e-01,\n",
       "         6.90062940e-02,  4.87969890e-02, -4.10381049e-01,\n",
       "        -4.41157073e-02,  1.07965395e-01,  5.10367632e-01,\n",
       "         2.43674949e-01, -3.16091061e-01,  2.04064444e-01,\n",
       "        -2.80220240e-01,  6.62050545e-01,  2.30600104e-01,\n",
       "         2.95521826e-01,  9.89766717e-02,  1.53292581e-01,\n",
       "         2.02029869e-01,  9.64686200e-02, -6.36239275e-02,\n",
       "        -9.24861506e-02, -1.34532452e-01,  3.44157726e-01,\n",
       "         4.00496423e-01,  3.19236191e-04, -1.03460990e-01,\n",
       "         3.36944044e-01,  1.28614381e-01,  2.29059294e-01,\n",
       "         4.00742516e-02,  7.41420910e-02,  4.20942307e-02,\n",
       "        -2.55442038e-02, -8.53251591e-02,  2.98758000e-01,\n",
       "         3.01829606e-01, -1.08162344e-01, -1.31807014e-01,\n",
       "        -2.14518577e-01, -6.83629736e-02,  2.17472106e-01,\n",
       "        -2.10005008e-02,  1.68693498e-01,  5.12310443e-03,\n",
       "        -2.00728670e-01,  4.28106561e-02, -1.36223137e-01,\n",
       "         9.94950682e-02,  3.87358785e-01,  3.54161173e-01,\n",
       "         2.90805846e-01,  9.90012661e-03,  1.01185665e-01,\n",
       "        -3.03487368e-02,  2.69663125e-01, -8.48868676e-03,\n",
       "        -2.63947546e-01, -2.73438305e-01, -7.39452392e-02,\n",
       "         2.98937440e-01, -1.11994669e-01,  4.53439169e-02,\n",
       "        -3.43015850e-01, -1.17619224e-01,  1.50781497e-01,\n",
       "        -9.11590159e-02,  2.41323840e-03,  1.05335779e-01,\n",
       "         1.17117211e-01,  3.51360619e-01, -2.54779845e-01,\n",
       "         1.05026454e-01,  2.42280379e-01, -1.44505605e-01,\n",
       "         2.43585065e-01,  4.54998851e-01,  2.07335219e-01,\n",
       "        -8.33911672e-02,  1.79265231e-01, -2.30156362e-01,\n",
       "        -2.48648629e-01,  5.34767866e-01, -1.03815049e-01,\n",
       "        -1.14728250e-01, -2.88390040e-01, -1.84428513e-01,\n",
       "         4.28340852e-01,  2.07193382e-02, -2.84689993e-01,\n",
       "        -2.15675548e-01,  1.40996724e-01,  4.89010662e-01,\n",
       "         1.44746408e-01,  1.41405016e-01,  1.89004123e-01,\n",
       "         1.13518827e-01, -2.86039114e-01, -1.00577146e-01,\n",
       "         2.66672730e-01,  2.25654900e-01, -3.68733928e-02,\n",
       "         1.66075304e-03, -3.10998410e-01,  3.07555705e-01,\n",
       "        -8.62251520e-02, -8.99316818e-02, -2.83052981e-01]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BERTEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        self.model = AutoModel.from_pretrained(model_ckpt)\n",
    "        \n",
    "    #CLS is a special classification token and the last hidden state of BERT Embedding\n",
    "    def cls_pooling(self, model_output):\n",
    "        return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "    #BERT tokenizer of input text\n",
    "    def getEmbeddings(self, text_list):\n",
    "        encoded_input = self.tokenizer(\n",
    "            text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        model_output = self.model(**encoded_input)\n",
    "        return self.cls_pooling(model_output).cpu().detach().numpy()\n",
    "    \n",
    "train = BERTEmbeddings()\n",
    "train.getEmbeddings([\"julia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f3bd06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell Plc\n",
      "Lvmh Moet Hennessy Louis Vuitton\n",
      "Danaher Corp /De/\n",
      "Anheuser-Busch Inbev Sa/Nv\n",
      "Spdr S&P 500 Etf Trust\n",
      "Qualcomm Inc/De\n",
      "Invesco Qqq Trust, Series 1\n",
      "T-Mobile Us, Inc.\n",
      "Wells Fargo & Company/Mn\n",
      "Merck & Co., Inc.\n",
      "Shell Plc\n",
      "Lvmh Moet Hennessy Louis Vuitton \n",
      "Danaher Corp /De/\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "    \n",
    "class Faiss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def faiss(self,xb):\n",
    "        d = xb[0].size\n",
    "        M = 32\n",
    "        index = faiss.IndexHNSWFlat(d, M)            \n",
    "        index.hnsw.efConstruction = 40         # Setting the value for efConstruction.\n",
    "        index.hnsw.efSearch = 16               # Setting the value for efSearch.\n",
    "        index.add(xb)\n",
    "        return index\n",
    "    \n",
    "    def query(self,index,xq,k=10):\n",
    "        D, I = index.search(xq, k)   \n",
    "        return D, I\n",
    "\n",
    "model = \"Transformer\"\n",
    "#if using Word2Vec\n",
    "if model==\"Word2Vec\":\n",
    "    q = [['shell']]\n",
    "    #Word2Vec needs prepocessing of data to transform sentences into individual tokens\n",
    "else:\n",
    "    q = [\"shell\"]\n",
    "    data = list(pd.read_csv(secDataPath,index_col=0).companyName[:100])\n",
    "xq = train.getEmbeddings(q)\n",
    "xb = train.getEmbeddings(data)\n",
    "index = Faiss().faiss(xb)\n",
    "D,I = Faiss().query(index,xq)\n",
    "I = I[0]\n",
    "\n",
    "for guess in I:\n",
    "    print(data[guess])\n",
    "    \n",
    "if model != \"Word2Vec\":\n",
    "    print(data[I[0]] + \"\\n\" + data[I[1]], \"\\n\" + data[I[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Shell Plc',\n",
       " 'Unilever Plc',\n",
       " 'Linde Plc',\n",
       " 'Accenture Plc',\n",
       " 'Hsbc Holdings Plc',\n",
       " 'Astrazeneca Plc',\n",
       " 'Bhp Group Ltd',\n",
       " 'Oracle Corp',\n",
       " 'Alibaba Group Holding Ltd',\n",
       " 'Mcdonalds Corp']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "key = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "class runDataset():\n",
    "    def __init__(self,model):\n",
    "        models = [\"Transformer\", \"BERT\", \"OpenAI\"]\n",
    "        self.model = model\n",
    "\n",
    "        if (\"data\" not in model.keys()):\n",
    "            raise Exception(\"No data inputted.\")\n",
    "        #not using word2vec because it performed terribly on datasets with only one or two words per code-word\n",
    "        if (model[\"name\"] == \"Transformer\"):\n",
    "            self.Embeddings=TransformerEmbeddings()\n",
    "            self.Embeddings.train(model[\"data\"])\n",
    "        elif (model[\"name\"] == \"BERT\"):\n",
    "            self.Embeddings=BERTEmbeddings()\n",
    "        elif (model[\"name\"] == \"OpenAI\"):\n",
    "            if (\"api_key\" not in model.keys()):\n",
    "                raise Exception(\"No API Key inputted.\")\n",
    "            self.Embeddings = OpenAIEmbeddings(model[\"api_key\"])\n",
    "        else:\n",
    "            raise Exception(\"Invalid Model\")\n",
    "        \n",
    "        xb = self.Embeddings.getEmbeddings(model[\"data\"])\n",
    "        self.index = Faiss().faiss(xb)\n",
    "\n",
    "    def getEmbeddings(self,x):\n",
    "        return self.Embeddings.getEmbeddings(x)\n",
    "    \n",
    "    def similaritySearch(self,q):\n",
    "        xq = self.getEmbeddings(q)\n",
    "        D,I = Faiss().query(self.index,xq)\n",
    "        I = I[0]\n",
    "        guesses = [data[guess] for guess in I]\n",
    "        return guesses\n",
    "\n",
    "data = list(pd.read_csv(secDataPath,index_col=0).companyName[:100])\n",
    "\n",
    "model = {\n",
    "    \"name\" : \"OpenAI\",\n",
    "    \"data\" : data,\n",
    "    \"api_key\":key\n",
    "}\n",
    "similaritySearch = runDataset(model)\n",
    "similaritySearch.similaritySearch([\"Shell Plc\"])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
