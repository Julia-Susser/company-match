{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9953a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "epoch 40\n",
      "epoch 41\n",
      "epoch 42\n",
      "epoch 43\n",
      "epoch 44\n",
      "epoch 45\n",
      "epoch 46\n",
      "epoch 47\n",
      "epoch 48\n",
      "epoch 49\n",
      "epoch 50\n",
      "epoch 51\n",
      "epoch 52\n",
      "epoch 53\n",
      "epoch 54\n",
      "epoch 55\n",
      "epoch 56\n",
      "epoch 57\n",
      "epoch 58\n",
      "epoch 59\n",
      "epoch 60\n",
      "epoch 61\n",
      "epoch 62\n",
      "epoch 63\n",
      "epoch 64\n",
      "epoch 65\n",
      "epoch 66\n",
      "epoch 67\n",
      "epoch 68\n",
      "epoch 69\n",
      "epoch 70\n",
      "epoch 71\n",
      "epoch 72\n",
      "epoch 73\n",
      "epoch 74\n",
      "epoch 75\n",
      "epoch 76\n",
      "epoch 77\n",
      "epoch 78\n",
      "epoch 79\n",
      "epoch 80\n",
      "epoch 81\n",
      "epoch 82\n",
      "epoch 83\n",
      "epoch 84\n",
      "epoch 85\n",
      "epoch 86\n",
      "epoch 87\n",
      "epoch 88\n",
      "epoch 89\n",
      "epoch 90\n",
      "epoch 91\n",
      "epoch 92\n",
      "epoch 93\n",
      "epoch 94\n",
      "epoch 95\n",
      "epoch 96\n",
      "epoch 97\n",
      "epoch 98\n",
      "epoch 99\n",
      "epoch 100\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer \n",
    "import torch.nn.functional as F \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "def sdp_attention(query, key, value):\n",
    "    dim_k = query.size(-1) # dimension component\n",
    "    sfact = sqrt(dim_k)     \n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sfact\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "'''\n",
    "\n",
    "Attention Class\n",
    "\n",
    "# nn.linear : apply linear transformation to incoming data\n",
    "#             y = x * A^T + b\n",
    "# Ax = b where x is input, b is output, A is weight\n",
    "\n",
    "# calculate scaled dot product attention matrix\n",
    "# Requires embedding dimension \n",
    "# Each attention head is made of different q,k,v vectors\n",
    "\n",
    "'''\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # initalisation \n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the three vectors\n",
    "        # input - embed_dim, output - head_dim\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # main class operation\n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # calculate scaled dot product given a \n",
    "        attn_outputs = sdp_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state))\n",
    "        \n",
    "        return attn_outputs\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "Multihead attention class\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class multiHeadAttention(nn.Module):\n",
    "    \n",
    "    # Config during initalisation\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # model params, read from config file\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # attention head (define only w/o hidden state)\n",
    "        # each attention head is initialised with embedd/heads head dimension\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # output uses whole embedding dimension for output\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # Given a hidden state (embeddings)\n",
    "    # Apply operation for multihead attention\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # for each head embed_size/heads, calculate attention\n",
    "        heads = [head(hidden_state) for head in self.heads] \n",
    "        x = torch.cat(heads, dim=-1) # merge/concat head data together\n",
    "    \n",
    "        # apply linear transformation to multihead attension scalar product\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class feedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # define layer operations input x\n",
    "        \n",
    "    def forward(self, x):    # note must be forward\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class encoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = multiHeadAttention(config)    # multihead attention layer \n",
    "        self.feed_forward = feedForward(config)        # feed forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer norm. to hidden state, copy input into query, key, value\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "Token + Position Embedding \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class tpEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding layer\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        \n",
    "        # positional embedding layer\n",
    "        # config.max_position_embeddings -> max number of positions in text 512 (tokens)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1) # number of tokens\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)[None,:] # range(0,9)\n",
    "        \n",
    "        # tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931]])\n",
    "        # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "        \n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Add normalisation & dropout layers\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    \n",
    "# full transformer encoder combining the `Embedding` with the ``Embedding` ` layers\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):       \n",
    "        super().__init__()\n",
    "        \n",
    "        # token & positional embedding layer\n",
    "        self.embeddings = tpEmbedding(config)\n",
    "        \n",
    "        # attention & forward feed layer \n",
    "        self.layers = nn.ModuleList([encoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embeddings layer output\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # cycle through all heads\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = x[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEmbeddings():\n",
    "    def __init__(self):\n",
    "        self.model = TransformerEncoder(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        \n",
    "    def getTensor(self,text):\n",
    "        inputs = self.tokenizer(text, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False,\n",
    "                          padding=True) # don't use pad, sep tokens\n",
    "        return inputs.input_ids\n",
    "    \n",
    "    def train(self,data,epochs=100):\n",
    "        self.model.train()\n",
    "        tensorData = self.getTensor(data)\n",
    "        batch_size = 10\n",
    "        data_loader = DataLoader(tensorData, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"epoch %s\" % (epoch+1))\n",
    "                for batch in data_loader:\n",
    "                    self.model.forward(batch)\n",
    "    \n",
    "    def getEmbeddings(self,qs):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            #CHANGE THIS\n",
    "            values = []\n",
    "            for q in qs:\n",
    "                input = self.getTensor(q)\n",
    "                x = self.model(input).cpu().detach().numpy()\n",
    "                values.append(x[0])\n",
    "            values = np.array(values)\n",
    "        return values\n",
    "        \n",
    "# secDataPath = \"../../inputs/SEC/SEC-CompanyTicker.csv\"\n",
    "# data = list(pd.read_csv(secDataPath,index_col=0).companyName[:400])\n",
    "\n",
    "quoraDataPath = \"../../inputs/quora/train.csv\"\n",
    "quoraData = pd.read_csv(quoraDataPath)\n",
    "quoraData = quoraData[quoraData['is_duplicate'] == 1]\n",
    "data = list(quoraData.question1[:400])\n",
    "\n",
    "train = TransformerEmbeddings()\n",
    "train.train(data)\n",
    "xb = train.getEmbeddings(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dcbd256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What should I do when my friends betray me?',\n",
       " 'What can I do to get a job at Microsoft?',\n",
       " 'What should someone do to overcome anxiety?',\n",
       " 'What would be a cool way to commit suicide?',\n",
       " 'What is a good song to lyric prank your best friend?',\n",
       " 'What would be the best way to quit smoking?',\n",
       " 'What are the best things to do on Halloween?',\n",
       " 'What can I eat every day to be more healthy?',\n",
       " 'What makes a great cup of coffee great?',\n",
       " 'What do you think about the Bermuda Triangle?',\n",
       " 'What is the funniest joke you know?',\n",
       " 'What can be done to reduce the pollution of India?',\n",
       " 'What’s the best time to have sex?',\n",
       " 'What are some interesting things to do when bored?',\n",
       " 'What is the best evidence for a historical Jesus?',\n",
       " 'What are the easy ways to earn money online?',\n",
       " 'What Game of Thrones villain would be the most likely to give you mercy?',\n",
       " 'What is the best way to prepare to IELTS?',\n",
       " 'What is a narcissistic personality disorder?',\n",
       " 'What can I do to improve my question on Quora?',\n",
       " 'What stocks are the best to invest in right now?',\n",
       " 'What is the easiest way to become a billionaire($)?',\n",
       " 'What are some of the best music albums ever?',\n",
       " 'What is the best way to get traffic on your website?',\n",
       " 'What is the best way to learn algebra by yourself?',\n",
       " 'What is the best age to teach a child how to swim?',\n",
       " 'What should I do if someone has posted a porn video under my name?',\n",
       " 'What is the first moment you remember in your Life?',\n",
       " 'What is the ultimate way to serve humanity?',\n",
       " 'What are the best available smartphones gadgets?',\n",
       " 'What are the minimum requirements to enter MIT?',\n",
       " 'What is reactance in a capacitor?',\n",
       " 'What can I do after completing BDS?',\n",
       " 'What do you think about the movie, \"Interstellar\"?',\n",
       " 'What is it like to live in Cologne?',\n",
       " 'What job possibilities exist for a Bachelors in Homeland Security?',\n",
       " 'What are some of the best romantic movies in English?',\n",
       " 'What are Mutual funds? How do they work?',\n",
       " 'What would Hillary Clinton do now that the election is over?',\n",
       " 'What is similar to 4Shared?',\n",
       " 'What makes you believe that \"Everything happens for a good reason\"?',\n",
       " \"What's it like to be a non-smoking mother of smoking children?\",\n",
       " 'What are the most interesting foods you have eaten in any country?',\n",
       " 'What are some tips for someone trying to teach themselves to sing?',\n",
       " 'What is the meaning and purpose to life?',\n",
       " 'What are the qualities of a good leader?',\n",
       " 'What is it like being a firefighter?',\n",
       " 'What is the best phone to buy below 15k?',\n",
       " 'What is the difference between a comet and an asteroid?',\n",
       " 'What will be the resolution one should have for the year 2016?',\n",
       " 'What programming language should I learn if I want to do a 2D MMORPG game?',\n",
       " 'What if I hired two private eyes and ordered them to follow each other?',\n",
       " 'What are the best places to recommend foreigners to visit in Nepal?',\n",
       " 'What happens if dictatorship is continuing in the present days?',\n",
       " 'What is the best combination of courses I can take up along with CA to enhance my career?',\n",
       " 'What is the best answer to what you did in two years gap in a job interview?',\n",
       " 'What do countries do to prevent war?',\n",
       " \"What are the best self-help books you've ever read?\",\n",
       " 'What can you do in lucid dreams?',\n",
       " 'What is the IELTS all about?',\n",
       " 'What would cause an AMP to cut out?',\n",
       " 'What does it mean if a dog vomits white foam?',\n",
       " 'What can make Physics easy to learn?',\n",
       " 'What are some new marketing techniques for language schools?',\n",
       " 'What is the difference in pleasure between masturbation and sex?',\n",
       " \"What is the best/most memorable thing you've ever eaten and why?\",\n",
       " 'What is the Hizmet movement?',\n",
       " 'What is the best place for sex?',\n",
       " 'What are the good websites to learn C programming for begineer?',\n",
       " 'What is your view on the recent demonetization in India?',\n",
       " 'What is the best lesson in life?',\n",
       " 'What were the main and most important political causes of World War 1?',\n",
       " 'What is the difference between the iPhone 6s and iPhone 6s Plus?',\n",
       " \"What's the difference between love and pity?\",\n",
       " 'What are the signs of an ultra smart person playing dumb?',\n",
       " 'What are some movies that everyone needs to watch at least once in life?',\n",
       " 'What are some good places to learn mountaineering in India?',\n",
       " 'What is the best book ever made?',\n",
       " 'What mineral holds the highest electrical charge?',\n",
       " 'What are some must watch TV shows before you die?',\n",
       " \"What's the best near-death experience you've ever had?\",\n",
       " 'What are the life lessons that Batman teaches us?',\n",
       " 'What are the conditions in European refugee camps?',\n",
       " 'What are the best daily life hacks one can use to make his life easy?',\n",
       " \"What could be Narendra Modi's next move after demonetization?\",\n",
       " 'What is difference between net and gross income?',\n",
       " 'What are some examples of products that can be make from crude oil?',\n",
       " 'What do you think China food?',\n",
       " 'What are movies with incest scenes?',\n",
       " 'What is the main difference between socialism and communism.?',\n",
       " 'What are the rights of a prisoner?',\n",
       " 'What would be effect of 500 and 1000 Rs notes ban?',\n",
       " 'What is your take on Urjit Patel as the new RBI governor?',\n",
       " 'What are some of the high salary income jobs in the field of biotechnology?',\n",
       " 'What is the reason Pakistan supports terrorism?',\n",
       " 'What are the pros and cons of the German Shepherd dog?',\n",
       " 'What are the best interview questions ever asked?',\n",
       " 'What can be the medium budget to visit best places in Kerala for three members (2-3 days)?',\n",
       " 'What is the meaning of life? Whats our purpose on Earth?',\n",
       " 'What are the major differences between Chinese culture and western cultures?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "class Faiss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def faiss(self,xb):\n",
    "        d = xb[0].size\n",
    "        M = 32\n",
    "        index = faiss.IndexHNSWFlat(d, M)            \n",
    "        index.hnsw.efConstruction = 40         # Setting the value for efConstruction.\n",
    "        index.hnsw.efSearch = 16               # Setting the value for efSearch.\n",
    "        index.add(xb)\n",
    "        return index\n",
    "    \n",
    "    def query(self,index,xq,k=3):\n",
    "        D, I = index.search(xq, k)   \n",
    "        return D, I\n",
    "    \n",
    "def similaritySearch(index,xq,k=100):\n",
    "    D,I = Faiss().query(index,xq,k=k)\n",
    "    I = I[0]\n",
    "    guesses = [data[guess] for guess in I]\n",
    "    return guesses\n",
    "\n",
    "q = list(quoraData.question2[1:2])\n",
    "xq = train.getEmbeddings(q)    \n",
    "\n",
    "index = Faiss().faiss(xb)\n",
    "similaritySearch(index,xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8146513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What should I do to be a great geologist?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/code/mahmoudebrahim12/ml-project1-bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
