{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56f50e95b245e7041cda2f292062fbc926f5f5ac"
   },
   "source": [
    "# **How to train your <del>dragon</del> custom word embeddings**\n",
    "\n",
    "In the [baseline-keras-lstm-is-all-you-need](https://www.kaggle.com/huikang/baseline-keras-lstm-is-all-you-need) notebook shared by Hui Kang (thanks again!), it was demonstrated that a LSTM model using generic global vector (GLOVE) achieved a pretty solid benchmark results.\n",
    "\n",
    "After playing around with GLOVE, you will quickly find that certain words in your training data are not present in its vocab. These are typically replaced with same-shape zero vector, which essentially means you are 'sacrificing' the word as your input feature, which can potentially be important for correct prediction. Another way to deal with this is to train your own word embeddings, using your training data, so that the semantic relationship of your own training corpus can be better represented.\n",
    "\n",
    "In this notebook, I will demonstrate how to train your custom word2vec using Gensim.\n",
    "\n",
    "For those who are new to word embeddings and would like to find out more, you can check out the following articles:\n",
    "1. [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "2. [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "41aea812920c93fd76c4b852814298c88f9bcf73"
   },
   "outputs": [],
   "source": [
    "def preprocessing(titles_array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take in an array of titles, and return the processed titles.\n",
    "    \n",
    "    (e.g. input: 'i am a boy', output - 'am boy')  -> since I remove those words with length 1\n",
    "    \n",
    "    Feel free to change the preprocessing steps and see how it affects the modelling results!\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_array = []\n",
    "    \n",
    "    for title in tqdm(titles_array):\n",
    "        \n",
    "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
    "        processed = re.sub('[^a-zA-Z ]', '', title)\n",
    "        \n",
    "        words = processed.split()\n",
    "        \n",
    "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
    "        processed_array.append(' '.join([word for word in words if len(word) > 1]))\n",
    "    \n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "821183ea8057c78e9e3280801e5b80329d248ada"
   },
   "source": [
    "## **Something to take note**\n",
    "Word2vec is a **self-supervised** method (well, sort of unsupervised but not unsupervised, since it provides its own labels. check out this [Quora](https://www.quora.com/Is-Word2vec-a-supervised-unsupervised-learning-algorithm) thread for a more detailed explanation), so we can make full use of the entire dataset (including test data) to obtain a more wholesome word embedding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "dd627323f593ee323a3ae62954bc06ccceca485d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('SEC-CompanyTicker.csv')\n",
    "df_test = pd.read_csv('SEC-CompanyTicker.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cik_str</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>320193</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Apple Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>789019</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corp</td>\n",
       "      <td>Microsoft Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1652044</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1018724</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon Com Inc</td>\n",
       "      <td>Amazon Com Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1045810</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>Nvidia Corp</td>\n",
       "      <td>Nvidia Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1075531</td>\n",
       "      <td>BKNG</td>\n",
       "      <td>Booking Holdings Inc.</td>\n",
       "      <td>Booking Holdings Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>829224</td>\n",
       "      <td>SBUX</td>\n",
       "      <td>Starbucks Corp</td>\n",
       "      <td>Starbucks Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1668717</td>\n",
       "      <td>BUD</td>\n",
       "      <td>Anheuser-Busch Inbev Sa/Nv</td>\n",
       "      <td>AnheuserBusch Inbev SaNv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>947263</td>\n",
       "      <td>TD</td>\n",
       "      <td>Toronto Dominion Bank</td>\n",
       "      <td>Toronto Dominion Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>101829</td>\n",
       "      <td>RTX</td>\n",
       "      <td>Rtx Corp</td>\n",
       "      <td>Rtx Corp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  cik_str ticker                       title  \\\n",
       "0            0   320193   AAPL                  Apple Inc.   \n",
       "1            1   789019   MSFT              Microsoft Corp   \n",
       "2            2  1652044  GOOGL               Alphabet Inc.   \n",
       "3            3  1018724   AMZN              Amazon Com Inc   \n",
       "4            4  1045810   NVDA                 Nvidia Corp   \n",
       "..         ...      ...    ...                         ...   \n",
       "95          95  1075531   BKNG       Booking Holdings Inc.   \n",
       "96          96   829224   SBUX              Starbucks Corp   \n",
       "97          97  1668717    BUD  Anheuser-Busch Inbev Sa/Nv   \n",
       "98          98   947263     TD       Toronto Dominion Bank   \n",
       "99          99   101829    RTX                    Rtx Corp   \n",
       "\n",
       "                   processed  \n",
       "0                  Apple Inc  \n",
       "1             Microsoft Corp  \n",
       "2               Alphabet Inc  \n",
       "3             Amazon Com Inc  \n",
       "4                Nvidia Corp  \n",
       "..                       ...  \n",
       "95      Booking Holdings Inc  \n",
       "96            Starbucks Corp  \n",
       "97  AnheuserBusch Inbev SaNv  \n",
       "98     Toronto Dominion Bank  \n",
       "99                  Rtx Corp  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.rename(columns={\"companyName\":\"title\"}).head(100)\n",
    "df_test = df_test.rename(columns={\"companyName\":\"title\"})\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "eefec6cadf21909d078915466e3fd672d62f4f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 178861.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10898/10898 [00:00<00:00, 376805.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10998/10998 [00:00<00:00, 916821.47it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['processed'] = preprocessing(df_train['title'])\n",
    "df_test['processed'] = preprocessing(df_test['title'])\n",
    "\n",
    "sentences = pd.concat([df_train['processed'], df_test['processed']],axis=0)\n",
    "train_sentences = list(sentences.progress_apply(str.split).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "e1acc9328db3e49a478a605c27ceb4886e69513e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0.00 mins\n"
     ]
    }
   ],
   "source": [
    "# Parameters reference : https://www.quora.com/How-do-I-determine-Word2Vec-parameters\n",
    "# Feel free to customise your own embedding\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Word2Vec(train_sentences, \n",
    "                 min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6820a6e76f3ed9f8386c4eddcdbeb9fb45e6e6b"
   },
   "source": [
    "## **Pretty fast isn't it.**\n",
    "\n",
    "Let's check out some of the features of the customised word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec([\"a\",\"b\",\"c\"], \n",
    "                 min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "f4cc2a2eb6e1a836b2fb39bdd1f1572ebde0517d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of vocab in our custom word embedding\n",
    "vocab_len = len(model.wv)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x283fc8bb0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "f288af1fc4d32841e948759840296956167ce78e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the dimension of each word (we set it to 100 in the above training step)\n",
    "\n",
    "model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "7ddfb22a1b2f0ef3a7b167580cc7abc324643730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.4563962e-05,  3.0773198e-03, -6.8126451e-03, -1.3754654e-03,\n",
       "        7.6685809e-03,  7.3464094e-03, -3.6732971e-03,  2.6427018e-03,\n",
       "       -8.3171297e-03,  6.2054861e-03, -4.6373224e-03, -3.1641065e-03,\n",
       "        9.3113566e-03,  8.7338570e-04,  7.4907029e-03, -6.0740625e-03,\n",
       "        5.1605068e-03,  9.9228229e-03, -8.4573915e-03, -5.1356913e-03,\n",
       "       -7.0648370e-03, -4.8626517e-03, -3.7785638e-03, -8.5361991e-03,\n",
       "        7.9556061e-03, -4.8439382e-03,  8.4236134e-03,  5.2625705e-03,\n",
       "       -6.5500261e-03,  3.9578713e-03,  5.4701497e-03, -7.4265362e-03,\n",
       "       -7.4057197e-03, -2.4752307e-03, -8.6257253e-03, -1.5815723e-03,\n",
       "       -4.0343284e-04,  3.2996845e-03,  1.4418805e-03, -8.8142155e-04,\n",
       "       -5.5940580e-03,  1.7303658e-03, -8.9737179e-04,  6.7936908e-03,\n",
       "        3.9735902e-03,  4.5294715e-03,  1.4343059e-03, -2.6998555e-03,\n",
       "       -4.3668128e-03, -1.0320747e-03,  1.4370275e-03, -2.6460087e-03,\n",
       "       -7.0737829e-03, -7.8053069e-03, -9.1217868e-03, -5.9351693e-03,\n",
       "       -1.8474245e-03, -4.3238713e-03, -6.4606704e-03, -3.7173224e-03,\n",
       "        4.2891586e-03, -3.7390434e-03,  8.3781751e-03,  1.5339935e-03,\n",
       "       -7.2423196e-03,  9.4337985e-03,  7.6312125e-03,  5.4932819e-03,\n",
       "       -6.8488456e-03,  5.8226790e-03,  4.0090932e-03,  5.1853694e-03,\n",
       "        4.2559016e-03,  1.9397545e-03, -3.1701624e-03,  8.3538452e-03,\n",
       "        9.6121803e-03,  3.7926030e-03, -2.8369951e-03,  7.1275235e-06,\n",
       "        1.2188185e-03, -8.4583247e-03, -8.2239453e-03, -2.3101569e-04,\n",
       "        1.2372875e-03, -5.7433806e-03, -4.7252737e-03, -7.3460746e-03,\n",
       "        8.3286157e-03,  1.2129784e-04, -4.5093987e-03,  5.7017053e-03,\n",
       "        9.1800150e-03, -4.0998720e-03,  7.9646818e-03,  5.3754342e-03,\n",
       "        5.8791232e-03,  5.1259040e-04,  8.2130842e-03, -7.0190406e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out how 'iphone' is represented (an array of 100 numbers)\n",
    "\n",
    "model.wv.get_vector('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "465e6d2e4ff285001317589a5abcc0a7a64607aa"
   },
   "source": [
    "## Now, why are word embeddings powerful? \n",
    "\n",
    "This is because they capture the semantics relationships between words. In other words, words with similar meanings should appear near each other in the vector space of our custom embeddings.\n",
    "\n",
    "Lets check out an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "3dfbc936126bc2fa261b244fb2ffca4c88887ba7"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'k' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find words with similar meaning to 'iphone'\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'k' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Find words with similar meaning to 'iphone'\n",
    "\n",
    "model.wv.most_similar('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "686e58aee82c64f9602fecc97fe554c031c2ce9a"
   },
   "source": [
    "Well, you will see words similar to 'iphone', sorted based on euclidean distance.\n",
    "Of cause, there are also not so intuitive and relevant ones (e.g. jetblack, cpo, ten). If you would like to tackle this, you can do a more thorough pre-processing/ try other embedding dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d18e90fe3baf47224ec8452c853916d514b9a3ad"
   },
   "source": [
    "## **The most important part!**\n",
    "Last but not least, save your word embeddings, so that you can used it for modelling. You can load the text file next time using Gensim KeyedVector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "617b3112a72305368bd235290fe393bc16577fbf"
   },
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('custom_glove_100d.txt')\n",
    "\n",
    "\n",
    "# How to load:\n",
    "# w2v = KeyedVectors.load_word2vec_format('custom_glove_100d.txt')\n",
    "\n",
    "# How to get vector using loaded model\n",
    "# w2v.get_vector('iphone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "2f272c0a251aadb452474dfb7e2139476497e8fd"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Apple Inc' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine similarity between \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      2\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwonderland\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - Skip Gram : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple Inc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple Inc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1234\u001b[0m, in \u001b[0;36mKeyedVectors.similarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, w1, w2):\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute cosine similarity between two keys.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot(matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m]\u001b[49m), matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28mself\u001b[39m[w2]))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Apple Inc' not present\""
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'alice' \" +\n",
    "          \"and 'wonderland' - Skip Gram : \",\n",
    "    model.wv.similarity('Apple Inc', 'Apple Inc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
