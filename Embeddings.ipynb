{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2a88ca",
   "metadata": {},
   "source": [
    "https://medium.com/nerd-for-tech/implementing-glove-from-scratch-word-embedding-for-transformers-95503138d65\n",
    "\n",
    "implementing glove from scratch\n",
    "\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.608.pdf Rice Attention word embedding\n",
    "\n",
    "https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e Make own self attention network LSTM\n",
    "\n",
    "https://deepanshusachdeva5.medium.com/understanding-transformers-step-by-step-word-embeddings-4f4101e7c2f Bag of words embeddings, lacks sequential encoding\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=SEnXr6v2ifU MIT RNN lecture\n",
    "\n",
    "https://stackoverflow.blog/2023/11/08/an-intuitive-introduction-to-text-embeddings/\n",
    "\n",
    "\n",
    "Word2Vec\n",
    "https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673\n",
    "- Distributed representation of words, in one hot encoding all separate parameters, but with distributed representations, similar words are closer\n",
    "- uses common bag of words and (CBOW) and Skip Gram\n",
    "Common Bag of Words\n",
    "- takes c context words that are one hot encoded and uses neural network to predict the next word, one hot encoding of all context words and then outputs the taking the average of \n",
    "- CBOW predicts the target-word based on its surrounding words. \n",
    "- CBOW thus smoothes over the distribution of the information as it treats the entire context as one observation. CBOW is a faster algorithm than skipgrams and works well with frequent words.\n",
    "Skipgram\n",
    "- each word to then predict context, takes longer better for small datasets\n",
    "\n",
    "\n",
    "Mistral\n",
    "- sliding window attention\n",
    "\n",
    "BERT\n",
    "- https://arxiv.org/pdf/1810.04805.pdf\n",
    "- positional encoding + segment encoding\n",
    "- attention mask shows whether it is information or whether it is padded characters\n",
    "\n",
    "OpenAI\n",
    "- OpenAI's embeddings are computed using a transformer-based neural network architecture. The basic idea behind this architecture is to use self-attention mechanisms to generate a representation of each word in a sentence based on its context.\n",
    "- The purpose of embeddings is to represent text in a continuous, dense and low-dimensional vector space, such that semantically similar words are mapped to vectors that are close to each other in that space.\n",
    "- it encodes your text as a length-1536 vector, there is no way to recover the original text from the embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f6cbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce39bb0512347d59e3a012eb9345fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3cdb6fbdd40e8b27b5ce54e511666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transform our world, but in ways we haven’t imagined yet. Here are the four things I see that will happen with AI over the next decade.\n",
      "\n",
      "### 1. We’ll be able to ask questions of data and get answers back.\n",
      "\n",
      "Today, you can search for a word or phrase and find pages of results. You can also use Google Trends to see how often a term is searched for. But it will take more than just words in the future. In fact, I believe that we’ll be able to ask questions of data and get answers back.\n",
      "\n",
      "The best way to do this is with a “chatbot.” This technology is currently being used by companies like Facebook Messenger and Slack. The idea behind chatbots is simple: you can talk to them as if they were another person in the room. You can ask questions about your data, or even make requests of their service providers (such as sending money from PayPal).\n",
      "\n",
      "I see two types of applications for these chatbots: 1) internal business intelligence (BI), where employees use them internally to get answers; and 2) external BI, where customers use them externally to get answers about products or services\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\", model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870e681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149a94b8ddd948d2ac147971ed231c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9357fbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1984, 16020,  2076,  2487,   349]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([prompt], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ea81cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"../models/models/\")\n",
    "# tokenizer.save_vocabulary(\"../models/tokenizers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4af24c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "key = \"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\"\n",
    "# Initialize OpenAI client (replace '...' with your API key)\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "\n",
    "class OpenAIEmbeddings:\n",
    "    def __init__(self,api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "    \n",
    "    def get_embeddings(self, text_list):\n",
    "        data = self.client.embeddings.create(input=text_list, model='text-embedding-ada-002').data\n",
    "        embeddings = [embedding.embedding for embedding in data]\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    #convert dataset into embeddings dataset to run FAISS\n",
    "    def makeEmbeddings(self,dataset):\n",
    "        embeddings = self.get_embeddings(dataset)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def getQueryEmbedding(self, query):\n",
    "        return self.get_embeddings([query])\n",
    "    \n",
    "e = OpenAIEmbeddings(key)\n",
    "e.makeEmbeddings([\"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e89a8940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03503197, -0.02060164, -0.01537573, ..., -0.01162699,\n",
       "        -0.00087646,  0.00465802]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01ec83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.embeddings.create(input=[\"hi\",\"hi\",\"hi\",\"julia is cool\"], model='text-embedding-ada-002').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308da023",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e70d83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8LeCiZTNb6QAtr6wNTMXFKvBQ1Zah', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='This is a test.', role='assistant', function_call=None, tool_calls=None))], created=1700170124, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62ed7624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[3].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2ab3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
