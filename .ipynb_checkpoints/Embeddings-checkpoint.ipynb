{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2a88ca",
   "metadata": {},
   "source": [
    "https://medium.com/nerd-for-tech/implementing-glove-from-scratch-word-embedding-for-transformers-95503138d65\n",
    "\n",
    "implementing glove from scratch\n",
    "\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.608.pdf Rice Attention word embedding\n",
    "\n",
    "https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e Make own self attention network LSTM\n",
    "\n",
    "https://deepanshusachdeva5.medium.com/understanding-transformers-step-by-step-word-embeddings-4f4101e7c2f Bag of words embeddings, lacks sequential encoding\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=SEnXr6v2ifU MIT RNN lecture\n",
    "\n",
    "https://stackoverflow.blog/2023/11/08/an-intuitive-introduction-to-text-embeddings/\n",
    "\n",
    "\n",
    "Word2Vec\n",
    "https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673\n",
    "- Distributed representation of words, in one hot encoding all separate parameters, but with distributed representations, similar words are closer\n",
    "- uses common bag of words and (CBOW) and Skip Gram\n",
    "Common Bag of Words\n",
    "- takes c context words that are one hot encoded and uses neural network to predict the next word, one hot encoding of all context words and then outputs the taking the average of \n",
    "- CBOW predicts the target-word based on its surrounding words. \n",
    "- CBOW thus smoothes over the distribution of the information as it treats the entire context as one observation. CBOW is a faster algorithm than skipgrams and works well with frequent words.\n",
    "Skipgram\n",
    "- each word to then predict context, takes longer better for small datasets\n",
    "\n",
    "\n",
    "Mistral\n",
    "- sliding window attention\n",
    "\n",
    "BERT\n",
    "- https://arxiv.org/pdf/1810.04805.pdf\n",
    "- positional encoding + segment encoding\n",
    "\n",
    "OpenAI\n",
    "- OpenAI's embeddings are computed using a transformer-based neural network architecture. The basic idea behind this architecture is to use self-attention mechanisms to generate a representation of each word in a sentence based on its context.\n",
    "- The purpose of embeddings is to represent text in a continuous, dense and low-dimensional vector space, such that semantically similar words are mapped to vectors that are close to each other in that space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f6cbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce39bb0512347d59e3a012eb9345fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3cdb6fbdd40e8b27b5ce54e511666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transform our world, but in ways we haven’t imagined yet. Here are the four things I see that will happen with AI over the next decade.\n",
      "\n",
      "### 1. We’ll be able to ask questions of data and get answers back.\n",
      "\n",
      "Today, you can search for a word or phrase and find pages of results. You can also use Google Trends to see how often a term is searched for. But it will take more than just words in the future. In fact, I believe that we’ll be able to ask questions of data and get answers back.\n",
      "\n",
      "The best way to do this is with a “chatbot.” This technology is currently being used by companies like Facebook Messenger and Slack. The idea behind chatbots is simple: you can talk to them as if they were another person in the room. You can ask questions about your data, or even make requests of their service providers (such as sending money from PayPal).\n",
      "\n",
      "I see two types of applications for these chatbots: 1) internal business intelligence (BI), where employees use them internally to get answers; and 2) external BI, where customers use them externally to get answers about products or services\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\", model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870e681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149a94b8ddd948d2ac147971ed231c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9357fbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1984, 16020,  2076,  2487,   349]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([prompt], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ea81cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"../models/models/\")\n",
    "# tokenizer.save_vocabulary(\"../models/tokenizers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4af24c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df.to_csv('output/embedded_1k_reviews.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
    "df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308da023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
