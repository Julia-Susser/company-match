{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e9a7e8",
   "metadata": {
    "papermill": {
     "duration": 0.009019,
     "end_time": "2023-11-07T12:11:42.558290",
     "exception": false,
     "start_time": "2023-11-07T12:11:42.549271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>1 |</span></b> <b>BACKGROUND</b></div>\n",
    "\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>ENCODER BASE</span></b></p></div>\n",
    "\n",
    "In the following notebook, we'll look at the following components of the Transformer Encoder structure\n",
    "\n",
    "<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n",
    "    \n",
    "<ul>\n",
    "<li>Simple Attention</li>\n",
    "<li>Multi-Head Self Attention</li>\n",
    "<li>Feed Forward Layer</li>\n",
    "<li>Normalisation</li>\n",
    "<li>Skip Connection</li>\n",
    "<li>Position Embeddings</li>\n",
    "<li>Transformer Encoder</li>\n",
    "<li>Classifier Head</li>\n",
    "</ul> \n",
    "</div> \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>ENCODER BASE</span></b></p></div>\n",
    "\n",
    "Encoder simply put:\n",
    "- Converts a **series tokens** into a **series of embedding vectors** (hidden state)\n",
    "- The encoder (neural network) consists of **multiple layers** (**blocks**) constructed together \n",
    "\n",
    "The encoder structure:\n",
    "- Composed of multiple encoder layers (blocks) stacked next to each other (similar to CNN layer stacks)\n",
    "- Each encoder block contains **multi-head self attention** & **fully connected feed forward layer** (for each input embedding)\n",
    "\n",
    "Purpose of the Encoder\n",
    "- Input tokens are encoded & modified into a form that **stores some contextual information** in the sequence\n",
    "\n",
    "The example we'll use:\n",
    "\n",
    "> the bark of a palm tree is very rough\n",
    "\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>CLASSIFICATION HEAD</span></b></p></div>\n",
    "\n",
    "- Transformers can be utilised for various application so they are created in a base form\n",
    "- If we want to utilise them for a specific task, we add an extra component **head** to the transformer\n",
    "- In this example, we'll utilise it for **classification** purposes, and look at how we can combine the base with the **head**\n",
    "\n",
    "\n",
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>2 |</span></b> <b>SIMPLE SELF ATTENTION</b></div>\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>TYPES OF ATTENTION</span></b></p></div>\n",
    "\n",
    "**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention</mark>**\n",
    " \n",
    "- Mechanism which allows networks to assign **different weight distributions to each element** in a sequence \n",
    "- Elements in sequence - `token embeddings` (each token mapped to a vector of fixed dimension) (eg. BERT model - 768 dimensions)\n",
    " \n",
    " \n",
    "**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">self-attention</mark>**\n",
    "\n",
    "- Instead of using fixed embeddings for each token, can use whole sequence to **compute weighted average** of each `embedding`\n",
    "- One can think of self-attention as a form of averaging\n",
    "- Common form of `self-attention` **scaled dot-product attention** \n",
    "\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>FOUR MAIN STEPS</span></b></p></div>\n",
    "\n",
    "\n",
    "- Project each `token embedding` into three vectors **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">query</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">key</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">value</mark>**\n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** (nxn)\n",
    "\n",
    "    - (we determine how much the query & key vectors relate to eachother using a similarity function)\n",
    "    - Similarity function for scaled dot-product attention - dot product\n",
    "    - queries & keys that are similar will have large dot product & visa versa\n",
    "    - Outputs from this step - attention scores\n",
    "    \n",
    "    \n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weight</mark>** (wij)\n",
    "\n",
    "    - dot products produce large numbers \n",
    "    - attention scores first multiplied by a scaling factor to normalise their variance\n",
    "    - Then normalised with softmax to ensure all column values sum to 1\n",
    "    \n",
    "    \n",
    "- Update the token embeddings (hidden state)\n",
    "\n",
    "    - multiply the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">weights</mark>** by the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">value</mark>** vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55217be8",
   "metadata": {
    "papermill": {
     "duration": 0.009999,
     "end_time": "2023-11-07T12:11:42.577461",
     "exception": false,
     "start_time": "2023-11-07T12:11:42.567462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>SIMPLE ATTENTION FORMULATION</span></b></p></div>\n",
    "\n",
    "\n",
    "- Well look at a simple example, and summarise the attention mechanism in one function\n",
    "- `bert-base-uncased` model will be used to extract different model settings (eg. number of attention heads), so we will be building a similar model \n",
    "\n",
    "<br>\n",
    "\n",
    "##### **1. DOCUMENT TOKENISATION**\n",
    "\n",
    "- Each token in the sentence has been mapped to a **unique identifier** from a **vocabulary** or **dictionary**\n",
    "- We start off by using the `bert-base-uncased` pretrained tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35551bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:11:42.597284Z",
     "iopub.status.busy": "2023-11-07T12:11:42.596990Z",
     "iopub.status.idle": "2023-11-07T12:12:00.483141Z",
     "shell.execute_reply": "2023-11-07T12:12:00.482187Z"
    },
    "papermill": {
     "duration": 17.898714,
     "end_time": "2023-11-07T12:12:00.485317",
     "exception": false,
     "start_time": "2023-11-07T12:11:42.586603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "\n",
    "# load tokeniser and model\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# document well be using as an exmaple\n",
    "text = [\"the bark of a palm tree is very rough of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f10c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:00.517236Z",
     "iopub.status.busy": "2023-11-07T12:12:00.516931Z",
     "iopub.status.idle": "2023-11-07T12:12:00.533190Z",
     "shell.execute_reply": "2023-11-07T12:12:00.532308Z"
    },
    "papermill": {
     "duration": 0.034096,
     "end_time": "2023-11-07T12:12:00.535098",
     "exception": false,
     "start_time": "2023-11-07T12:12:00.501002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931,  1997]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenise input (text)\n",
    "inputs = tokenizer(text, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False) # don't use pad, sep tokens\n",
    "\n",
    "print(inputs.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656eec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.is_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a0445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb56d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:00.568327Z",
     "iopub.status.busy": "2023-11-07T12:12:00.568012Z",
     "iopub.status.idle": "2023-11-07T12:12:06.186950Z",
     "shell.execute_reply": "2023-11-07T12:12:06.186030Z"
    },
    "papermill": {
     "duration": 5.637794,
     "end_time": "2023-11-07T12:12:06.188698",
     "exception": false,
     "start_time": "2023-11-07T12:12:00.550904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the bark of a palm tree is very rough of'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode sequence\n",
    "tokenizer.decode(inputs['input_ids'].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e975fd",
   "metadata": {
    "papermill": {
     "duration": 0.014971,
     "end_time": "2023-11-07T12:12:06.218934",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.203963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At this point:\n",
    "\n",
    "- `inputs.inpits_ids` A tensor of id mapped tokens\n",
    "- Token embeddings are **independent of their context**\n",
    "- **Homonyms** (same spelling, but different meaning) have the same representation\n",
    "\n",
    "Role of subsequent attention layers:\n",
    "\n",
    "- Mix the **token embeddings** to disambiguate & inform the representation of each token with the context of its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8859aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.251254Z",
     "iopub.status.busy": "2023-11-07T12:12:06.250611Z",
     "iopub.status.idle": "2023-11-07T12:12:06.511660Z",
     "shell.execute_reply": "2023-11-07T12:12:06.510800Z"
    },
    "papermill": {
     "duration": 0.279247,
     "end_time": "2023-11-07T12:12:06.513439",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.234192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 hidden size\n",
      "30522 vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Create an embedding layer\n",
    "\n",
    "'''\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "print(config.hidden_size,\"hidden size\")\n",
    "print(config.vocab_size,\"vocabulary size\")\n",
    "\n",
    "# load sample embedding layer of size (30522,758) -> same as bert-base\n",
    "token_emb = nn.Embedding(config.vocab_size,\n",
    "                         config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee78d5",
   "metadata": {
    "papermill": {
     "duration": 0.015119,
     "end_time": "2023-11-07T12:12:06.544245",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.529126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### **2. EMBEDDING VECTORS**\n",
    "\n",
    "\n",
    "- Convert Tokenised data into embedding data (768 dimensions) using vocab of 30522 tokens\n",
    "- Each input_ids is **mapped to one of 30522 embedding vectors** stored in nn.embedding, each with a size of 768 \n",
    "- Our output will be [batch_size,seq_len,hidden_dim] by calling `nn.Embedding(hidden)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd751b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.576500Z",
     "iopub.status.busy": "2023-11-07T12:12:06.576191Z",
     "iopub.status.idle": "2023-11-07T12:12:06.586332Z",
     "shell.execute_reply": "2023-11-07T12:12:06.585567Z"
    },
    "papermill": {
     "duration": 0.028427,
     "end_time": "2023-11-07T12:12:06.588032",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.559605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Convert Tokens to Embedding Vectors\n",
    "utilising the existing model embedding embeddings\n",
    "\n",
    "'''\n",
    "\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3feb8277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.620723Z",
     "iopub.status.busy": "2023-11-07T12:12:06.620434Z",
     "iopub.status.idle": "2023-11-07T12:12:06.627221Z",
     "shell.execute_reply": "2023-11-07T12:12:06.626491Z"
    },
    "papermill": {
     "duration": 0.024936,
     "end_time": "2023-11-07T12:12:06.628895",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.603959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2747, -1.8235,  0.8008,  ...,  0.7047,  1.3227, -0.5303],\n",
       "         [ 1.4124,  0.7308, -0.4173,  ...,  1.8594, -1.0689,  0.1558],\n",
       "         [ 0.9272,  0.1791,  0.3571,  ...,  0.3408,  0.8663, -0.5704],\n",
       "         ...,\n",
       "         [-0.4634, -0.4977, -0.3177,  ...,  1.3512, -0.7524, -0.1523],\n",
       "         [ 1.7729,  1.0229,  1.6567,  ..., -0.3762,  0.5209,  3.1483],\n",
       "         [ 0.9272,  0.1791,  0.3571,  ...,  0.3408,  0.8663, -0.5704]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 embedding vectors of 768 dimensions\n",
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f90749",
   "metadata": {
    "papermill": {
     "duration": 0.014998,
     "end_time": "2023-11-07T12:12:06.659344",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.644346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### **3. QUERY, KEY, VALUE VECTORS**\n",
    "\n",
    "- As the most simplistic case of attention, **we set them equal to one another**\n",
    "- Attention mechanism with equal query and key vectors will assign a **very large score to identical words in the context** (diagonal component of matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729ded24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.690843Z",
     "iopub.status.busy": "2023-11-07T12:12:06.690541Z",
     "iopub.status.idle": "2023-11-07T12:12:06.696736Z",
     "shell.execute_reply": "2023-11-07T12:12:06.696023Z"
    },
    "papermill": {
     "duration": 0.023794,
     "end_time": "2023-11-07T12:12:06.698387",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.674593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query and key components\n",
      "\n",
      "query size: torch.Size([1, 10, 768])\n",
      "key size: torch.Size([1, 768, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "# setting them equal to one another\n",
    "print(\"query and key components\\n\")\n",
    "query = key = value = inputs_embeds\n",
    "print('query size:',query.size())\n",
    "dim_k = key.size(-1)   # hidden dimension \n",
    "print('key size:',key.transpose(1,2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc551bd",
   "metadata": {
    "papermill": {
     "duration": 0.01558,
     "end_time": "2023-11-07T12:12:06.729699",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.714119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### **4. COMPUTE ATTENTION SCORES**\n",
    "\n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** using the **dot product as the similarity function**\n",
    "- `torch.bmm` - batch matrix matrix product (as we work in batches during training)\n",
    "- If we need to transpose a vector `vector.transpose(1,2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514d67e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.762745Z",
     "iopub.status.busy": "2023-11-07T12:12:06.762404Z",
     "iopub.status.idle": "2023-11-07T12:12:06.776470Z",
     "shell.execute_reply": "2023-11-07T12:12:06.775853Z"
    },
    "papermill": {
     "duration": 0.032784,
     "end_time": "2023-11-07T12:12:06.778682",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.745898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dot product (attention scores)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product & apply normalisation\n",
    "print(\"\\ndot product (attention scores)\")\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46d911a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.812364Z",
     "iopub.status.busy": "2023-11-07T12:12:06.812033Z",
     "iopub.status.idle": "2023-11-07T12:12:06.818346Z",
     "shell.execute_reply": "2023-11-07T12:12:06.817521Z"
    },
    "papermill": {
     "duration": 0.025187,
     "end_time": "2023-11-07T12:12:06.819967",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.794780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5829e+01, -1.0454e+00,  3.1074e-01, -2.6252e+00, -1.3836e+00,\n",
       "          -1.2813e+00, -8.3912e-01, -8.1960e-01,  1.0559e+00,  3.1074e-01],\n",
       "         [-1.0454e+00,  2.7458e+01,  7.7848e-01,  1.1003e+00,  1.4799e+00,\n",
       "           9.2834e-02,  2.7509e-02,  1.4460e+00, -8.3715e-01,  7.7848e-01],\n",
       "         [ 3.1074e-01,  7.7848e-01,  2.8841e+01,  1.1517e-01,  1.9508e-01,\n",
       "           1.3662e-01,  4.5663e-02,  4.8583e-01, -3.8592e-01,  2.8841e+01],\n",
       "         [-2.6252e+00,  1.1003e+00,  1.1517e-01,  2.9882e+01,  2.1040e-01,\n",
       "          -2.5550e+00,  5.2171e-02,  4.1555e-01, -1.6756e+00,  1.1517e-01],\n",
       "         [-1.3836e+00,  1.4799e+00,  1.9508e-01,  2.1040e-01,  2.8372e+01,\n",
       "           8.4505e-02, -3.4399e-01,  3.4558e-02,  2.5317e-01,  1.9508e-01],\n",
       "         [-1.2813e+00,  9.2834e-02,  1.3662e-01, -2.5550e+00,  8.4505e-02,\n",
       "           2.8661e+01, -1.2571e+00, -7.4733e-01,  1.0242e-01,  1.3662e-01],\n",
       "         [-8.3912e-01,  2.7509e-02,  4.5663e-02,  5.2171e-02, -3.4399e-01,\n",
       "          -1.2571e+00,  2.9053e+01,  7.8676e-02, -9.0552e-01,  4.5663e-02],\n",
       "         [-8.1960e-01,  1.4460e+00,  4.8583e-01,  4.1555e-01,  3.4558e-02,\n",
       "          -7.4733e-01,  7.8676e-02,  2.7864e+01,  3.1538e-01,  4.8583e-01],\n",
       "         [ 1.0559e+00, -8.3715e-01, -3.8592e-01, -1.6756e+00,  2.5317e-01,\n",
       "           1.0242e-01, -9.0552e-01,  3.1538e-01,  2.6468e+01, -3.8592e-01],\n",
       "         [ 3.1074e-01,  7.7848e-01,  2.8841e+01,  1.1517e-01,  1.9508e-01,\n",
       "           1.3662e-01,  4.5663e-02,  4.8583e-01, -3.8592e-01,  2.8841e+01]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291f7de",
   "metadata": {
    "papermill": {
     "duration": 0.015722,
     "end_time": "2023-11-07T12:12:06.851771",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.836049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### **5. COMPUTE ATTENTION WEIGHTS (SOFTMAX FUNCTION)**\n",
    "\n",
    "\n",
    "\n",
    "- Created a 5x5 matrix of **attention scores** per sample in the batch\n",
    "- Apply the softmax for normalisation to get the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848e4e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.885652Z",
     "iopub.status.busy": "2023-11-07T12:12:06.885333Z",
     "iopub.status.idle": "2023-11-07T12:12:06.894232Z",
     "shell.execute_reply": "2023-11-07T12:12:06.893421Z"
    },
    "papermill": {
     "duration": 0.02817,
     "end_time": "2023-11-07T12:12:06.896060",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.867890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax applied, attention weights :\n",
      "\n",
      "torch.Size([1, 10, 10])\n",
      "tensor([[[1.0000e+00, 2.1312e-12, 8.2716e-12, 4.3904e-13, 1.5197e-12,\n",
      "          1.6834e-12, 2.6195e-12, 2.6711e-12, 1.7427e-11, 8.2716e-12],\n",
      "         [4.1812e-13, 1.0000e+00, 2.5905e-12, 3.5741e-12, 5.2240e-12,\n",
      "          1.3050e-12, 1.2225e-12, 5.0501e-12, 5.1491e-13, 2.5905e-12],\n",
      "         [2.0336e-13, 3.2464e-13, 5.0000e-01, 1.6724e-13, 1.8115e-13,\n",
      "          1.7086e-13, 1.5601e-13, 2.4227e-13, 1.0132e-13, 5.0000e-01],\n",
      "         [7.6282e-15, 3.1653e-13, 1.1819e-13, 1.0000e+00, 1.2999e-13,\n",
      "          8.1837e-15, 1.1097e-13, 1.5960e-13, 1.9717e-14, 1.1819e-13],\n",
      "         [1.1944e-13, 2.0928e-12, 5.7909e-13, 5.8803e-13, 1.0000e+00,\n",
      "          5.1847e-13, 3.3778e-13, 4.9321e-13, 6.1373e-13, 5.7909e-13],\n",
      "         [9.9100e-14, 3.9159e-13, 4.0912e-13, 2.7728e-14, 3.8834e-13,\n",
      "          1.0000e+00, 1.0152e-13, 1.6903e-13, 3.9536e-13, 4.0912e-13],\n",
      "         [1.0422e-13, 2.4792e-13, 2.5246e-13, 2.5411e-13, 1.7099e-13,\n",
      "          6.8616e-14, 1.0000e+00, 2.6094e-13, 9.7523e-14, 2.5246e-13],\n",
      "         [3.4907e-13, 3.3641e-12, 1.2878e-12, 1.2004e-12, 8.2011e-13,\n",
      "          3.7524e-13, 8.5710e-13, 1.0000e+00, 1.0860e-12, 1.2878e-12],\n",
      "         [9.1930e-12, 1.3845e-12, 2.1741e-12, 5.9866e-13, 4.1194e-12,\n",
      "          3.5429e-12, 1.2930e-12, 4.3837e-12, 1.0000e+00, 2.1741e-12],\n",
      "         [2.0336e-13, 3.2464e-13, 5.0000e-01, 1.6724e-13, 1.8115e-13,\n",
      "          1.7086e-13, 1.5601e-13, 2.4227e-13, 1.0132e-13, 5.0000e-01]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "sum of column values:/n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"sotfmax applied, attention weights :\\n\")\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights.size())\n",
    "print(weights)\n",
    "\n",
    "print(\"\\nsum of column values:/n\")\n",
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf19419",
   "metadata": {
    "papermill": {
     "duration": 0.017197,
     "end_time": "2023-11-07T12:12:06.930058",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.912861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### **6. UPDATE VALUES**\n",
    "\n",
    "Multiply the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>** matrix by the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">values</mark>** vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d827e60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:06.963834Z",
     "iopub.status.busy": "2023-11-07T12:12:06.963500Z",
     "iopub.status.idle": "2023-11-07T12:12:06.970332Z",
     "shell.execute_reply": "2023-11-07T12:12:06.969370Z"
    },
    "papermill": {
     "duration": 0.025786,
     "end_time": "2023-11-07T12:12:06.972071",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.946285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2747, -1.8235,  0.8008,  ...,  0.7047,  1.3227, -0.5303],\n",
      "         [ 1.4124,  0.7308, -0.4173,  ...,  1.8594, -1.0689,  0.1558],\n",
      "         [ 0.9272,  0.1791,  0.3571,  ...,  0.3408,  0.8663, -0.5704],\n",
      "         ...,\n",
      "         [-0.4634, -0.4977, -0.3177,  ...,  1.3512, -0.7524, -0.1523],\n",
      "         [ 1.7729,  1.0229,  1.6567,  ..., -0.3762,  0.5209,  3.1483],\n",
      "         [ 0.9272,  0.1791,  0.3571,  ...,  0.3408,  0.8663, -0.5704]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, value)\n",
    "print(attn_outputs)\n",
    "print(attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5cf7f",
   "metadata": {
    "papermill": {
     "duration": 0.016049,
     "end_time": "2023-11-07T12:12:07.004544",
     "exception": false,
     "start_time": "2023-11-07T12:12:06.988495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we have a general function:\n",
    "- Which inputs vectors `query`, `key` & `value` \n",
    "- Calculates the scalar dot product attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadf54f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.039142Z",
     "iopub.status.busy": "2023-11-07T12:12:07.038780Z",
     "iopub.status.idle": "2023-11-07T12:12:07.044075Z",
     "shell.execute_reply": "2023-11-07T12:12:07.043479Z"
    },
    "papermill": {
     "duration": 0.024195,
     "end_time": "2023-11-07T12:12:07.045542",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.021347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Scalar Dot Product Attention\n",
    "scores = query*key.T / sqrt(dims)\n",
    "weight = softmax(scores) \n",
    "\n",
    "'''\n",
    "\n",
    "def sdp_attention(query, key, value):\n",
    "    dim_k = query.size(-1) # dimension component\n",
    "    sfact = sqrt(dim_k)     \n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sfact\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ff305",
   "metadata": {
    "papermill": {
     "duration": 0.0158,
     "end_time": "2023-11-07T12:12:07.077434",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.061634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>3 |</span></b> <b>MULTIHEAD SELF ATTENTION</b></div>\n",
    "\n",
    "\n",
    "- The meaning of the word will be better informed by **complementary words in the context** than by **identical words** (which gives 1)\n",
    "\n",
    "##### **SIMPLISTIC APPROACH**\n",
    "\n",
    "- We only used the embeddings \"as is\" (no linear transformation) to compute the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>**\n",
    "\n",
    "##### **BETTER APPROACH**\n",
    "\n",
    "- The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">self-attention</mark>** layer applies **three independent linear transformations (`nn.linear`) to each embedding** to generate **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">query</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">key</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">value</mark>** \n",
    "- These transformations project the embeddings and **each projection carries its own set of learnable parameters** (**Weights**)\n",
    "- This **allows the self-attention layer to focus on different semantic aspects of the sequence**\n",
    "\n",
    "\n",
    "\n",
    "Its beneficial to have **multiple sets of linear projections** (each one represents an **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention head</mark>**)\n",
    "\n",
    "Why do we need more than one **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention head</mark>**?\n",
    "- The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">softmax</mark>** of one head tends to focus on mostly **one aspect of similarity**\n",
    "\n",
    "\n",
    "**Several heads** allows the model to **focus on several apsects at once**\n",
    "- Eg. one head can focus on subject-verb interaction, another finds nearby adjectives\n",
    "- **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">CV analogy</mark>**: filters; one filter responsible for detecting the head, another for facial features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae55c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.111569Z",
     "iopub.status.busy": "2023-11-07T12:12:07.110662Z",
     "iopub.status.idle": "2023-11-07T12:12:07.116625Z",
     "shell.execute_reply": "2023-11-07T12:12:07.116040Z"
    },
    "papermill": {
     "duration": 0.024562,
     "end_time": "2023-11-07T12:12:07.118152",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.093590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Attention Class\n",
    "\n",
    "# nn.linear : apply linear transformation to incoming data\n",
    "#             y = x * A^T + b\n",
    "# Ax = b where x is input, b is output, A is weight\n",
    "\n",
    "# calculate scaled dot product attention matrix\n",
    "# Requires embedding dimension \n",
    "# Each attention head is made of different q,k,v vectors\n",
    "\n",
    "'''\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # initalisation \n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the three vectors\n",
    "        # input - embed_dim, output - head_dim\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # main class operation\n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # calculate scaled dot product given a \n",
    "        attn_outputs = sdp_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state))\n",
    "        \n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647aecb",
   "metadata": {
    "papermill": {
     "duration": 0.015925,
     "end_time": "2023-11-07T12:12:07.150302",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.134377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`Attention` will be used in the construction of a model\n",
    "\n",
    "- Weâ€™ve **initialised three independent linear layers** that apply matrix multiplication to the embedding vectors to produce tensors of shape [batch_size, seq_len, head_dim]\n",
    "- Where head_dim is the number of dimensions we are projecting into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f47172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.184165Z",
     "iopub.status.busy": "2023-11-07T12:12:07.183786Z",
     "iopub.status.idle": "2023-11-07T12:12:07.188846Z",
     "shell.execute_reply": "2023-11-07T12:12:07.187928Z"
    },
    "papermill": {
     "duration": 0.024268,
     "end_time": "2023-11-07T12:12:07.190719",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.166451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 heads\n",
      "768 hidden state embedding dimension\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(config.num_attention_heads,'heads')\n",
    "print(config.hidden_size,'hidden state embedding dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc47326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.224688Z",
     "iopub.status.busy": "2023-11-07T12:12:07.224386Z",
     "iopub.status.idle": "2023-11-07T12:12:07.230964Z",
     "shell.execute_reply": "2023-11-07T12:12:07.230200Z"
    },
    "papermill": {
     "duration": 0.025767,
     "end_time": "2023-11-07T12:12:07.232720",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.206953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (q): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (k): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (v): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Sample Initialisation '''\n",
    "\n",
    "# Initialised just one head, requires token embedding vector for forward operation\n",
    "\n",
    "embed_dim = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "\n",
    "attention = Attention(embed_dim,num_heads)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da230f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.266405Z",
     "iopub.status.busy": "2023-11-07T12:12:07.265958Z",
     "iopub.status.idle": "2023-11-07T12:12:07.276364Z",
     "shell.execute_reply": "2023-11-07T12:12:07.275745Z"
    },
    "papermill": {
     "duration": 0.028895,
     "end_time": "2023-11-07T12:12:07.278006",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.249111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.2667e-04, -7.7737e-02, -5.5776e-02,  3.0149e-02, -1.9336e-01,\n",
       "          -3.2226e-01,  1.1496e-01,  3.2659e-02, -1.3306e-01, -1.6859e-01,\n",
       "           9.3852e-02, -4.2003e-01],\n",
       "         [-9.9959e-02, -1.0941e-01, -1.8432e-01,  3.6811e-02, -2.3326e-01,\n",
       "          -3.2482e-01,  8.3666e-02,  1.0681e-01, -1.1075e-01, -3.4479e-01,\n",
       "          -8.4057e-03, -4.5280e-01],\n",
       "         [-1.2049e-02, -1.1459e-01, -1.6258e-01,  3.2581e-02, -2.4308e-01,\n",
       "          -4.3216e-01,  9.4243e-02,  3.7738e-02, -1.4588e-01, -2.2938e-01,\n",
       "           2.0252e-02, -4.3899e-01],\n",
       "         [ 6.0900e-02,  4.9530e-02, -3.2478e-02,  3.4811e-02, -3.6170e-01,\n",
       "          -1.8953e-01, -4.4606e-02,  5.6850e-02, -2.7326e-01, -1.6369e-01,\n",
       "           1.8536e-01, -3.4686e-01],\n",
       "         [-2.7913e-02, -4.9991e-02, -4.2541e-02,  4.2598e-02, -2.3129e-01,\n",
       "          -2.1277e-01,  7.6612e-02,  1.0055e-01, -2.0214e-01, -2.0287e-01,\n",
       "           1.5863e-01, -4.0711e-01],\n",
       "         [ 3.6021e-02, -4.2606e-02, -1.3185e-01,  2.3449e-02, -3.3114e-01,\n",
       "          -2.4745e-01,  5.4744e-02, -1.0887e-03, -2.2097e-01, -2.2369e-01,\n",
       "           1.4015e-01, -4.1773e-01],\n",
       "         [ 1.1123e-02, -2.2394e-02, -2.8306e-02,  3.0363e-02, -2.8009e-01,\n",
       "          -2.4101e-01,  1.7159e-02, -2.5905e-03, -2.4045e-01, -1.7102e-01,\n",
       "           1.6483e-01, -4.1596e-01],\n",
       "         [ 2.3899e-02,  1.5123e-02, -9.6454e-02,  5.9592e-02, -2.9089e-01,\n",
       "          -2.5737e-01,  1.1541e-01, -2.5904e-02, -2.1346e-01, -1.3958e-01,\n",
       "           1.6006e-01, -3.7852e-01],\n",
       "         [-4.4024e-03, -4.3807e-02, -1.6361e-02,  3.9349e-02, -2.0023e-01,\n",
       "          -2.4675e-01,  7.9439e-02,  4.9923e-02, -1.7597e-01, -1.6291e-01,\n",
       "           1.6096e-01, -4.0631e-01],\n",
       "         [-1.2049e-02, -1.1459e-01, -1.6258e-01,  3.2581e-02, -2.4308e-01,\n",
       "          -4.3216e-01,  9.4243e-02,  3.7738e-02, -1.4588e-01, -2.2938e-01,\n",
       "           2.0252e-02, -4.3899e-01]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights are always initialised randomly, attention_outputs varies\n",
    "attention_outputs = attention(inputs_embeds)\n",
    "attention_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54eec369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.312835Z",
     "iopub.status.busy": "2023-11-07T12:12:07.312013Z",
     "iopub.status.idle": "2023-11-07T12:12:07.318401Z",
     "shell.execute_reply": "2023-11-07T12:12:07.317850Z"
    },
    "papermill": {
     "duration": 0.025446,
     "end_time": "2023-11-07T12:12:07.320076",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.294630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Multihead attention class\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class multiHeadAttention(nn.Module):\n",
    "    \n",
    "    # Config during initalisation\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # model params, read from config file\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # attention head (define only w/o hidden state)\n",
    "        # each attention head is initialised with embedd/heads head dimension\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # output uses whole embedding dimension for output\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # Given a hidden state (embeddings)\n",
    "    # Apply operation for multihead attention\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # for each head embed_size/heads, calculate attention\n",
    "        heads = [head(hidden_state) for head in self.heads] \n",
    "        x = torch.cat(heads, dim=-1) # merge/concat head data together\n",
    "    \n",
    "        # apply linear transformation to multihead attension scalar product\n",
    "        x = self.out_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bba389d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.354402Z",
     "iopub.status.busy": "2023-11-07T12:12:07.353650Z",
     "iopub.status.idle": "2023-11-07T12:12:07.380981Z",
     "shell.execute_reply": "2023-11-07T12:12:07.379853Z"
    },
    "papermill": {
     "duration": 0.046191,
     "end_time": "2023-11-07T12:12:07.382557",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.336366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0662, -0.0831,  0.0020,  ..., -0.0755, -0.1310, -0.0358],\n",
      "         [-0.0500,  0.0506, -0.0379,  ..., -0.1019, -0.1485, -0.0841],\n",
      "         [-0.0568,  0.0297, -0.0643,  ..., -0.0936, -0.1314,  0.0077],\n",
      "         ...,\n",
      "         [-0.0688,  0.0733, -0.0078,  ..., -0.0790, -0.0736, -0.0299],\n",
      "         [-0.0882, -0.0383, -0.0683,  ..., -0.0849, -0.1354,  0.0153],\n",
      "         [-0.0568,  0.0297, -0.0643,  ..., -0.0936, -0.1314,  0.0077]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Sample Usage: Multi-Head Attention\n",
    "\n",
    "'''\n",
    "\n",
    "# Every time will be different due to randomised weights\n",
    "multihead_attn = multiHeadAttention(config) # initialisation with config\n",
    "attn_output = multihead_attn(inputs_embeds) # forward by inputting embedding vectors (one for each token)\n",
    "\n",
    "# Attention output (attention weights matrix x vector weights concat)\n",
    "print(attn_output)\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2aebb7",
   "metadata": {
    "papermill": {
     "duration": 0.01628,
     "end_time": "2023-11-07T12:12:07.415605",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.399325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>4 |</span></b> <b>FEED FORWARD LAYER</b></div>\n",
    "\n",
    "**position-wise feed-forward layer**\n",
    "\n",
    "The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">feed-forward</mark>** sublayer in the encoder & decoder\n",
    "- **two layer fully connected neural network**\n",
    "\n",
    "\n",
    "However, instead of processing the whole sequence of embedding as a single vector, \n",
    "- it **processes each embedding** independently\n",
    "- Also see it referred to as a Conv1D with a kernel size of 1 (people with a CV background)\n",
    "\n",
    "\n",
    "The **hidden size** of the **1st layer = 4x size of the embeddings** & **GELU activation function**\n",
    "- Place where most of the capacity & memorization is hypothesized to happen\n",
    "- It is most often scaled, when scaling up the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0da5c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.450016Z",
     "iopub.status.busy": "2023-11-07T12:12:07.449132Z",
     "iopub.status.idle": "2023-11-07T12:12:07.454841Z",
     "shell.execute_reply": "2023-11-07T12:12:07.454256Z"
    },
    "papermill": {
     "duration": 0.024374,
     "end_time": "2023-11-07T12:12:07.456328",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.431954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class feedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # define layer operations input x\n",
    "        \n",
    "    def forward(self, x):    # note must be forward\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b36e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.489862Z",
     "iopub.status.busy": "2023-11-07T12:12:07.489589Z",
     "iopub.status.idle": "2023-11-07T12:12:07.544434Z",
     "shell.execute_reply": "2023-11-07T12:12:07.543744Z"
    },
    "papermill": {
     "duration": 0.073582,
     "end_time": "2023-11-07T12:12:07.546107",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.472525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedForward(\n",
      "  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0489,  0.0626,  0.0695,  ..., -0.0066, -0.0000, -0.0140],\n",
       "         [ 0.0520,  0.0506,  0.0638,  ..., -0.0022, -0.0000, -0.0243],\n",
       "         [ 0.0455,  0.0482,  0.0706,  ..., -0.0121, -0.0302, -0.0320],\n",
       "         ...,\n",
       "         [ 0.0393,  0.0534,  0.0690,  ..., -0.0027, -0.0427, -0.0256],\n",
       "         [ 0.0459,  0.0553,  0.0588,  ..., -0.0135, -0.0292, -0.0136],\n",
       "         [ 0.0455,  0.0482,  0.0706,  ..., -0.0000, -0.0000, -0.0320]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initailise feedforward layer\n",
    "feed_forward = feedForward(config)              # initialise \n",
    "print(feed_forward,'\\n')\n",
    "\n",
    "# requires config & attn_outputs outputs\n",
    "ff_outputs = feed_forward(attn_output) # forward operation\n",
    "ff_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fc03c",
   "metadata": {
    "papermill": {
     "duration": 0.016237,
     "end_time": "2023-11-07T12:12:07.579139",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.562902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>5 |</span></b> <b>NORMALISATION LAYERS</b></div>\n",
    "\n",
    "Transformer architecture also uses **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">layer normalisation</mark>** & **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">skip connections</mark>**\n",
    "- **normalisation** - normalises batch input to have zero mean & unit variance\n",
    "- **skip connections** - pass a tensor to the next level of the model w/o processing & adding it to the processed tensor\n",
    "\n",
    "Two main approaches, when it comes to normalisation layer placement in decoder, encoder:\n",
    "- **post layer** normalisation (transformer paper, layer normalisation b/w skip connections)\n",
    "- **pre layer** normalisation \n",
    "\n",
    "<br>\n",
    "\n",
    "| `post-layer` normalisation |  `pre-layer` normalisation in literature |\n",
    "| - | - |\n",
    "| Arrangement is tricky to train from scractch, as the gradients can diverge |  Most often found arrangement\n",
    "| Used with LR warm up (learning rate gradually increased, from small value to some maximum value during training) | Places layer normalization within the span of the skip connection |\n",
    "|  | Tends to be much more stable during training, and it does not usually require any learning rate warm-up |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc50d01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.613606Z",
     "iopub.status.busy": "2023-11-07T12:12:07.612856Z",
     "iopub.status.idle": "2023-11-07T12:12:07.618561Z",
     "shell.execute_reply": "2023-11-07T12:12:07.617989Z"
    },
    "papermill": {
     "duration": 0.024708,
     "end_time": "2023-11-07T12:12:07.620134",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.595426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class encoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = multiHeadAttention(config)    # multihead attention layer \n",
    "        self.feed_forward = feedForward(config)        # feed forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer norm. to hidden state, copy input into query, key, value\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcecb13b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.655181Z",
     "iopub.status.busy": "2023-11-07T12:12:07.654374Z",
     "iopub.status.idle": "2023-11-07T12:12:07.719063Z",
     "shell.execute_reply": "2023-11-07T12:12:07.718146Z"
    },
    "papermill": {
     "duration": 0.08428,
     "end_time": "2023-11-07T12:12:07.721179",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.636899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoderLayer(\n",
      "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): multiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-11): 12 x Attention(\n",
      "        (q): Linear(in_features=768, out_features=64, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=64, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (feed_forward): feedForward(\n",
      "    (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ") \n",
      "\n",
      "input torch.Size([1, 10, 768])\n",
      "output torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Transformer layer output\n",
    "encoder_layer = encoderLayer(config) # initialise encoder layer\n",
    "print(encoder_layer,'\\n')\n",
    "\n",
    "print('input',inputs_embeds.shape) \n",
    "print('output',encoder_layer(inputs_embeds).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232eac0",
   "metadata": {
    "papermill": {
     "duration": 0.01686,
     "end_time": "2023-11-07T12:12:07.754870",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.738010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is an issue with the way we set up the **encoder layers** (which uses just embedding inputs)\n",
    "- they are totally **invariant to the position of the tokens**\n",
    "- Multi-head attention layer is effectively a weighted sum, the **information on token position is lost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ed26c",
   "metadata": {
    "papermill": {
     "duration": 0.016274,
     "end_time": "2023-11-07T12:12:07.787762",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.771488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>6 |</span></b> <b>POSITIONAL EMBEDDINGS</b></div>\n",
    "\n",
    "Let's incorporate positional information using **positional embeddings**\n",
    "\n",
    "\n",
    "**positional embeddings** are based on idea:\n",
    "  - Modify the **token embeddings** with a **position-dependent pattern** of values arranged in a vector\n",
    "  \n",
    "  \n",
    "If the pattern is characteristic for each position\n",
    "- the **attention heads** and **feed-forward layers** in each stack can learn to incorporate positional information into their transformations\n",
    "\n",
    "\n",
    "\n",
    "- There are several ways to achieve this, and one of the most popular approaches is to use a `learnable pattern`\n",
    "- This works exactly the same way as the token embeddings, but using the **position index** instead of the **token identifier** (from vocabulary dictionary) as input\n",
    "- An efficient way of encoding the positions of tokens is learned during pretraining\n",
    "\n",
    "Creating Custom `Embedding` class\n",
    "\n",
    "Letâ€™s create a custom Embeddings module (**token embeddings + positional embeddings**)\n",
    " - That combines a token embedding layer that projects the input_ids to a dense hidden state \n",
    " - Together with the positional embedding that does the same for position_ids\n",
    " - The resulting embedding is simply the **sum of both embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "436fdf8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.823932Z",
     "iopub.status.busy": "2023-11-07T12:12:07.823592Z",
     "iopub.status.idle": "2023-11-07T12:12:07.831753Z",
     "shell.execute_reply": "2023-11-07T12:12:07.830873Z"
    },
    "papermill": {
     "duration": 0.02821,
     "end_time": "2023-11-07T12:12:07.833862",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.805652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Token + Position Embedding \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class tpEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding layer\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        \n",
    "        # positional embedding layer\n",
    "        # config.max_position_embeddings -> max number of positions in text 512 (tokens)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1) # number of tokens\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)[None,:] # range(0,9)\n",
    "        \n",
    "        # tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931]])\n",
    "        # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "        \n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Add normalisation & dropout layers\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1eb5779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:07.870017Z",
     "iopub.status.busy": "2023-11-07T12:12:07.869347Z",
     "iopub.status.idle": "2023-11-07T12:12:08.024643Z",
     "shell.execute_reply": "2023-11-07T12:12:08.023751Z"
    },
    "papermill": {
     "duration": 0.175058,
     "end_time": "2023-11-07T12:12:08.026668",
     "exception": false,
     "start_time": "2023-11-07T12:12:07.851610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0000, -0.0000,  ...,  1.3367, -1.0344,  0.6113],\n",
       "         [ 0.0000,  0.9846,  1.7784,  ..., -1.6795, -1.3171, -0.0000],\n",
       "         [ 4.6805, -0.0000,  0.7242,  ...,  0.8793,  0.1183,  0.0000],\n",
       "         ...,\n",
       "         [ 2.4357,  1.7012, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.9958,  0.0000,  ..., -0.3156,  1.0391, -1.5924],\n",
       "         [ 1.5081, -2.0370,  1.8315,  ..., -0.4919,  0.0000, -2.9875]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token and Position Embeddings\n",
    "embedding_layer = tpEmbedding(config)\n",
    "embedding_layer(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89b398b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931,  1997]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2413",
   "metadata": {
    "papermill": {
     "duration": 0.016522,
     "end_time": "2023-11-07T12:12:08.060364",
     "exception": false,
     "start_time": "2023-11-07T12:12:08.043842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>7 |</span></b> <b>PUTTING IT ALL TOGETHER</b></div>\n",
    "\n",
    "- Constructing the Transformer **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">encoder</mark>**, combining the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Embedding</mark>** and **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Encoder</mark>**  layers\n",
    "- We utilise both **token** & **positional** embeddings using `tpEmbedding`\n",
    "- For a given number of heads, we store `encoderLayer`, which contains the **attention** & **feed-forward** layers (which are our layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6f1e105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:08.094928Z",
     "iopub.status.busy": "2023-11-07T12:12:08.094587Z",
     "iopub.status.idle": "2023-11-07T12:12:08.100489Z",
     "shell.execute_reply": "2023-11-07T12:12:08.099664Z"
    },
    "papermill": {
     "duration": 0.024909,
     "end_time": "2023-11-07T12:12:08.102005",
     "exception": false,
     "start_time": "2023-11-07T12:12:08.077096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full transformer encoder combining the `Embedding` with the ``Embedding` ` layers\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):       \n",
    "        super().__init__()\n",
    "        \n",
    "        # token & positional embedding layer\n",
    "        self.embeddings = tpEmbedding(config)\n",
    "        \n",
    "        # attention & forward feed layer \n",
    "        self.layers = nn.ModuleList([encoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embeddings layer output\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # cycle through all heads\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2dd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bbadb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:08.136166Z",
     "iopub.status.busy": "2023-11-07T12:12:08.135908Z",
     "iopub.status.idle": "2023-11-07T12:12:08.948737Z",
     "shell.execute_reply": "2023-11-07T12:12:08.948016Z"
    },
    "papermill": {
     "duration": 0.831712,
     "end_time": "2023-11-07T12:12:08.950315",
     "exception": false,
     "start_time": "2023-11-07T12:12:08.118603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m      9\u001b[0m config\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 10\u001b[0m encoder_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerClassifier\u001b[49m(config)\n\u001b[1;32m     12\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/multi-qa-mpnet-base-dot-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_ckpt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "config.num_labels = 5\n",
    "encoder_classifier = TransformerClassifier(config)\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = list(pd.read_csv(\"SEC-CompanyTicker.csv\",index_col=0).companyName[:10])\n",
    "\n",
    "# Tokenise input (text)\n",
    "bert_embeddings = tokenizer(data, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   padding=True,\n",
    "                   add_special_tokens=False) # don't use pad, sep tokens\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "# Print the same output multiple times\n",
    "for _ in range(1000):\n",
    "    if count % 100 == 0: \n",
    "        print(count)\n",
    "    # Use the transformer to generate output\n",
    "    output = encoder_classifier(bert_embeddings.input_ids).cpu().detach().numpy()\n",
    "    count += 1\n",
    "output\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bcdcea43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.125596  , -0.36863506, -2.3599467 ,  0.84576607,  1.0254472 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenise input (text)\n",
    "q = tokenizer([\"Apple\"], \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   padding=True,\n",
    "                   add_special_tokens=False) # don't use pad, sep tokens\n",
    "\n",
    "\n",
    "xq = encoder_classifier(q.input_ids).cpu().detach().numpy()\n",
    "xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "babc008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 9, 8]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "class Faiss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def faiss(self,xb):\n",
    "        d = xb[0].size\n",
    "        M = 32\n",
    "        index = faiss.IndexHNSWFlat(d, M)            \n",
    "        index.hnsw.efConstruction = 40         # Setting the value for efConstruction.\n",
    "        index.hnsw.efSearch = 16               # Setting the value for efSearch.\n",
    "        index.add(xb)\n",
    "        return index\n",
    "    \n",
    "    def query(self,index,xq,k=3):\n",
    "        D, I = index.search(xq, k)   \n",
    "        return D, I\n",
    "\n",
    "xb = output\n",
    "index = Faiss().faiss(xb)\n",
    "D,I = Faiss().query(index,xq)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "702bccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.token_embeddings.weight: False\n",
      "embeddings.position_embeddings.weight: False\n",
      "embeddings.norm.weight: False\n",
      "embeddings.norm.bias: False\n",
      "layers.0.norm1.weight: False\n",
      "layers.0.norm1.bias: False\n",
      "layers.0.norm2.weight: False\n",
      "layers.0.norm2.bias: False\n",
      "layers.0.attention.heads.0.q.weight: False\n",
      "layers.0.attention.heads.0.q.bias: False\n",
      "layers.0.attention.heads.0.k.weight: False\n",
      "layers.0.attention.heads.0.k.bias: False\n",
      "layers.0.attention.heads.0.v.weight: False\n",
      "layers.0.attention.heads.0.v.bias: False\n",
      "layers.0.attention.heads.1.q.weight: False\n",
      "layers.0.attention.heads.1.q.bias: False\n",
      "layers.0.attention.heads.1.k.weight: False\n",
      "layers.0.attention.heads.1.k.bias: False\n",
      "layers.0.attention.heads.1.v.weight: False\n",
      "layers.0.attention.heads.1.v.bias: False\n",
      "layers.0.attention.heads.2.q.weight: False\n",
      "layers.0.attention.heads.2.q.bias: False\n",
      "layers.0.attention.heads.2.k.weight: False\n",
      "layers.0.attention.heads.2.k.bias: False\n",
      "layers.0.attention.heads.2.v.weight: False\n",
      "layers.0.attention.heads.2.v.bias: False\n",
      "layers.0.attention.heads.3.q.weight: False\n",
      "layers.0.attention.heads.3.q.bias: False\n",
      "layers.0.attention.heads.3.k.weight: False\n",
      "layers.0.attention.heads.3.k.bias: False\n",
      "layers.0.attention.heads.3.v.weight: False\n",
      "layers.0.attention.heads.3.v.bias: False\n",
      "layers.0.attention.heads.4.q.weight: False\n",
      "layers.0.attention.heads.4.q.bias: False\n",
      "layers.0.attention.heads.4.k.weight: False\n",
      "layers.0.attention.heads.4.k.bias: False\n",
      "layers.0.attention.heads.4.v.weight: False\n",
      "layers.0.attention.heads.4.v.bias: False\n",
      "layers.0.attention.heads.5.q.weight: False\n",
      "layers.0.attention.heads.5.q.bias: False\n",
      "layers.0.attention.heads.5.k.weight: False\n",
      "layers.0.attention.heads.5.k.bias: False\n",
      "layers.0.attention.heads.5.v.weight: False\n",
      "layers.0.attention.heads.5.v.bias: False\n",
      "layers.0.attention.heads.6.q.weight: False\n",
      "layers.0.attention.heads.6.q.bias: False\n",
      "layers.0.attention.heads.6.k.weight: False\n",
      "layers.0.attention.heads.6.k.bias: False\n",
      "layers.0.attention.heads.6.v.weight: False\n",
      "layers.0.attention.heads.6.v.bias: False\n",
      "layers.0.attention.heads.7.q.weight: False\n",
      "layers.0.attention.heads.7.q.bias: False\n",
      "layers.0.attention.heads.7.k.weight: False\n",
      "layers.0.attention.heads.7.k.bias: False\n",
      "layers.0.attention.heads.7.v.weight: False\n",
      "layers.0.attention.heads.7.v.bias: False\n",
      "layers.0.attention.heads.8.q.weight: False\n",
      "layers.0.attention.heads.8.q.bias: False\n",
      "layers.0.attention.heads.8.k.weight: False\n",
      "layers.0.attention.heads.8.k.bias: False\n",
      "layers.0.attention.heads.8.v.weight: False\n",
      "layers.0.attention.heads.8.v.bias: False\n",
      "layers.0.attention.heads.9.q.weight: False\n",
      "layers.0.attention.heads.9.q.bias: False\n",
      "layers.0.attention.heads.9.k.weight: False\n",
      "layers.0.attention.heads.9.k.bias: False\n",
      "layers.0.attention.heads.9.v.weight: False\n",
      "layers.0.attention.heads.9.v.bias: False\n",
      "layers.0.attention.heads.10.q.weight: False\n",
      "layers.0.attention.heads.10.q.bias: False\n",
      "layers.0.attention.heads.10.k.weight: False\n",
      "layers.0.attention.heads.10.k.bias: False\n",
      "layers.0.attention.heads.10.v.weight: False\n",
      "layers.0.attention.heads.10.v.bias: False\n",
      "layers.0.attention.heads.11.q.weight: False\n",
      "layers.0.attention.heads.11.q.bias: False\n",
      "layers.0.attention.heads.11.k.weight: False\n",
      "layers.0.attention.heads.11.k.bias: False\n",
      "layers.0.attention.heads.11.v.weight: False\n",
      "layers.0.attention.heads.11.v.bias: False\n",
      "layers.0.attention.out_linear.weight: False\n",
      "layers.0.attention.out_linear.bias: False\n",
      "layers.0.feed_forward.linear1.weight: False\n",
      "layers.0.feed_forward.linear1.bias: False\n",
      "layers.0.feed_forward.linear2.weight: False\n",
      "layers.0.feed_forward.linear2.bias: False\n",
      "layers.1.norm1.weight: False\n",
      "layers.1.norm1.bias: False\n",
      "layers.1.norm2.weight: False\n",
      "layers.1.norm2.bias: False\n",
      "layers.1.attention.heads.0.q.weight: False\n",
      "layers.1.attention.heads.0.q.bias: False\n",
      "layers.1.attention.heads.0.k.weight: False\n",
      "layers.1.attention.heads.0.k.bias: False\n",
      "layers.1.attention.heads.0.v.weight: False\n",
      "layers.1.attention.heads.0.v.bias: False\n",
      "layers.1.attention.heads.1.q.weight: False\n",
      "layers.1.attention.heads.1.q.bias: False\n",
      "layers.1.attention.heads.1.k.weight: False\n",
      "layers.1.attention.heads.1.k.bias: False\n",
      "layers.1.attention.heads.1.v.weight: False\n",
      "layers.1.attention.heads.1.v.bias: False\n",
      "layers.1.attention.heads.2.q.weight: False\n",
      "layers.1.attention.heads.2.q.bias: False\n",
      "layers.1.attention.heads.2.k.weight: False\n",
      "layers.1.attention.heads.2.k.bias: False\n",
      "layers.1.attention.heads.2.v.weight: False\n",
      "layers.1.attention.heads.2.v.bias: False\n",
      "layers.1.attention.heads.3.q.weight: False\n",
      "layers.1.attention.heads.3.q.bias: False\n",
      "layers.1.attention.heads.3.k.weight: False\n",
      "layers.1.attention.heads.3.k.bias: False\n",
      "layers.1.attention.heads.3.v.weight: False\n",
      "layers.1.attention.heads.3.v.bias: False\n",
      "layers.1.attention.heads.4.q.weight: False\n",
      "layers.1.attention.heads.4.q.bias: False\n",
      "layers.1.attention.heads.4.k.weight: False\n",
      "layers.1.attention.heads.4.k.bias: False\n",
      "layers.1.attention.heads.4.v.weight: False\n",
      "layers.1.attention.heads.4.v.bias: False\n",
      "layers.1.attention.heads.5.q.weight: False\n",
      "layers.1.attention.heads.5.q.bias: False\n",
      "layers.1.attention.heads.5.k.weight: False\n",
      "layers.1.attention.heads.5.k.bias: False\n",
      "layers.1.attention.heads.5.v.weight: False\n",
      "layers.1.attention.heads.5.v.bias: False\n",
      "layers.1.attention.heads.6.q.weight: False\n",
      "layers.1.attention.heads.6.q.bias: False\n",
      "layers.1.attention.heads.6.k.weight: False\n",
      "layers.1.attention.heads.6.k.bias: False\n",
      "layers.1.attention.heads.6.v.weight: False\n",
      "layers.1.attention.heads.6.v.bias: False\n",
      "layers.1.attention.heads.7.q.weight: False\n",
      "layers.1.attention.heads.7.q.bias: False\n",
      "layers.1.attention.heads.7.k.weight: False\n",
      "layers.1.attention.heads.7.k.bias: False\n",
      "layers.1.attention.heads.7.v.weight: False\n",
      "layers.1.attention.heads.7.v.bias: False\n",
      "layers.1.attention.heads.8.q.weight: False\n",
      "layers.1.attention.heads.8.q.bias: False\n",
      "layers.1.attention.heads.8.k.weight: False\n",
      "layers.1.attention.heads.8.k.bias: False\n",
      "layers.1.attention.heads.8.v.weight: False\n",
      "layers.1.attention.heads.8.v.bias: False\n",
      "layers.1.attention.heads.9.q.weight: False\n",
      "layers.1.attention.heads.9.q.bias: False\n",
      "layers.1.attention.heads.9.k.weight: False\n",
      "layers.1.attention.heads.9.k.bias: False\n",
      "layers.1.attention.heads.9.v.weight: False\n",
      "layers.1.attention.heads.9.v.bias: False\n",
      "layers.1.attention.heads.10.q.weight: False\n",
      "layers.1.attention.heads.10.q.bias: False\n",
      "layers.1.attention.heads.10.k.weight: False\n",
      "layers.1.attention.heads.10.k.bias: False\n",
      "layers.1.attention.heads.10.v.weight: False\n",
      "layers.1.attention.heads.10.v.bias: False\n",
      "layers.1.attention.heads.11.q.weight: False\n",
      "layers.1.attention.heads.11.q.bias: False\n",
      "layers.1.attention.heads.11.k.weight: False\n",
      "layers.1.attention.heads.11.k.bias: False\n",
      "layers.1.attention.heads.11.v.weight: False\n",
      "layers.1.attention.heads.11.v.bias: False\n",
      "layers.1.attention.out_linear.weight: False\n",
      "layers.1.attention.out_linear.bias: False\n",
      "layers.1.feed_forward.linear1.weight: False\n",
      "layers.1.feed_forward.linear1.bias: False\n",
      "layers.1.feed_forward.linear2.weight: False\n",
      "layers.1.feed_forward.linear2.bias: False\n",
      "layers.2.norm1.weight: False\n",
      "layers.2.norm1.bias: False\n",
      "layers.2.norm2.weight: False\n",
      "layers.2.norm2.bias: False\n",
      "layers.2.attention.heads.0.q.weight: False\n",
      "layers.2.attention.heads.0.q.bias: False\n",
      "layers.2.attention.heads.0.k.weight: False\n",
      "layers.2.attention.heads.0.k.bias: False\n",
      "layers.2.attention.heads.0.v.weight: False\n",
      "layers.2.attention.heads.0.v.bias: False\n",
      "layers.2.attention.heads.1.q.weight: False\n",
      "layers.2.attention.heads.1.q.bias: False\n",
      "layers.2.attention.heads.1.k.weight: False\n",
      "layers.2.attention.heads.1.k.bias: False\n",
      "layers.2.attention.heads.1.v.weight: False\n",
      "layers.2.attention.heads.1.v.bias: False\n",
      "layers.2.attention.heads.2.q.weight: False\n",
      "layers.2.attention.heads.2.q.bias: False\n",
      "layers.2.attention.heads.2.k.weight: False\n",
      "layers.2.attention.heads.2.k.bias: False\n",
      "layers.2.attention.heads.2.v.weight: False\n",
      "layers.2.attention.heads.2.v.bias: False\n",
      "layers.2.attention.heads.3.q.weight: False\n",
      "layers.2.attention.heads.3.q.bias: False\n",
      "layers.2.attention.heads.3.k.weight: False\n",
      "layers.2.attention.heads.3.k.bias: False\n",
      "layers.2.attention.heads.3.v.weight: False\n",
      "layers.2.attention.heads.3.v.bias: False\n",
      "layers.2.attention.heads.4.q.weight: False\n",
      "layers.2.attention.heads.4.q.bias: False\n",
      "layers.2.attention.heads.4.k.weight: False\n",
      "layers.2.attention.heads.4.k.bias: False\n",
      "layers.2.attention.heads.4.v.weight: False\n",
      "layers.2.attention.heads.4.v.bias: False\n",
      "layers.2.attention.heads.5.q.weight: False\n",
      "layers.2.attention.heads.5.q.bias: False\n",
      "layers.2.attention.heads.5.k.weight: False\n",
      "layers.2.attention.heads.5.k.bias: False\n",
      "layers.2.attention.heads.5.v.weight: False\n",
      "layers.2.attention.heads.5.v.bias: False\n",
      "layers.2.attention.heads.6.q.weight: False\n",
      "layers.2.attention.heads.6.q.bias: False\n",
      "layers.2.attention.heads.6.k.weight: False\n",
      "layers.2.attention.heads.6.k.bias: False\n",
      "layers.2.attention.heads.6.v.weight: False\n",
      "layers.2.attention.heads.6.v.bias: False\n",
      "layers.2.attention.heads.7.q.weight: False\n",
      "layers.2.attention.heads.7.q.bias: False\n",
      "layers.2.attention.heads.7.k.weight: False\n",
      "layers.2.attention.heads.7.k.bias: False\n",
      "layers.2.attention.heads.7.v.weight: False\n",
      "layers.2.attention.heads.7.v.bias: False\n",
      "layers.2.attention.heads.8.q.weight: False\n",
      "layers.2.attention.heads.8.q.bias: False\n",
      "layers.2.attention.heads.8.k.weight: False\n",
      "layers.2.attention.heads.8.k.bias: False\n",
      "layers.2.attention.heads.8.v.weight: False\n",
      "layers.2.attention.heads.8.v.bias: False\n",
      "layers.2.attention.heads.9.q.weight: False\n",
      "layers.2.attention.heads.9.q.bias: False\n",
      "layers.2.attention.heads.9.k.weight: False\n",
      "layers.2.attention.heads.9.k.bias: False\n",
      "layers.2.attention.heads.9.v.weight: False\n",
      "layers.2.attention.heads.9.v.bias: False\n",
      "layers.2.attention.heads.10.q.weight: False\n",
      "layers.2.attention.heads.10.q.bias: False\n",
      "layers.2.attention.heads.10.k.weight: False\n",
      "layers.2.attention.heads.10.k.bias: False\n",
      "layers.2.attention.heads.10.v.weight: False\n",
      "layers.2.attention.heads.10.v.bias: False\n",
      "layers.2.attention.heads.11.q.weight: False\n",
      "layers.2.attention.heads.11.q.bias: False\n",
      "layers.2.attention.heads.11.k.weight: False\n",
      "layers.2.attention.heads.11.k.bias: False\n",
      "layers.2.attention.heads.11.v.weight: False\n",
      "layers.2.attention.heads.11.v.bias: False\n",
      "layers.2.attention.out_linear.weight: False\n",
      "layers.2.attention.out_linear.bias: False\n",
      "layers.2.feed_forward.linear1.weight: False\n",
      "layers.2.feed_forward.linear1.bias: False\n",
      "layers.2.feed_forward.linear2.weight: False\n",
      "layers.2.feed_forward.linear2.bias: False\n",
      "layers.3.norm1.weight: False\n",
      "layers.3.norm1.bias: False\n",
      "layers.3.norm2.weight: False\n",
      "layers.3.norm2.bias: False\n",
      "layers.3.attention.heads.0.q.weight: False\n",
      "layers.3.attention.heads.0.q.bias: False\n",
      "layers.3.attention.heads.0.k.weight: False\n",
      "layers.3.attention.heads.0.k.bias: False\n",
      "layers.3.attention.heads.0.v.weight: False\n",
      "layers.3.attention.heads.0.v.bias: False\n",
      "layers.3.attention.heads.1.q.weight: False\n",
      "layers.3.attention.heads.1.q.bias: False\n",
      "layers.3.attention.heads.1.k.weight: False\n",
      "layers.3.attention.heads.1.k.bias: False\n",
      "layers.3.attention.heads.1.v.weight: False\n",
      "layers.3.attention.heads.1.v.bias: False\n",
      "layers.3.attention.heads.2.q.weight: False\n",
      "layers.3.attention.heads.2.q.bias: False\n",
      "layers.3.attention.heads.2.k.weight: False\n",
      "layers.3.attention.heads.2.k.bias: False\n",
      "layers.3.attention.heads.2.v.weight: False\n",
      "layers.3.attention.heads.2.v.bias: False\n",
      "layers.3.attention.heads.3.q.weight: False\n",
      "layers.3.attention.heads.3.q.bias: False\n",
      "layers.3.attention.heads.3.k.weight: False\n",
      "layers.3.attention.heads.3.k.bias: False\n",
      "layers.3.attention.heads.3.v.weight: False\n",
      "layers.3.attention.heads.3.v.bias: False\n",
      "layers.3.attention.heads.4.q.weight: False\n",
      "layers.3.attention.heads.4.q.bias: False\n",
      "layers.3.attention.heads.4.k.weight: False\n",
      "layers.3.attention.heads.4.k.bias: False\n",
      "layers.3.attention.heads.4.v.weight: False\n",
      "layers.3.attention.heads.4.v.bias: False\n",
      "layers.3.attention.heads.5.q.weight: False\n",
      "layers.3.attention.heads.5.q.bias: False\n",
      "layers.3.attention.heads.5.k.weight: False\n",
      "layers.3.attention.heads.5.k.bias: False\n",
      "layers.3.attention.heads.5.v.weight: False\n",
      "layers.3.attention.heads.5.v.bias: False\n",
      "layers.3.attention.heads.6.q.weight: False\n",
      "layers.3.attention.heads.6.q.bias: False\n",
      "layers.3.attention.heads.6.k.weight: False\n",
      "layers.3.attention.heads.6.k.bias: False\n",
      "layers.3.attention.heads.6.v.weight: False\n",
      "layers.3.attention.heads.6.v.bias: False\n",
      "layers.3.attention.heads.7.q.weight: False\n",
      "layers.3.attention.heads.7.q.bias: False\n",
      "layers.3.attention.heads.7.k.weight: False\n",
      "layers.3.attention.heads.7.k.bias: False\n",
      "layers.3.attention.heads.7.v.weight: False\n",
      "layers.3.attention.heads.7.v.bias: False\n",
      "layers.3.attention.heads.8.q.weight: False\n",
      "layers.3.attention.heads.8.q.bias: False\n",
      "layers.3.attention.heads.8.k.weight: False\n",
      "layers.3.attention.heads.8.k.bias: False\n",
      "layers.3.attention.heads.8.v.weight: False\n",
      "layers.3.attention.heads.8.v.bias: False\n",
      "layers.3.attention.heads.9.q.weight: False\n",
      "layers.3.attention.heads.9.q.bias: False\n",
      "layers.3.attention.heads.9.k.weight: False\n",
      "layers.3.attention.heads.9.k.bias: False\n",
      "layers.3.attention.heads.9.v.weight: False\n",
      "layers.3.attention.heads.9.v.bias: False\n",
      "layers.3.attention.heads.10.q.weight: False\n",
      "layers.3.attention.heads.10.q.bias: False\n",
      "layers.3.attention.heads.10.k.weight: False\n",
      "layers.3.attention.heads.10.k.bias: False\n",
      "layers.3.attention.heads.10.v.weight: False\n",
      "layers.3.attention.heads.10.v.bias: False\n",
      "layers.3.attention.heads.11.q.weight: False\n",
      "layers.3.attention.heads.11.q.bias: False\n",
      "layers.3.attention.heads.11.k.weight: False\n",
      "layers.3.attention.heads.11.k.bias: False\n",
      "layers.3.attention.heads.11.v.weight: False\n",
      "layers.3.attention.heads.11.v.bias: False\n",
      "layers.3.attention.out_linear.weight: False\n",
      "layers.3.attention.out_linear.bias: False\n",
      "layers.3.feed_forward.linear1.weight: False\n",
      "layers.3.feed_forward.linear1.bias: False\n",
      "layers.3.feed_forward.linear2.weight: False\n",
      "layers.3.feed_forward.linear2.bias: False\n",
      "layers.4.norm1.weight: False\n",
      "layers.4.norm1.bias: False\n",
      "layers.4.norm2.weight: False\n",
      "layers.4.norm2.bias: False\n",
      "layers.4.attention.heads.0.q.weight: False\n",
      "layers.4.attention.heads.0.q.bias: False\n",
      "layers.4.attention.heads.0.k.weight: False\n",
      "layers.4.attention.heads.0.k.bias: False\n",
      "layers.4.attention.heads.0.v.weight: False\n",
      "layers.4.attention.heads.0.v.bias: False\n",
      "layers.4.attention.heads.1.q.weight: False\n",
      "layers.4.attention.heads.1.q.bias: False\n",
      "layers.4.attention.heads.1.k.weight: False\n",
      "layers.4.attention.heads.1.k.bias: False\n",
      "layers.4.attention.heads.1.v.weight: False\n",
      "layers.4.attention.heads.1.v.bias: False\n",
      "layers.4.attention.heads.2.q.weight: False\n",
      "layers.4.attention.heads.2.q.bias: False\n",
      "layers.4.attention.heads.2.k.weight: False\n",
      "layers.4.attention.heads.2.k.bias: False\n",
      "layers.4.attention.heads.2.v.weight: False\n",
      "layers.4.attention.heads.2.v.bias: False\n",
      "layers.4.attention.heads.3.q.weight: False\n",
      "layers.4.attention.heads.3.q.bias: False\n",
      "layers.4.attention.heads.3.k.weight: False\n",
      "layers.4.attention.heads.3.k.bias: False\n",
      "layers.4.attention.heads.3.v.weight: False\n",
      "layers.4.attention.heads.3.v.bias: False\n",
      "layers.4.attention.heads.4.q.weight: False\n",
      "layers.4.attention.heads.4.q.bias: False\n",
      "layers.4.attention.heads.4.k.weight: False\n",
      "layers.4.attention.heads.4.k.bias: False\n",
      "layers.4.attention.heads.4.v.weight: False\n",
      "layers.4.attention.heads.4.v.bias: False\n",
      "layers.4.attention.heads.5.q.weight: False\n",
      "layers.4.attention.heads.5.q.bias: False\n",
      "layers.4.attention.heads.5.k.weight: False\n",
      "layers.4.attention.heads.5.k.bias: False\n",
      "layers.4.attention.heads.5.v.weight: False\n",
      "layers.4.attention.heads.5.v.bias: False\n",
      "layers.4.attention.heads.6.q.weight: False\n",
      "layers.4.attention.heads.6.q.bias: False\n",
      "layers.4.attention.heads.6.k.weight: False\n",
      "layers.4.attention.heads.6.k.bias: False\n",
      "layers.4.attention.heads.6.v.weight: False\n",
      "layers.4.attention.heads.6.v.bias: False\n",
      "layers.4.attention.heads.7.q.weight: False\n",
      "layers.4.attention.heads.7.q.bias: False\n",
      "layers.4.attention.heads.7.k.weight: False\n",
      "layers.4.attention.heads.7.k.bias: False\n",
      "layers.4.attention.heads.7.v.weight: False\n",
      "layers.4.attention.heads.7.v.bias: False\n",
      "layers.4.attention.heads.8.q.weight: False\n",
      "layers.4.attention.heads.8.q.bias: False\n",
      "layers.4.attention.heads.8.k.weight: False\n",
      "layers.4.attention.heads.8.k.bias: False\n",
      "layers.4.attention.heads.8.v.weight: False\n",
      "layers.4.attention.heads.8.v.bias: False\n",
      "layers.4.attention.heads.9.q.weight: False\n",
      "layers.4.attention.heads.9.q.bias: False\n",
      "layers.4.attention.heads.9.k.weight: False\n",
      "layers.4.attention.heads.9.k.bias: False\n",
      "layers.4.attention.heads.9.v.weight: False\n",
      "layers.4.attention.heads.9.v.bias: False\n",
      "layers.4.attention.heads.10.q.weight: False\n",
      "layers.4.attention.heads.10.q.bias: False\n",
      "layers.4.attention.heads.10.k.weight: False\n",
      "layers.4.attention.heads.10.k.bias: False\n",
      "layers.4.attention.heads.10.v.weight: False\n",
      "layers.4.attention.heads.10.v.bias: False\n",
      "layers.4.attention.heads.11.q.weight: False\n",
      "layers.4.attention.heads.11.q.bias: False\n",
      "layers.4.attention.heads.11.k.weight: False\n",
      "layers.4.attention.heads.11.k.bias: False\n",
      "layers.4.attention.heads.11.v.weight: False\n",
      "layers.4.attention.heads.11.v.bias: False\n",
      "layers.4.attention.out_linear.weight: False\n",
      "layers.4.attention.out_linear.bias: False\n",
      "layers.4.feed_forward.linear1.weight: False\n",
      "layers.4.feed_forward.linear1.bias: False\n",
      "layers.4.feed_forward.linear2.weight: False\n",
      "layers.4.feed_forward.linear2.bias: False\n",
      "layers.5.norm1.weight: False\n",
      "layers.5.norm1.bias: False\n",
      "layers.5.norm2.weight: False\n",
      "layers.5.norm2.bias: False\n",
      "layers.5.attention.heads.0.q.weight: False\n",
      "layers.5.attention.heads.0.q.bias: False\n",
      "layers.5.attention.heads.0.k.weight: False\n",
      "layers.5.attention.heads.0.k.bias: False\n",
      "layers.5.attention.heads.0.v.weight: False\n",
      "layers.5.attention.heads.0.v.bias: False\n",
      "layers.5.attention.heads.1.q.weight: False\n",
      "layers.5.attention.heads.1.q.bias: False\n",
      "layers.5.attention.heads.1.k.weight: False\n",
      "layers.5.attention.heads.1.k.bias: False\n",
      "layers.5.attention.heads.1.v.weight: False\n",
      "layers.5.attention.heads.1.v.bias: False\n",
      "layers.5.attention.heads.2.q.weight: False\n",
      "layers.5.attention.heads.2.q.bias: False\n",
      "layers.5.attention.heads.2.k.weight: False\n",
      "layers.5.attention.heads.2.k.bias: False\n",
      "layers.5.attention.heads.2.v.weight: False\n",
      "layers.5.attention.heads.2.v.bias: False\n",
      "layers.5.attention.heads.3.q.weight: False\n",
      "layers.5.attention.heads.3.q.bias: False\n",
      "layers.5.attention.heads.3.k.weight: False\n",
      "layers.5.attention.heads.3.k.bias: False\n",
      "layers.5.attention.heads.3.v.weight: False\n",
      "layers.5.attention.heads.3.v.bias: False\n",
      "layers.5.attention.heads.4.q.weight: False\n",
      "layers.5.attention.heads.4.q.bias: False\n",
      "layers.5.attention.heads.4.k.weight: False\n",
      "layers.5.attention.heads.4.k.bias: False\n",
      "layers.5.attention.heads.4.v.weight: False\n",
      "layers.5.attention.heads.4.v.bias: False\n",
      "layers.5.attention.heads.5.q.weight: False\n",
      "layers.5.attention.heads.5.q.bias: False\n",
      "layers.5.attention.heads.5.k.weight: False\n",
      "layers.5.attention.heads.5.k.bias: False\n",
      "layers.5.attention.heads.5.v.weight: False\n",
      "layers.5.attention.heads.5.v.bias: False\n",
      "layers.5.attention.heads.6.q.weight: False\n",
      "layers.5.attention.heads.6.q.bias: False\n",
      "layers.5.attention.heads.6.k.weight: False\n",
      "layers.5.attention.heads.6.k.bias: False\n",
      "layers.5.attention.heads.6.v.weight: False\n",
      "layers.5.attention.heads.6.v.bias: False\n",
      "layers.5.attention.heads.7.q.weight: False\n",
      "layers.5.attention.heads.7.q.bias: False\n",
      "layers.5.attention.heads.7.k.weight: False\n",
      "layers.5.attention.heads.7.k.bias: False\n",
      "layers.5.attention.heads.7.v.weight: False\n",
      "layers.5.attention.heads.7.v.bias: False\n",
      "layers.5.attention.heads.8.q.weight: False\n",
      "layers.5.attention.heads.8.q.bias: False\n",
      "layers.5.attention.heads.8.k.weight: False\n",
      "layers.5.attention.heads.8.k.bias: False\n",
      "layers.5.attention.heads.8.v.weight: False\n",
      "layers.5.attention.heads.8.v.bias: False\n",
      "layers.5.attention.heads.9.q.weight: False\n",
      "layers.5.attention.heads.9.q.bias: False\n",
      "layers.5.attention.heads.9.k.weight: False\n",
      "layers.5.attention.heads.9.k.bias: False\n",
      "layers.5.attention.heads.9.v.weight: False\n",
      "layers.5.attention.heads.9.v.bias: False\n",
      "layers.5.attention.heads.10.q.weight: False\n",
      "layers.5.attention.heads.10.q.bias: False\n",
      "layers.5.attention.heads.10.k.weight: False\n",
      "layers.5.attention.heads.10.k.bias: False\n",
      "layers.5.attention.heads.10.v.weight: False\n",
      "layers.5.attention.heads.10.v.bias: False\n",
      "layers.5.attention.heads.11.q.weight: False\n",
      "layers.5.attention.heads.11.q.bias: False\n",
      "layers.5.attention.heads.11.k.weight: False\n",
      "layers.5.attention.heads.11.k.bias: False\n",
      "layers.5.attention.heads.11.v.weight: False\n",
      "layers.5.attention.heads.11.v.bias: False\n",
      "layers.5.attention.out_linear.weight: False\n",
      "layers.5.attention.out_linear.bias: False\n",
      "layers.5.feed_forward.linear1.weight: False\n",
      "layers.5.feed_forward.linear1.bias: False\n",
      "layers.5.feed_forward.linear2.weight: False\n",
      "layers.5.feed_forward.linear2.bias: False\n",
      "layers.6.norm1.weight: False\n",
      "layers.6.norm1.bias: False\n",
      "layers.6.norm2.weight: False\n",
      "layers.6.norm2.bias: False\n",
      "layers.6.attention.heads.0.q.weight: False\n",
      "layers.6.attention.heads.0.q.bias: False\n",
      "layers.6.attention.heads.0.k.weight: False\n",
      "layers.6.attention.heads.0.k.bias: False\n",
      "layers.6.attention.heads.0.v.weight: False\n",
      "layers.6.attention.heads.0.v.bias: False\n",
      "layers.6.attention.heads.1.q.weight: False\n",
      "layers.6.attention.heads.1.q.bias: False\n",
      "layers.6.attention.heads.1.k.weight: False\n",
      "layers.6.attention.heads.1.k.bias: False\n",
      "layers.6.attention.heads.1.v.weight: False\n",
      "layers.6.attention.heads.1.v.bias: False\n",
      "layers.6.attention.heads.2.q.weight: False\n",
      "layers.6.attention.heads.2.q.bias: False\n",
      "layers.6.attention.heads.2.k.weight: False\n",
      "layers.6.attention.heads.2.k.bias: False\n",
      "layers.6.attention.heads.2.v.weight: False\n",
      "layers.6.attention.heads.2.v.bias: False\n",
      "layers.6.attention.heads.3.q.weight: False\n",
      "layers.6.attention.heads.3.q.bias: False\n",
      "layers.6.attention.heads.3.k.weight: False\n",
      "layers.6.attention.heads.3.k.bias: False\n",
      "layers.6.attention.heads.3.v.weight: False\n",
      "layers.6.attention.heads.3.v.bias: False\n",
      "layers.6.attention.heads.4.q.weight: False\n",
      "layers.6.attention.heads.4.q.bias: False\n",
      "layers.6.attention.heads.4.k.weight: False\n",
      "layers.6.attention.heads.4.k.bias: False\n",
      "layers.6.attention.heads.4.v.weight: False\n",
      "layers.6.attention.heads.4.v.bias: False\n",
      "layers.6.attention.heads.5.q.weight: False\n",
      "layers.6.attention.heads.5.q.bias: False\n",
      "layers.6.attention.heads.5.k.weight: False\n",
      "layers.6.attention.heads.5.k.bias: False\n",
      "layers.6.attention.heads.5.v.weight: False\n",
      "layers.6.attention.heads.5.v.bias: False\n",
      "layers.6.attention.heads.6.q.weight: False\n",
      "layers.6.attention.heads.6.q.bias: False\n",
      "layers.6.attention.heads.6.k.weight: False\n",
      "layers.6.attention.heads.6.k.bias: False\n",
      "layers.6.attention.heads.6.v.weight: False\n",
      "layers.6.attention.heads.6.v.bias: False\n",
      "layers.6.attention.heads.7.q.weight: False\n",
      "layers.6.attention.heads.7.q.bias: False\n",
      "layers.6.attention.heads.7.k.weight: False\n",
      "layers.6.attention.heads.7.k.bias: False\n",
      "layers.6.attention.heads.7.v.weight: False\n",
      "layers.6.attention.heads.7.v.bias: False\n",
      "layers.6.attention.heads.8.q.weight: False\n",
      "layers.6.attention.heads.8.q.bias: False\n",
      "layers.6.attention.heads.8.k.weight: False\n",
      "layers.6.attention.heads.8.k.bias: False\n",
      "layers.6.attention.heads.8.v.weight: False\n",
      "layers.6.attention.heads.8.v.bias: False\n",
      "layers.6.attention.heads.9.q.weight: False\n",
      "layers.6.attention.heads.9.q.bias: False\n",
      "layers.6.attention.heads.9.k.weight: False\n",
      "layers.6.attention.heads.9.k.bias: False\n",
      "layers.6.attention.heads.9.v.weight: False\n",
      "layers.6.attention.heads.9.v.bias: False\n",
      "layers.6.attention.heads.10.q.weight: False\n",
      "layers.6.attention.heads.10.q.bias: False\n",
      "layers.6.attention.heads.10.k.weight: False\n",
      "layers.6.attention.heads.10.k.bias: False\n",
      "layers.6.attention.heads.10.v.weight: False\n",
      "layers.6.attention.heads.10.v.bias: False\n",
      "layers.6.attention.heads.11.q.weight: False\n",
      "layers.6.attention.heads.11.q.bias: False\n",
      "layers.6.attention.heads.11.k.weight: False\n",
      "layers.6.attention.heads.11.k.bias: False\n",
      "layers.6.attention.heads.11.v.weight: False\n",
      "layers.6.attention.heads.11.v.bias: False\n",
      "layers.6.attention.out_linear.weight: False\n",
      "layers.6.attention.out_linear.bias: False\n",
      "layers.6.feed_forward.linear1.weight: False\n",
      "layers.6.feed_forward.linear1.bias: False\n",
      "layers.6.feed_forward.linear2.weight: False\n",
      "layers.6.feed_forward.linear2.bias: False\n",
      "layers.7.norm1.weight: False\n",
      "layers.7.norm1.bias: False\n",
      "layers.7.norm2.weight: False\n",
      "layers.7.norm2.bias: False\n",
      "layers.7.attention.heads.0.q.weight: False\n",
      "layers.7.attention.heads.0.q.bias: False\n",
      "layers.7.attention.heads.0.k.weight: False\n",
      "layers.7.attention.heads.0.k.bias: False\n",
      "layers.7.attention.heads.0.v.weight: False\n",
      "layers.7.attention.heads.0.v.bias: False\n",
      "layers.7.attention.heads.1.q.weight: False\n",
      "layers.7.attention.heads.1.q.bias: False\n",
      "layers.7.attention.heads.1.k.weight: False\n",
      "layers.7.attention.heads.1.k.bias: False\n",
      "layers.7.attention.heads.1.v.weight: False\n",
      "layers.7.attention.heads.1.v.bias: False\n",
      "layers.7.attention.heads.2.q.weight: False\n",
      "layers.7.attention.heads.2.q.bias: False\n",
      "layers.7.attention.heads.2.k.weight: False\n",
      "layers.7.attention.heads.2.k.bias: False\n",
      "layers.7.attention.heads.2.v.weight: False\n",
      "layers.7.attention.heads.2.v.bias: False\n",
      "layers.7.attention.heads.3.q.weight: False\n",
      "layers.7.attention.heads.3.q.bias: False\n",
      "layers.7.attention.heads.3.k.weight: False\n",
      "layers.7.attention.heads.3.k.bias: False\n",
      "layers.7.attention.heads.3.v.weight: False\n",
      "layers.7.attention.heads.3.v.bias: False\n",
      "layers.7.attention.heads.4.q.weight: False\n",
      "layers.7.attention.heads.4.q.bias: False\n",
      "layers.7.attention.heads.4.k.weight: False\n",
      "layers.7.attention.heads.4.k.bias: False\n",
      "layers.7.attention.heads.4.v.weight: False\n",
      "layers.7.attention.heads.4.v.bias: False\n",
      "layers.7.attention.heads.5.q.weight: False\n",
      "layers.7.attention.heads.5.q.bias: False\n",
      "layers.7.attention.heads.5.k.weight: False\n",
      "layers.7.attention.heads.5.k.bias: False\n",
      "layers.7.attention.heads.5.v.weight: False\n",
      "layers.7.attention.heads.5.v.bias: False\n",
      "layers.7.attention.heads.6.q.weight: False\n",
      "layers.7.attention.heads.6.q.bias: False\n",
      "layers.7.attention.heads.6.k.weight: False\n",
      "layers.7.attention.heads.6.k.bias: False\n",
      "layers.7.attention.heads.6.v.weight: False\n",
      "layers.7.attention.heads.6.v.bias: False\n",
      "layers.7.attention.heads.7.q.weight: False\n",
      "layers.7.attention.heads.7.q.bias: False\n",
      "layers.7.attention.heads.7.k.weight: False\n",
      "layers.7.attention.heads.7.k.bias: False\n",
      "layers.7.attention.heads.7.v.weight: False\n",
      "layers.7.attention.heads.7.v.bias: False\n",
      "layers.7.attention.heads.8.q.weight: False\n",
      "layers.7.attention.heads.8.q.bias: False\n",
      "layers.7.attention.heads.8.k.weight: False\n",
      "layers.7.attention.heads.8.k.bias: False\n",
      "layers.7.attention.heads.8.v.weight: False\n",
      "layers.7.attention.heads.8.v.bias: False\n",
      "layers.7.attention.heads.9.q.weight: False\n",
      "layers.7.attention.heads.9.q.bias: False\n",
      "layers.7.attention.heads.9.k.weight: False\n",
      "layers.7.attention.heads.9.k.bias: False\n",
      "layers.7.attention.heads.9.v.weight: False\n",
      "layers.7.attention.heads.9.v.bias: False\n",
      "layers.7.attention.heads.10.q.weight: False\n",
      "layers.7.attention.heads.10.q.bias: False\n",
      "layers.7.attention.heads.10.k.weight: False\n",
      "layers.7.attention.heads.10.k.bias: False\n",
      "layers.7.attention.heads.10.v.weight: False\n",
      "layers.7.attention.heads.10.v.bias: False\n",
      "layers.7.attention.heads.11.q.weight: False\n",
      "layers.7.attention.heads.11.q.bias: False\n",
      "layers.7.attention.heads.11.k.weight: False\n",
      "layers.7.attention.heads.11.k.bias: False\n",
      "layers.7.attention.heads.11.v.weight: False\n",
      "layers.7.attention.heads.11.v.bias: False\n",
      "layers.7.attention.out_linear.weight: False\n",
      "layers.7.attention.out_linear.bias: False\n",
      "layers.7.feed_forward.linear1.weight: False\n",
      "layers.7.feed_forward.linear1.bias: False\n",
      "layers.7.feed_forward.linear2.weight: False\n",
      "layers.7.feed_forward.linear2.bias: False\n",
      "layers.8.norm1.weight: False\n",
      "layers.8.norm1.bias: False\n",
      "layers.8.norm2.weight: False\n",
      "layers.8.norm2.bias: False\n",
      "layers.8.attention.heads.0.q.weight: False\n",
      "layers.8.attention.heads.0.q.bias: False\n",
      "layers.8.attention.heads.0.k.weight: False\n",
      "layers.8.attention.heads.0.k.bias: False\n",
      "layers.8.attention.heads.0.v.weight: False\n",
      "layers.8.attention.heads.0.v.bias: False\n",
      "layers.8.attention.heads.1.q.weight: False\n",
      "layers.8.attention.heads.1.q.bias: False\n",
      "layers.8.attention.heads.1.k.weight: False\n",
      "layers.8.attention.heads.1.k.bias: False\n",
      "layers.8.attention.heads.1.v.weight: False\n",
      "layers.8.attention.heads.1.v.bias: False\n",
      "layers.8.attention.heads.2.q.weight: False\n",
      "layers.8.attention.heads.2.q.bias: False\n",
      "layers.8.attention.heads.2.k.weight: False\n",
      "layers.8.attention.heads.2.k.bias: False\n",
      "layers.8.attention.heads.2.v.weight: False\n",
      "layers.8.attention.heads.2.v.bias: False\n",
      "layers.8.attention.heads.3.q.weight: False\n",
      "layers.8.attention.heads.3.q.bias: False\n",
      "layers.8.attention.heads.3.k.weight: False\n",
      "layers.8.attention.heads.3.k.bias: False\n",
      "layers.8.attention.heads.3.v.weight: False\n",
      "layers.8.attention.heads.3.v.bias: False\n",
      "layers.8.attention.heads.4.q.weight: False\n",
      "layers.8.attention.heads.4.q.bias: False\n",
      "layers.8.attention.heads.4.k.weight: False\n",
      "layers.8.attention.heads.4.k.bias: False\n",
      "layers.8.attention.heads.4.v.weight: False\n",
      "layers.8.attention.heads.4.v.bias: False\n",
      "layers.8.attention.heads.5.q.weight: False\n",
      "layers.8.attention.heads.5.q.bias: False\n",
      "layers.8.attention.heads.5.k.weight: False\n",
      "layers.8.attention.heads.5.k.bias: False\n",
      "layers.8.attention.heads.5.v.weight: False\n",
      "layers.8.attention.heads.5.v.bias: False\n",
      "layers.8.attention.heads.6.q.weight: False\n",
      "layers.8.attention.heads.6.q.bias: False\n",
      "layers.8.attention.heads.6.k.weight: False\n",
      "layers.8.attention.heads.6.k.bias: False\n",
      "layers.8.attention.heads.6.v.weight: False\n",
      "layers.8.attention.heads.6.v.bias: False\n",
      "layers.8.attention.heads.7.q.weight: False\n",
      "layers.8.attention.heads.7.q.bias: False\n",
      "layers.8.attention.heads.7.k.weight: False\n",
      "layers.8.attention.heads.7.k.bias: False\n",
      "layers.8.attention.heads.7.v.weight: False\n",
      "layers.8.attention.heads.7.v.bias: False\n",
      "layers.8.attention.heads.8.q.weight: False\n",
      "layers.8.attention.heads.8.q.bias: False\n",
      "layers.8.attention.heads.8.k.weight: False\n",
      "layers.8.attention.heads.8.k.bias: False\n",
      "layers.8.attention.heads.8.v.weight: False\n",
      "layers.8.attention.heads.8.v.bias: False\n",
      "layers.8.attention.heads.9.q.weight: False\n",
      "layers.8.attention.heads.9.q.bias: False\n",
      "layers.8.attention.heads.9.k.weight: False\n",
      "layers.8.attention.heads.9.k.bias: False\n",
      "layers.8.attention.heads.9.v.weight: False\n",
      "layers.8.attention.heads.9.v.bias: False\n",
      "layers.8.attention.heads.10.q.weight: False\n",
      "layers.8.attention.heads.10.q.bias: False\n",
      "layers.8.attention.heads.10.k.weight: False\n",
      "layers.8.attention.heads.10.k.bias: False\n",
      "layers.8.attention.heads.10.v.weight: False\n",
      "layers.8.attention.heads.10.v.bias: False\n",
      "layers.8.attention.heads.11.q.weight: False\n",
      "layers.8.attention.heads.11.q.bias: False\n",
      "layers.8.attention.heads.11.k.weight: False\n",
      "layers.8.attention.heads.11.k.bias: False\n",
      "layers.8.attention.heads.11.v.weight: False\n",
      "layers.8.attention.heads.11.v.bias: False\n",
      "layers.8.attention.out_linear.weight: False\n",
      "layers.8.attention.out_linear.bias: False\n",
      "layers.8.feed_forward.linear1.weight: False\n",
      "layers.8.feed_forward.linear1.bias: False\n",
      "layers.8.feed_forward.linear2.weight: False\n",
      "layers.8.feed_forward.linear2.bias: False\n",
      "layers.9.norm1.weight: False\n",
      "layers.9.norm1.bias: False\n",
      "layers.9.norm2.weight: False\n",
      "layers.9.norm2.bias: False\n",
      "layers.9.attention.heads.0.q.weight: False\n",
      "layers.9.attention.heads.0.q.bias: False\n",
      "layers.9.attention.heads.0.k.weight: False\n",
      "layers.9.attention.heads.0.k.bias: False\n",
      "layers.9.attention.heads.0.v.weight: False\n",
      "layers.9.attention.heads.0.v.bias: False\n",
      "layers.9.attention.heads.1.q.weight: False\n",
      "layers.9.attention.heads.1.q.bias: False\n",
      "layers.9.attention.heads.1.k.weight: False\n",
      "layers.9.attention.heads.1.k.bias: False\n",
      "layers.9.attention.heads.1.v.weight: False\n",
      "layers.9.attention.heads.1.v.bias: False\n",
      "layers.9.attention.heads.2.q.weight: False\n",
      "layers.9.attention.heads.2.q.bias: False\n",
      "layers.9.attention.heads.2.k.weight: False\n",
      "layers.9.attention.heads.2.k.bias: False\n",
      "layers.9.attention.heads.2.v.weight: False\n",
      "layers.9.attention.heads.2.v.bias: False\n",
      "layers.9.attention.heads.3.q.weight: False\n",
      "layers.9.attention.heads.3.q.bias: False\n",
      "layers.9.attention.heads.3.k.weight: False\n",
      "layers.9.attention.heads.3.k.bias: False\n",
      "layers.9.attention.heads.3.v.weight: False\n",
      "layers.9.attention.heads.3.v.bias: False\n",
      "layers.9.attention.heads.4.q.weight: False\n",
      "layers.9.attention.heads.4.q.bias: False\n",
      "layers.9.attention.heads.4.k.weight: False\n",
      "layers.9.attention.heads.4.k.bias: False\n",
      "layers.9.attention.heads.4.v.weight: False\n",
      "layers.9.attention.heads.4.v.bias: False\n",
      "layers.9.attention.heads.5.q.weight: False\n",
      "layers.9.attention.heads.5.q.bias: False\n",
      "layers.9.attention.heads.5.k.weight: False\n",
      "layers.9.attention.heads.5.k.bias: False\n",
      "layers.9.attention.heads.5.v.weight: False\n",
      "layers.9.attention.heads.5.v.bias: False\n",
      "layers.9.attention.heads.6.q.weight: False\n",
      "layers.9.attention.heads.6.q.bias: False\n",
      "layers.9.attention.heads.6.k.weight: False\n",
      "layers.9.attention.heads.6.k.bias: False\n",
      "layers.9.attention.heads.6.v.weight: False\n",
      "layers.9.attention.heads.6.v.bias: False\n",
      "layers.9.attention.heads.7.q.weight: False\n",
      "layers.9.attention.heads.7.q.bias: False\n",
      "layers.9.attention.heads.7.k.weight: False\n",
      "layers.9.attention.heads.7.k.bias: False\n",
      "layers.9.attention.heads.7.v.weight: False\n",
      "layers.9.attention.heads.7.v.bias: False\n",
      "layers.9.attention.heads.8.q.weight: False\n",
      "layers.9.attention.heads.8.q.bias: False\n",
      "layers.9.attention.heads.8.k.weight: False\n",
      "layers.9.attention.heads.8.k.bias: False\n",
      "layers.9.attention.heads.8.v.weight: False\n",
      "layers.9.attention.heads.8.v.bias: False\n",
      "layers.9.attention.heads.9.q.weight: False\n",
      "layers.9.attention.heads.9.q.bias: False\n",
      "layers.9.attention.heads.9.k.weight: False\n",
      "layers.9.attention.heads.9.k.bias: False\n",
      "layers.9.attention.heads.9.v.weight: False\n",
      "layers.9.attention.heads.9.v.bias: False\n",
      "layers.9.attention.heads.10.q.weight: False\n",
      "layers.9.attention.heads.10.q.bias: False\n",
      "layers.9.attention.heads.10.k.weight: False\n",
      "layers.9.attention.heads.10.k.bias: False\n",
      "layers.9.attention.heads.10.v.weight: False\n",
      "layers.9.attention.heads.10.v.bias: False\n",
      "layers.9.attention.heads.11.q.weight: False\n",
      "layers.9.attention.heads.11.q.bias: False\n",
      "layers.9.attention.heads.11.k.weight: False\n",
      "layers.9.attention.heads.11.k.bias: False\n",
      "layers.9.attention.heads.11.v.weight: False\n",
      "layers.9.attention.heads.11.v.bias: False\n",
      "layers.9.attention.out_linear.weight: False\n",
      "layers.9.attention.out_linear.bias: False\n",
      "layers.9.feed_forward.linear1.weight: False\n",
      "layers.9.feed_forward.linear1.bias: False\n",
      "layers.9.feed_forward.linear2.weight: False\n",
      "layers.9.feed_forward.linear2.bias: False\n",
      "layers.10.norm1.weight: False\n",
      "layers.10.norm1.bias: False\n",
      "layers.10.norm2.weight: False\n",
      "layers.10.norm2.bias: False\n",
      "layers.10.attention.heads.0.q.weight: False\n",
      "layers.10.attention.heads.0.q.bias: False\n",
      "layers.10.attention.heads.0.k.weight: False\n",
      "layers.10.attention.heads.0.k.bias: False\n",
      "layers.10.attention.heads.0.v.weight: False\n",
      "layers.10.attention.heads.0.v.bias: False\n",
      "layers.10.attention.heads.1.q.weight: False\n",
      "layers.10.attention.heads.1.q.bias: False\n",
      "layers.10.attention.heads.1.k.weight: False\n",
      "layers.10.attention.heads.1.k.bias: False\n",
      "layers.10.attention.heads.1.v.weight: False\n",
      "layers.10.attention.heads.1.v.bias: False\n",
      "layers.10.attention.heads.2.q.weight: False\n",
      "layers.10.attention.heads.2.q.bias: False\n",
      "layers.10.attention.heads.2.k.weight: False\n",
      "layers.10.attention.heads.2.k.bias: False\n",
      "layers.10.attention.heads.2.v.weight: False\n",
      "layers.10.attention.heads.2.v.bias: False\n",
      "layers.10.attention.heads.3.q.weight: False\n",
      "layers.10.attention.heads.3.q.bias: False\n",
      "layers.10.attention.heads.3.k.weight: False\n",
      "layers.10.attention.heads.3.k.bias: False\n",
      "layers.10.attention.heads.3.v.weight: False\n",
      "layers.10.attention.heads.3.v.bias: False\n",
      "layers.10.attention.heads.4.q.weight: False\n",
      "layers.10.attention.heads.4.q.bias: False\n",
      "layers.10.attention.heads.4.k.weight: False\n",
      "layers.10.attention.heads.4.k.bias: False\n",
      "layers.10.attention.heads.4.v.weight: False\n",
      "layers.10.attention.heads.4.v.bias: False\n",
      "layers.10.attention.heads.5.q.weight: False\n",
      "layers.10.attention.heads.5.q.bias: False\n",
      "layers.10.attention.heads.5.k.weight: False\n",
      "layers.10.attention.heads.5.k.bias: False\n",
      "layers.10.attention.heads.5.v.weight: False\n",
      "layers.10.attention.heads.5.v.bias: False\n",
      "layers.10.attention.heads.6.q.weight: False\n",
      "layers.10.attention.heads.6.q.bias: False\n",
      "layers.10.attention.heads.6.k.weight: False\n",
      "layers.10.attention.heads.6.k.bias: False\n",
      "layers.10.attention.heads.6.v.weight: False\n",
      "layers.10.attention.heads.6.v.bias: False\n",
      "layers.10.attention.heads.7.q.weight: False\n",
      "layers.10.attention.heads.7.q.bias: False\n",
      "layers.10.attention.heads.7.k.weight: False\n",
      "layers.10.attention.heads.7.k.bias: False\n",
      "layers.10.attention.heads.7.v.weight: False\n",
      "layers.10.attention.heads.7.v.bias: False\n",
      "layers.10.attention.heads.8.q.weight: False\n",
      "layers.10.attention.heads.8.q.bias: False\n",
      "layers.10.attention.heads.8.k.weight: False\n",
      "layers.10.attention.heads.8.k.bias: False\n",
      "layers.10.attention.heads.8.v.weight: False\n",
      "layers.10.attention.heads.8.v.bias: False\n",
      "layers.10.attention.heads.9.q.weight: False\n",
      "layers.10.attention.heads.9.q.bias: False\n",
      "layers.10.attention.heads.9.k.weight: False\n",
      "layers.10.attention.heads.9.k.bias: False\n",
      "layers.10.attention.heads.9.v.weight: False\n",
      "layers.10.attention.heads.9.v.bias: False\n",
      "layers.10.attention.heads.10.q.weight: False\n",
      "layers.10.attention.heads.10.q.bias: False\n",
      "layers.10.attention.heads.10.k.weight: False\n",
      "layers.10.attention.heads.10.k.bias: False\n",
      "layers.10.attention.heads.10.v.weight: False\n",
      "layers.10.attention.heads.10.v.bias: False\n",
      "layers.10.attention.heads.11.q.weight: False\n",
      "layers.10.attention.heads.11.q.bias: False\n",
      "layers.10.attention.heads.11.k.weight: False\n",
      "layers.10.attention.heads.11.k.bias: False\n",
      "layers.10.attention.heads.11.v.weight: False\n",
      "layers.10.attention.heads.11.v.bias: False\n",
      "layers.10.attention.out_linear.weight: False\n",
      "layers.10.attention.out_linear.bias: False\n",
      "layers.10.feed_forward.linear1.weight: False\n",
      "layers.10.feed_forward.linear1.bias: False\n",
      "layers.10.feed_forward.linear2.weight: False\n",
      "layers.10.feed_forward.linear2.bias: False\n",
      "layers.11.norm1.weight: False\n",
      "layers.11.norm1.bias: False\n",
      "layers.11.norm2.weight: False\n",
      "layers.11.norm2.bias: False\n",
      "layers.11.attention.heads.0.q.weight: False\n",
      "layers.11.attention.heads.0.q.bias: False\n",
      "layers.11.attention.heads.0.k.weight: False\n",
      "layers.11.attention.heads.0.k.bias: False\n",
      "layers.11.attention.heads.0.v.weight: False\n",
      "layers.11.attention.heads.0.v.bias: False\n",
      "layers.11.attention.heads.1.q.weight: False\n",
      "layers.11.attention.heads.1.q.bias: False\n",
      "layers.11.attention.heads.1.k.weight: False\n",
      "layers.11.attention.heads.1.k.bias: False\n",
      "layers.11.attention.heads.1.v.weight: False\n",
      "layers.11.attention.heads.1.v.bias: False\n",
      "layers.11.attention.heads.2.q.weight: False\n",
      "layers.11.attention.heads.2.q.bias: False\n",
      "layers.11.attention.heads.2.k.weight: False\n",
      "layers.11.attention.heads.2.k.bias: False\n",
      "layers.11.attention.heads.2.v.weight: False\n",
      "layers.11.attention.heads.2.v.bias: False\n",
      "layers.11.attention.heads.3.q.weight: False\n",
      "layers.11.attention.heads.3.q.bias: False\n",
      "layers.11.attention.heads.3.k.weight: False\n",
      "layers.11.attention.heads.3.k.bias: False\n",
      "layers.11.attention.heads.3.v.weight: False\n",
      "layers.11.attention.heads.3.v.bias: False\n",
      "layers.11.attention.heads.4.q.weight: False\n",
      "layers.11.attention.heads.4.q.bias: False\n",
      "layers.11.attention.heads.4.k.weight: False\n",
      "layers.11.attention.heads.4.k.bias: False\n",
      "layers.11.attention.heads.4.v.weight: False\n",
      "layers.11.attention.heads.4.v.bias: False\n",
      "layers.11.attention.heads.5.q.weight: False\n",
      "layers.11.attention.heads.5.q.bias: False\n",
      "layers.11.attention.heads.5.k.weight: False\n",
      "layers.11.attention.heads.5.k.bias: False\n",
      "layers.11.attention.heads.5.v.weight: False\n",
      "layers.11.attention.heads.5.v.bias: False\n",
      "layers.11.attention.heads.6.q.weight: False\n",
      "layers.11.attention.heads.6.q.bias: False\n",
      "layers.11.attention.heads.6.k.weight: False\n",
      "layers.11.attention.heads.6.k.bias: False\n",
      "layers.11.attention.heads.6.v.weight: False\n",
      "layers.11.attention.heads.6.v.bias: False\n",
      "layers.11.attention.heads.7.q.weight: False\n",
      "layers.11.attention.heads.7.q.bias: False\n",
      "layers.11.attention.heads.7.k.weight: False\n",
      "layers.11.attention.heads.7.k.bias: False\n",
      "layers.11.attention.heads.7.v.weight: False\n",
      "layers.11.attention.heads.7.v.bias: False\n",
      "layers.11.attention.heads.8.q.weight: False\n",
      "layers.11.attention.heads.8.q.bias: False\n",
      "layers.11.attention.heads.8.k.weight: False\n",
      "layers.11.attention.heads.8.k.bias: False\n",
      "layers.11.attention.heads.8.v.weight: False\n",
      "layers.11.attention.heads.8.v.bias: False\n",
      "layers.11.attention.heads.9.q.weight: False\n",
      "layers.11.attention.heads.9.q.bias: False\n",
      "layers.11.attention.heads.9.k.weight: False\n",
      "layers.11.attention.heads.9.k.bias: False\n",
      "layers.11.attention.heads.9.v.weight: False\n",
      "layers.11.attention.heads.9.v.bias: False\n",
      "layers.11.attention.heads.10.q.weight: False\n",
      "layers.11.attention.heads.10.q.bias: False\n",
      "layers.11.attention.heads.10.k.weight: False\n",
      "layers.11.attention.heads.10.k.bias: False\n",
      "layers.11.attention.heads.10.v.weight: False\n",
      "layers.11.attention.heads.10.v.bias: False\n",
      "layers.11.attention.heads.11.q.weight: False\n",
      "layers.11.attention.heads.11.q.bias: False\n",
      "layers.11.attention.heads.11.k.weight: False\n",
      "layers.11.attention.heads.11.k.bias: False\n",
      "layers.11.attention.heads.11.v.weight: False\n",
      "layers.11.attention.heads.11.v.bias: False\n",
      "layers.11.attention.out_linear.weight: False\n",
      "layers.11.attention.out_linear.bias: False\n",
      "layers.11.feed_forward.linear1.weight: False\n",
      "layers.11.feed_forward.linear1.bias: False\n",
      "layers.11.feed_forward.linear2.weight: False\n",
      "layers.11.feed_forward.linear2.bias: False\n"
     ]
    }
   ],
   "source": [
    "# Now, check if the parameters are frozen\n",
    "for name, param in encoder.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')\n",
    "\n",
    "# Use the encoder for inference\n",
    "encoder_output = encoder(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463eb937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "50cb84d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:08.986890Z",
     "iopub.status.busy": "2023-11-07T12:12:08.985912Z",
     "iopub.status.idle": "2023-11-07T12:12:08.991281Z",
     "shell.execute_reply": "2023-11-07T12:12:08.990709Z"
    },
    "papermill": {
     "duration": 0.024749,
     "end_time": "2023-11-07T12:12:08.992754",
     "exception": false,
     "start_time": "2023-11-07T12:12:08.968005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hidden state for each token in a batch\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7c9c2b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6423]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4161345",
   "metadata": {
    "papermill": {
     "duration": 0.016716,
     "end_time": "2023-11-07T12:12:09.026352",
     "exception": false,
     "start_time": "2023-11-07T12:12:09.009636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#F1A424\"><b><span style='color:#FFFFFF'>8 |</span></b> <b>CLASSIFICATION HEAD</b></div>\n",
    "\n",
    "Quite often, transformers are divided into:\n",
    "- Task independent body (`TransformerEncoder`)\n",
    "- Task dependent head (`TransformerClassifier`)\n",
    "\n",
    "Select one of the token outputs:\n",
    "- The first token in such models is often used for the prediction **[CLS] token**\n",
    "- Can attach a `dropout` and a `linear` transformation layer to make a classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "50466a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:09.063655Z",
     "iopub.status.busy": "2023-11-07T12:12:09.063083Z",
     "iopub.status.idle": "2023-11-07T12:12:09.068612Z",
     "shell.execute_reply": "2023-11-07T12:12:09.068049Z"
    },
    "papermill": {
     "duration": 0.025845,
     "end_time": "2023-11-07T12:12:09.070128",
     "exception": false,
     "start_time": "2023-11-07T12:12:09.044283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x) # 768 -> 3 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a5f09",
   "metadata": {
    "papermill": {
     "duration": 0.016599,
     "end_time": "2023-11-07T12:12:09.103550",
     "exception": false,
     "start_time": "2023-11-07T12:12:09.086951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- For each sample in the batch we get the **unnormalized logits** for each class in the output, which corresponds to the BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "74c29271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T12:12:09.138526Z",
     "iopub.status.busy": "2023-11-07T12:12:09.138069Z",
     "iopub.status.idle": "2023-11-07T12:12:10.191689Z",
     "shell.execute_reply": "2023-11-07T12:12:10.191034Z"
    },
    "papermill": {
     "duration": 1.073469,
     "end_time": "2023-11-07T12:12:10.193779",
     "exception": false,
     "start_time": "2023-11-07T12:12:09.120310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1077, -1.1667, -1.6423, -0.6971, -1.2382, -0.1741, -1.3646, -1.4697,\n",
       "         -0.2726,  2.0157,  1.1196, -0.8590,  0.5565,  1.2117, -0.5707, -0.7668,\n",
       "         -0.7323, -2.0161,  0.0433,  0.3031, -1.2364,  0.1676, -0.1053, -0.3740,\n",
       "         -0.2358,  0.6749,  1.2393,  1.2737, -1.4780,  0.2356,  1.2692,  0.9412,\n",
       "          0.1535,  0.3772, -0.4796,  0.2924,  0.5760,  1.2281, -0.7525, -1.7116,\n",
       "         -0.4864,  0.9515, -1.4321, -0.1793,  0.4792, -0.5470,  0.3365,  0.8325,\n",
       "          1.2968,  0.1978, -1.2673,  1.9813, -1.1665, -1.8995,  0.4980,  1.6997,\n",
       "         -2.3371,  0.0271, -1.0663, -1.0742,  0.2720,  0.9621,  0.0054, -0.9917,\n",
       "          0.5763, -1.1573, -0.3858, -0.3850,  0.9824, -0.3361, -0.5372,  0.9757,\n",
       "          0.9338, -0.6942,  0.8172,  0.5721, -1.2012, -0.1277, -0.0718, -0.5320,\n",
       "         -1.0751,  0.3630,  0.0348, -0.1394, -0.9022,  0.1989, -1.3688,  0.3751,\n",
       "         -0.5361,  3.0035,  1.3790, -0.1013,  0.8584, -1.4457,  0.1728,  0.8734,\n",
       "          0.5113,  0.4873, -0.0541, -1.2157]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 100\n",
    "encoder_classifier = TransformerClassifier(config)\n",
    "output = encoder_classifier(inputs.input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f5e54b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82af357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7248, -1.0299, -0.2984, -0.8205,  1.5645]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78cfec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6423, 2003, 3835],\n",
      "        [6423, 2003, 2785],\n",
      "        [6423, 2003, 2812],\n",
      "        [6423, 2003, 3407]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenise input (text)\n",
    "inputs = tokenizer([\"julia is nice\",\"julia is kind\",\"julia is mean\",\"julia is happy\"], \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False) # don't use pad, sep tokens\n",
    "\n",
    "print(inputs.input_ids)\n",
    "inputs.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b60fc987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9110e-01,  5.7940e-01,  3.9706e-01, -1.8225e-01,  6.0488e-01],\n",
       "        [-6.3441e-01, -1.2129e+00,  2.4899e-01, -8.7813e-01, -2.3223e+00],\n",
       "        [ 9.6574e-01, -2.9699e-01,  6.2808e-01, -5.4478e-01, -4.3743e-01],\n",
       "        [ 3.2243e-01, -8.6369e-01, -4.8803e-01,  6.2011e-01, -2.7719e+00],\n",
       "        [ 1.7241e-01, -2.4628e-01,  2.8055e-01, -1.6542e-02,  2.9267e-01],\n",
       "        [ 5.2029e-01, -1.7808e-01, -1.1362e+00, -2.1143e+00, -1.1951e+00],\n",
       "        [ 9.5677e-02, -2.1166e+00, -8.8535e-01, -1.2726e+00, -1.1311e+00],\n",
       "        [ 2.0610e-01, -9.4082e-02,  9.8888e-01, -1.0614e-01,  1.4996e-01],\n",
       "        [-1.2756e+00, -1.2334e+00, -1.0847e-01, -8.5860e-01, -5.6302e-01],\n",
       "        [ 1.1313e+00,  1.0202e+00,  1.4249e+00, -1.8494e-01, -1.2267e+00],\n",
       "        [ 1.0440e+00,  1.4323e+00,  3.5243e-01,  2.9605e-02, -1.8420e+00],\n",
       "        [-4.1269e-01, -3.1108e-01, -8.8196e-02, -7.1487e-01, -1.9682e-01],\n",
       "        [-1.1787e+00, -8.7122e-01, -8.6970e-01, -6.9106e-01, -1.1400e+00],\n",
       "        [ 2.5004e-01,  3.8412e-01, -8.4898e-01, -1.4634e+00,  6.4380e-02],\n",
       "        [ 7.8142e-01, -2.4171e-01, -3.5669e-01, -5.9684e-01, -1.2965e+00],\n",
       "        [ 1.6333e+00,  3.4422e-01,  1.4059e-01, -3.2552e-01, -6.7811e-01],\n",
       "        [ 1.6313e+00, -5.9838e-01,  1.0557e+00, -7.2074e-01, -1.7132e-01],\n",
       "        [ 1.1656e+00,  2.0337e-01, -6.9283e-01, -1.5471e+00, -1.1255e+00],\n",
       "        [ 1.1599e+00, -1.6320e+00, -5.1865e-01, -1.6641e+00, -1.3560e-01],\n",
       "        [ 1.2299e-01, -1.2470e+00, -1.6500e-01,  1.0834e-01, -2.7135e+00],\n",
       "        [-4.2376e-01,  7.8625e-01, -1.2481e+00, -1.8583e-01, -1.0670e+00],\n",
       "        [ 8.8201e-01, -3.6452e-01, -1.1313e+00,  2.3454e-02,  3.4621e-01],\n",
       "        [ 2.6264e-01,  7.2091e-01, -2.5267e-01,  1.3747e+00,  3.8162e-01],\n",
       "        [ 1.4070e-02,  1.0427e-01, -8.6672e-02, -1.5911e+00, -9.3913e-01],\n",
       "        [ 5.9478e-01, -1.1675e-01,  2.3111e-01, -3.4203e-02, -1.1257e+00],\n",
       "        [ 8.7989e-01, -2.7736e-01,  3.8005e-01,  7.7288e-01, -9.7946e-01],\n",
       "        [ 3.6292e-01, -1.2397e+00, -2.1162e-01, -1.6989e+00,  6.0603e-01],\n",
       "        [-1.5945e+00, -7.3564e-01, -3.0662e-01, -6.3096e-01, -9.0825e-01],\n",
       "        [ 9.2098e-01, -1.3900e+00, -4.3720e-01, -1.0417e+00, -1.6203e+00],\n",
       "        [-2.8266e-03,  1.2306e-01,  5.6160e-01,  9.2427e-02, -6.9863e-01],\n",
       "        [ 1.1253e+00,  7.9683e-01, -1.0520e+00, -9.7253e-01, -6.8689e-02],\n",
       "        [ 2.8706e-01, -7.1292e-01, -2.0087e-01,  5.0003e-01, -7.7339e-01],\n",
       "        [ 7.6461e-01, -1.4970e+00,  3.8990e-01,  1.1178e-01, -1.3419e+00],\n",
       "        [ 4.9284e-01, -2.4889e+00, -5.7913e-02, -9.0148e-01, -1.1378e+00],\n",
       "        [ 7.1613e-01, -1.4417e+00, -1.2693e+00, -2.7630e-01, -1.6185e-01],\n",
       "        [-2.6838e-01,  1.3214e-01,  7.7085e-02, -1.9312e-01, -5.9134e-01],\n",
       "        [ 2.4948e+00,  1.2750e+00, -2.0590e-02, -9.1751e-01, -2.0417e-01],\n",
       "        [ 2.0176e-01, -1.2005e+00,  3.3404e-02,  9.3414e-01, -1.9468e+00],\n",
       "        [-1.0386e+00,  8.7069e-01,  8.6785e-01, -3.0028e-01, -1.0842e+00],\n",
       "        [ 8.8663e-01,  2.0366e-01, -1.3738e+00, -2.9591e-01,  1.1079e+00],\n",
       "        [ 1.3745e+00, -8.3879e-01,  3.9139e-01, -7.7236e-01, -3.3533e-01],\n",
       "        [ 8.2542e-02,  1.1199e+00, -4.7710e-01,  1.3843e-01, -1.7478e+00],\n",
       "        [ 2.1475e+00, -2.6520e-01, -2.2611e+00, -1.4776e+00, -1.0585e+00],\n",
       "        [ 2.9611e-01, -6.5133e-01, -2.2959e-01, -9.3196e-02, -2.3771e+00],\n",
       "        [ 1.0707e+00, -1.3703e+00,  1.7736e-01, -9.8441e-01, -2.3037e+00],\n",
       "        [-6.2055e-01, -1.0971e+00,  5.9999e-01, -1.9990e+00, -1.4568e+00],\n",
       "        [ 7.7576e-01, -2.2540e+00, -2.8128e-01, -1.2080e+00, -8.9818e-01],\n",
       "        [ 1.7457e+00, -1.3976e+00,  8.4484e-01,  1.5784e+00, -8.2951e-01],\n",
       "        [ 1.7401e+00, -9.5754e-01,  8.3213e-02, -1.1227e+00, -5.6448e-01],\n",
       "        [ 4.9751e-02, -1.9439e+00,  3.8119e-01, -2.8163e-01,  1.1576e-01],\n",
       "        [ 9.8102e-02, -9.0715e-01, -3.3274e-01, -1.0685e+00, -1.1169e+00],\n",
       "        [ 9.3555e-01, -6.4996e-01,  1.6404e+00, -1.7186e-01, -2.4956e+00],\n",
       "        [ 1.9935e+00, -5.5462e-01,  6.7823e-01, -6.0224e-01, -3.8622e-01],\n",
       "        [ 1.5865e+00, -8.1559e-01,  2.1349e-01,  6.2506e-01, -6.9114e-01],\n",
       "        [-1.9189e-01,  3.8584e-02, -1.1641e+00,  1.3678e+00, -1.6728e-01],\n",
       "        [-1.0522e+00,  6.5318e-01,  2.0743e-01, -1.7865e+00,  5.4325e-01],\n",
       "        [-1.1011e-01,  1.1253e-01, -5.9066e-01, -6.5202e-01, -9.5747e-02],\n",
       "        [-4.2655e-01, -2.3059e+00,  1.5156e+00, -5.5185e-01, -1.7296e+00],\n",
       "        [-1.3425e+00, -1.0043e+00,  1.3787e+00,  4.7277e-01, -3.8991e-01],\n",
       "        [ 5.5627e-01, -2.2658e-01, -1.2290e-02, -5.7706e-01, -1.3410e+00],\n",
       "        [ 1.0163e+00, -1.9785e+00,  7.5079e-01,  2.2818e+00, -2.1156e+00],\n",
       "        [ 7.3808e-01, -2.8046e-01, -1.5356e+00, -1.7217e-01, -1.3976e-01],\n",
       "        [-1.6854e-01, -1.7987e+00,  3.0647e-01, -1.2098e+00, -2.5329e+00],\n",
       "        [-9.5936e-01,  7.8008e-02, -9.8397e-03, -1.4873e+00, -1.5819e+00],\n",
       "        [ 1.1687e+00, -6.4984e-01, -2.1895e-01, -7.9697e-02, -4.4818e-01],\n",
       "        [ 1.6028e-01, -1.5025e+00, -2.1951e-01, -2.2958e-01, -2.0008e+00],\n",
       "        [ 9.2455e-01, -5.0578e-01, -2.0756e+00, -1.1379e+00, -5.0994e-01],\n",
       "        [ 8.9424e-01, -8.4253e-01,  3.3202e-01, -1.5546e-01, -2.0224e+00],\n",
       "        [ 8.5485e-01, -2.2774e-01,  5.0732e-01, -6.7903e-01, -5.1638e-01],\n",
       "        [ 1.4261e-01, -1.3378e+00, -8.8241e-02, -1.0728e+00, -1.0897e+00],\n",
       "        [ 9.6077e-02, -1.3217e+00, -1.2139e+00, -1.4919e-01, -1.3530e+00],\n",
       "        [ 2.0334e-01, -1.4609e+00, -1.2910e+00,  3.3247e-01, -8.9390e-01],\n",
       "        [ 3.8137e-01,  8.0514e-01, -1.2984e+00, -1.4890e-01, -5.9475e-01],\n",
       "        [-2.8561e-01, -6.7955e-01, -1.5766e+00, -1.4394e+00, -2.4167e+00],\n",
       "        [-3.9136e-02, -1.0943e+00, -1.0790e+00, -1.4356e+00, -7.5429e-01],\n",
       "        [ 1.8228e+00,  6.8628e-01,  1.2173e+00, -3.5622e-01, -2.9946e-01],\n",
       "        [ 1.3327e+00, -9.6749e-01,  4.5130e-01, -1.1864e-01, -3.0864e-01],\n",
       "        [ 6.0147e-01,  3.6756e-03,  5.9449e-01, -9.1844e-01, -1.6944e+00],\n",
       "        [-2.1344e-01, -1.2416e+00,  6.3060e-02,  5.2987e-01, -3.2400e-01],\n",
       "        [ 1.8962e+00, -5.9363e-01, -2.0981e-01, -1.0366e-01, -2.0161e+00],\n",
       "        [-1.2756e+00, -6.3326e-01, -3.2087e-01, -8.4711e-01,  5.2533e-02],\n",
       "        [-1.7170e+00, -3.5184e-01, -3.0173e-01, -1.7217e+00, -3.3912e+00],\n",
       "        [ 3.0994e+00, -1.7156e+00, -5.6689e-01, -9.9784e-01,  8.4441e-02],\n",
       "        [ 1.5109e+00, -9.1285e-01,  1.2882e+00, -7.1812e-01, -4.6459e-01],\n",
       "        [ 7.8726e-01, -2.5604e+00, -5.0641e-01, -1.7723e+00, -2.0267e+00],\n",
       "        [ 3.3609e-01, -7.1987e-02,  9.9036e-01, -1.1614e+00, -2.1507e+00],\n",
       "        [ 5.2675e-01, -7.1611e-01, -1.5832e+00, -6.5006e-01, -8.5685e-01],\n",
       "        [-2.9241e-01, -1.7741e+00, -6.3316e-01, -1.7257e-04, -1.4713e+00],\n",
       "        [ 6.4515e-01, -2.9836e-01, -2.0832e+00, -1.4145e+00, -8.4607e-01],\n",
       "        [-4.4740e-01, -5.3761e-01,  4.6266e-01, -1.2417e+00, -1.4126e+00],\n",
       "        [-3.7340e-01, -1.6307e+00, -1.7354e-01, -4.7242e-01, -5.7655e-01],\n",
       "        [-8.3914e-02, -1.7516e+00, -1.2076e+00, -2.9477e-01, -1.8658e+00],\n",
       "        [ 1.3508e+00,  1.0124e+00, -9.3796e-01, -1.1778e+00, -1.4109e+00],\n",
       "        [ 1.0477e+00, -8.7153e-01,  1.4414e-01,  9.7652e-01, -2.0117e+00],\n",
       "        [-4.7622e-01,  1.0305e+00, -1.2123e+00, -1.0549e+00, -1.6745e+00],\n",
       "        [ 1.2504e+00, -1.5228e+00,  1.7909e-01, -6.1848e-01, -1.6859e+00],\n",
       "        [-7.8982e-01,  1.0924e+00, -1.2691e+00, -1.1751e+00, -2.7680e-01],\n",
       "        [ 5.3805e-01,  1.3295e-01, -1.1482e+00, -1.6907e+00,  9.1383e-01],\n",
       "        [ 5.4817e-01,  1.0456e+00,  9.8532e-03, -1.6674e-01, -3.3794e-01],\n",
       "        [ 5.9921e-01,  2.3468e-01, -6.2906e-02, -1.1213e+00, -1.0334e+00],\n",
       "        [ 3.9204e-01,  1.1387e+00,  5.7455e-01, -5.6257e-01, -1.0728e+00],\n",
       "        [-5.4318e-02, -2.4960e+00,  6.6910e-01, -5.5391e-01, -2.4460e+00],\n",
       "        [ 9.4497e-01, -2.4175e+00, -1.6371e-01, -1.0009e+00, -1.1429e+00],\n",
       "        [ 1.1200e+00, -1.4927e+00, -6.6200e-01, -1.0906e+00, -8.9809e-01],\n",
       "        [-5.6779e-01, -6.7024e-01,  4.6477e-01, -5.4315e-01,  9.2781e-01],\n",
       "        [-9.4583e-02, -2.4688e+00, -1.0716e+00,  2.0735e-01, -1.1980e+00],\n",
       "        [ 9.2503e-01, -2.8698e+00,  7.3982e-01, -8.7767e-01, -8.1937e-01],\n",
       "        [ 2.7318e-01, -4.5633e-01,  8.1994e-01, -4.1109e-02, -2.4457e+00],\n",
       "        [ 7.5393e-01, -1.1624e-01, -8.1314e-01, -5.2817e-01, -4.6125e-01],\n",
       "        [-3.5236e-01, -1.3782e-01, -4.3198e-01, -5.0853e-01, -5.7405e-01],\n",
       "        [-2.5882e-01, -5.7427e-01, -1.0426e-01, -1.2947e+00, -1.5297e+00],\n",
       "        [ 3.2511e-01,  2.2427e+00, -2.1358e-01, -1.0603e+00, -5.8885e-01],\n",
       "        [ 1.2557e+00, -7.4532e-01, -1.2233e+00, -5.6296e-01, -1.2836e+00],\n",
       "        [ 8.6503e-01, -2.3966e-01, -9.1240e-01, -4.0879e-01, -1.0852e+00],\n",
       "        [ 7.5421e-01,  1.2333e-01,  2.3332e-01,  1.5563e+00, -6.9522e-01],\n",
       "        [ 1.1675e-01, -4.9170e-01, -4.4889e-01,  9.1544e-02, -8.7689e-01],\n",
       "        [-3.8266e-01, -1.5523e+00,  1.0506e-01, -7.6373e-01, -2.5390e-01],\n",
       "        [ 8.5346e-01, -1.2837e+00, -1.6613e+00, -2.3468e+00, -5.3287e-01],\n",
       "        [-2.3983e-02, -2.3992e-01, -1.0471e+00,  5.5895e-01, -8.4996e-01],\n",
       "        [-8.8435e-02,  6.1865e-01,  3.1114e-01,  6.7037e-01, -1.7333e+00],\n",
       "        [ 7.6135e-01, -8.3111e-01, -9.3204e-01,  1.3279e-01, -1.8305e+00],\n",
       "        [-2.8442e-01, -1.1820e-01,  5.7764e-02, -1.0848e-02, -1.5461e+00],\n",
       "        [ 5.5012e-01, -1.5003e+00,  8.9848e-01, -2.2232e+00, -1.6881e+00],\n",
       "        [ 1.0538e-01,  9.5212e-02,  6.1123e-01,  3.4064e-01, -1.4035e+00],\n",
       "        [ 8.8862e-01,  1.5667e-01, -8.6439e-01, -3.0696e-02, -9.6477e-01],\n",
       "        [ 5.0686e-01, -2.1364e-01,  3.8783e-01, -1.0944e+00,  1.3401e-01],\n",
       "        [-2.7179e-02, -2.2208e-01,  6.5224e-01,  4.3405e-01, -1.8662e+00],\n",
       "        [ 1.2191e+00,  1.2774e+00,  9.4031e-02, -1.9102e+00, -1.1414e+00],\n",
       "        [ 3.3611e-01, -3.9558e-01, -9.1052e-01, -2.6087e-01,  9.4020e-01],\n",
       "        [-1.0303e+00, -7.4836e-02,  4.1170e-01,  6.9865e-01, -9.7103e-01],\n",
       "        [ 6.8304e-01, -3.4465e-01, -4.8385e-01,  3.8700e-01, -6.1415e-01],\n",
       "        [ 4.2785e-01, -2.9852e-01,  1.7003e+00,  3.7909e-01,  4.7752e-01],\n",
       "        [ 4.7386e-03,  5.1368e-01, -1.0331e+00, -1.6041e-01, -2.0111e+00],\n",
       "        [ 1.0381e+00, -4.7553e-01, -5.2165e-01, -8.7164e-01, -1.2409e+00],\n",
       "        [-1.8133e-02, -2.6695e-01, -2.0867e+00, -1.4332e+00, -4.8996e-01],\n",
       "        [ 8.9862e-01,  6.8772e-01, -6.9759e-01, -2.9665e+00, -5.3648e-01],\n",
       "        [ 6.6710e-01, -1.6746e+00,  2.4957e-01, -5.1768e-01, -6.2108e-01],\n",
       "        [ 9.0157e-01,  9.8728e-01,  1.0798e+00,  2.6698e-01,  1.9129e-01],\n",
       "        [ 6.8412e-01, -9.9837e-01, -6.5762e-01,  9.1043e-01, -9.2495e-01],\n",
       "        [-1.4875e+00,  1.3119e+00,  1.7584e+00,  5.5112e-02, -1.3007e+00],\n",
       "        [ 1.6577e+00, -2.8830e-01, -1.7563e+00,  3.3975e-01, -1.9267e+00],\n",
       "        [ 8.9048e-01,  5.5571e-01, -1.1114e+00,  6.6896e-01, -1.5042e+00],\n",
       "        [ 1.3310e+00, -3.0560e-01,  9.7741e-01, -1.1049e+00, -2.0470e+00],\n",
       "        [ 4.8966e-01, -3.5461e-01, -3.0596e-01, -7.9324e-01, -1.3434e+00],\n",
       "        [ 1.8653e+00,  1.7096e+00,  1.5891e+00, -1.6319e+00, -2.6920e-01],\n",
       "        [ 9.4789e-01, -1.3522e+00,  4.6826e-01, -7.6759e-01, -2.1594e+00],\n",
       "        [ 8.6972e-01, -8.3600e-01, -4.4618e-01, -5.6113e-01, -2.7077e-01],\n",
       "        [ 5.6234e-01, -7.2923e-01,  7.7211e-01,  5.9361e-01, -2.7457e+00],\n",
       "        [ 6.6399e-01, -6.0193e-01, -3.6856e-01, -1.3199e+00, -2.9222e-01],\n",
       "        [ 1.4222e+00, -7.7914e-01, -1.1338e+00, -1.3001e+00, -8.8267e-01],\n",
       "        [ 7.7720e-01, -1.9868e-01, -1.0772e+00, -1.1076e+00, -1.6720e+00],\n",
       "        [-3.6086e-01, -2.7515e-01,  1.4322e-01, -2.2489e+00, -9.4087e-01],\n",
       "        [-5.9278e-01,  2.6560e-01,  5.0736e-01, -4.7889e-01, -2.0348e+00],\n",
       "        [ 1.3553e+00, -7.0106e-01, -2.3044e-01, -1.6703e+00,  3.4062e-01],\n",
       "        [ 5.6639e-01, -9.7594e-01,  1.0090e-01, -1.1906e+00, -1.1178e+00],\n",
       "        [-4.1898e-01,  1.5448e-01, -1.7752e+00,  5.3236e-01, -2.5578e+00],\n",
       "        [ 2.1657e+00, -9.0628e-01,  2.4666e-01,  8.7113e-01, -6.4746e-01],\n",
       "        [ 1.0150e+00,  1.1406e+00, -9.7644e-01,  2.8654e-01, -1.1581e+00],\n",
       "        [ 9.7142e-02, -2.9020e-02, -2.2761e-01, -1.5021e+00, -2.7825e-02],\n",
       "        [-1.1518e-01, -2.2600e+00, -9.5395e-01, -4.3528e-01, -2.4364e+00],\n",
       "        [-3.5726e-02, -1.5275e+00,  9.5719e-01,  4.6210e-01, -8.3850e-01],\n",
       "        [-1.1447e-01, -1.5708e+00, -6.9203e-01, -8.2321e-01, -2.0743e+00],\n",
       "        [ 7.5152e-01, -6.1023e-01, -6.7255e-01, -3.1791e+00, -3.7036e-01],\n",
       "        [ 2.0639e-01,  1.0755e-01, -2.5891e+00,  5.4903e-01, -1.3441e+00],\n",
       "        [-9.0596e-01,  1.3528e+00, -3.8898e-01, -3.0496e-01, -2.4661e+00],\n",
       "        [ 1.8472e-01, -8.9349e-02,  1.8199e+00, -1.8287e-01,  1.4160e-01],\n",
       "        [ 5.1995e-01, -2.3349e-01,  1.0138e-01, -4.5563e-01, -1.1831e+00],\n",
       "        [ 1.7089e+00, -1.3631e+00,  8.8420e-01, -1.1926e+00, -4.6161e-01],\n",
       "        [-4.8102e-01,  1.2092e+00, -5.3237e-01, -1.1349e+00,  3.5063e-03],\n",
       "        [ 9.1658e-01,  1.0347e-01, -2.0209e-01, -5.9176e-01,  4.6841e-01],\n",
       "        [ 7.9896e-01, -5.6760e-01,  2.0407e-01,  1.6572e-01, -3.4586e+00],\n",
       "        [ 6.7101e-01, -2.1026e+00,  1.3114e+00,  5.7797e-02,  1.0683e+00],\n",
       "        [ 1.5511e+00,  3.2749e-01, -1.5390e+00, -2.3937e-01, -1.7710e+00],\n",
       "        [ 9.7439e-01, -1.7488e-01, -4.3824e-01,  5.1801e-02, -2.1316e+00],\n",
       "        [ 3.6139e-01, -8.9172e-01, -2.8166e-01,  1.9481e-01,  2.3714e-01],\n",
       "        [ 2.1679e+00,  4.1718e-01,  3.0926e-01,  3.3911e-01, -1.5600e+00],\n",
       "        [-1.4637e-01,  1.0172e+00, -3.4236e-01, -1.9440e+00, -1.8206e+00],\n",
       "        [-2.4575e-01, -1.5368e+00, -1.3831e+00,  2.1400e-01, -1.3332e+00],\n",
       "        [ 1.6174e+00, -4.6784e-01, -2.0258e-01, -1.5002e-01, -1.1070e+00],\n",
       "        [-3.2447e-01, -8.6092e-01, -7.2703e-01, -1.1686e+00, -1.5922e+00],\n",
       "        [-4.1244e-01, -9.8998e-01, -1.3237e+00, -9.4289e-01, -7.9683e-01],\n",
       "        [ 4.5666e-01, -1.3607e+00, -5.1446e-01,  2.5883e-01, -3.6898e-01],\n",
       "        [-6.0580e-01,  9.6023e-01, -1.0001e+00, -1.9308e+00, -7.7415e-01],\n",
       "        [ 1.4078e-01, -1.4828e+00,  5.8546e-01, -1.2643e+00, -1.6919e+00],\n",
       "        [ 7.0713e-01,  1.7650e+00, -1.2349e+00, -5.4949e-01, -1.7201e-01],\n",
       "        [ 5.9744e-01, -3.1950e-01,  5.8801e-01,  6.3391e-01, -1.0887e+00],\n",
       "        [ 4.2815e-01, -9.0870e-01, -1.9359e-01,  4.6235e-01, -8.3492e-02],\n",
       "        [-3.3039e-01,  1.9260e+00,  5.9336e-01, -1.0167e+00, -1.1437e+00],\n",
       "        [ 1.9967e-01, -1.3432e+00, -7.3979e-01, -6.6538e-01, -1.7279e+00],\n",
       "        [ 2.2686e-01,  3.2312e-02,  1.2398e+00, -1.2536e+00, -4.0986e-01],\n",
       "        [ 2.3849e-01, -1.3443e+00, -1.4307e+00,  2.6575e-01, -1.9120e+00],\n",
       "        [ 9.3470e-01, -1.5488e+00, -6.8210e-01, -5.1445e-01,  1.8266e-02],\n",
       "        [ 2.1039e+00,  1.8200e-01,  4.4248e-01, -2.5368e+00, -2.1028e+00],\n",
       "        [ 1.6149e+00, -6.6962e-01,  2.4276e+00,  2.7506e-01, -3.0394e+00],\n",
       "        [ 2.0746e-01, -6.9721e-01, -1.2505e-01, -1.1252e+00, -1.7313e+00],\n",
       "        [ 1.8709e-01, -1.1634e+00,  5.6352e-01,  8.1729e-01, -1.7578e+00],\n",
       "        [-9.1005e-01, -5.3653e-01,  2.6282e-01, -4.1224e-01, -1.8406e+00],\n",
       "        [ 7.6186e-01, -1.1533e-01, -7.6063e-01, -2.6193e-01, -9.5030e-01],\n",
       "        [-1.0933e+00, -4.6667e-01,  9.8590e-01, -5.0443e-01,  1.2042e+00],\n",
       "        [ 1.0666e+00, -4.3637e-01,  8.2500e-01, -1.0148e+00, -9.0379e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 5\n",
    "encoder_classifier = TransformerClassifier(config)\n",
    "output = encoder_classifier(inputs.input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0c8481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2858461 ,  2.291011  ,  1.1027378 , -1.3654897 ,  0.50570506],\n",
       "       [-0.43984768,  1.707121  , -0.33323628, -1.3266757 ,  0.0403869 ],\n",
       "       [ 1.2801098 , -0.38431922, -0.89034826,  0.26452443,  0.01639184],\n",
       "       [ 0.787185  ,  0.47547337, -0.2487884 ,  0.93552494,  0.02594745]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "11ede128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.30067572, -0.19365877,  1.5731525 ,  0.34049684,  0.61987585],\n",
       "       [ 1.599041  ,  0.65223604, -0.30029094, -0.67572856,  1.787407  ],\n",
       "       [ 1.168354  ,  0.27019048,  0.68573517, -0.11251289,  0.5283011 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(x, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False,\n",
    "                  padding=True ) # don't use pad, sep tokens\n",
    "                    \n",
    "output = encoder_classifier(inputs.input_ids)\n",
    "output.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d8f51137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.1925466 , -2.004026  ,  0.37853348, -0.53329134,  3.518763  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x):\n",
    "    inputs = x\n",
    "    inputs = tokenizer(inputs, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False,\n",
    "                      ) # don't use pad, sep tokens\n",
    "    output = encoder_classifier(inputs.input_ids)\n",
    "    output = output.detach().numpy()\n",
    "    return output\n",
    "encoder([\"julia\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78f8a7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.4516523 , -0.29492986, -1.4100991 , -0.5428195 ,  1.6403229 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder([\"julia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6e1c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "[[ 1.8389405  -1.0258632  -1.9472094   0.24997222  2.9709525 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.2937596  -0.55749947 -1.5676594  -0.10609275  2.590736  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.7001611  -0.7492791  -1.4172636   0.41050863  1.5200357 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.38916868 -2.6472218  -1.4488543   0.6265552   2.8951118 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.47231916 -0.53634554 -0.5050098  -0.97858167  2.0714037 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.1571622  -0.14157459 -0.44913638  0.04301509  2.4572744 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.1925933  -2.0172298  -0.03272817  0.05695865  4.1540723 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.26143798 -0.39048213 -0.5843675  -0.12257874  1.6031576 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.00853661 -1.8508751  -0.90174556  0.15717727  0.42443526]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.10971469 -0.96365035 -1.2132721   0.33689216  0.30131382]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.90288585 -1.5594695  -0.31239155  0.93199253  0.9063376 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.575763   -1.474622   -0.01893973 -1.6097691   1.625571  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.2535087  -0.69128287 -1.3176934   0.89493334  2.8466616 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.83197725  0.6289131  -0.68537045  0.7565023   0.5172089 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.0083814  -0.34590343  0.36498713 -0.28030363  1.0539081 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.8730753 -0.437684  -1.0078217  0.6640351  1.1810595]]\n",
      "torch.Size([1, 768])\n",
      "[[ 2.5567908  -0.6370684  -0.46333343  0.81403226  3.3931787 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.08064005 -0.54463744 -0.7790845   1.1092944   3.923053  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.61899036 -1.3680719  -1.1154611   0.6824911   0.77538145]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.33561593 -2.0343149  -1.524899    1.6989415   2.596325  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.2722288  -0.3572669  -0.9616324   0.10787922  3.452022  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.4053358  -0.39284718 -0.38204122 -0.5995668   1.8725339 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.51498365 -1.5810821  -2.2393973   0.70850456  0.48347938]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.59225935 -0.39468363 -0.00732887  0.4983155   1.4752239 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.03592961 -1.4822865  -1.620661    0.117576    1.4544673 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.5614438   0.38075566 -1.739773   -0.3653347   1.0539811 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-2.1210542   0.8818643   1.1112943   0.48594812  1.6453836 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.32551077 -0.50986385 -0.502342   -0.04689592  3.6750865 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.13877499 -0.15563437 -0.18800986  0.5182731  -0.6704424 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.11773807 -0.16428983 -2.3842516   0.6039371   1.92588   ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 2.1425457  -0.11512989 -1.0949836   0.75699246  1.7416682 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.3208417  -1.5464118   0.62125015 -0.29034024  2.550439  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.60144365 -0.6072734  -1.378515    1.053808   -0.98943233]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.32254073  0.38350117 -0.18454117  0.03002122  1.4879704 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.09191891 -1.0092053  -1.630301   -0.3063442   2.4158995 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.4171348 -1.353207  -1.0982419  1.6467578  1.3735278]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.48716962 -0.32202932 -2.1942446   0.55379605  0.7841842 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.2397477   0.22638905 -0.30939156  0.835444    2.2260337 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.4308114   0.17134306 -0.5939125  -0.0462442   1.8684789 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.626323    0.06796354 -0.68810093  1.3190384   2.118757  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.6390155  -2.112834   -0.5299003  -0.05023673  0.7252248 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.25856912 -0.7226182  -2.2387235  -0.23198855  1.1656077 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.274028  -1.7574086  0.096921   1.8841717  2.709712 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.36222175 -1.3736453  -0.40836054  0.15382233  1.7772515 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.23381081 -0.23392355 -1.9281043   0.39893186  3.5724769 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.4431894   0.72074956 -0.5359353  -1.6675327   2.0148437 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.2262979  -0.8515811   0.67380524  0.29503873  2.5982652 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.8360472  -0.18120378 -0.69020474 -1.4587284   1.9068171 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.04131383  0.11164767 -0.8080493  -0.52783376 -2.3015463 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.39576072 -1.1639065  -1.4804735  -0.12025744  2.436457  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.4484625 -2.0538921 -1.6958259  0.9933771  2.2242017]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.4160248  -1.3930333  -0.18942246  0.623612    0.08719949]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.43605617 -2.2344255  -0.39197445  0.44472963  1.8808546 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.89805245  0.22225529 -0.6375909  -0.6696243   2.0862026 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.1713114   0.7319886  -0.40045345  1.1540313   2.8416429 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.32465222  0.48543108 -0.9959302   1.2915214   3.5112507 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.99878085 -0.8349626  -0.92004585  1.3523242   3.560196  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.52633226 -0.99074924 -0.960544    0.9033803   1.8321087 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.23120403  0.7676372  -1.2836509  -0.22950771  1.865881  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.8924391 -0.7007999 -0.571493   1.5513531  3.5236638]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.6974694  -0.08364642 -1.7986574   1.113228    1.8901657 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.5756172  -0.14693576 -1.1404445  -0.5029555   0.75923127]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.4092139 -0.3651498 -2.7156615 -1.0569198  2.508409 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.3930943  -0.3135575  -0.8457608  -0.46265143  2.0891001 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.298455  -1.0888726 -1.8916965 -0.802361   2.314303 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.5716372  -0.59711695 -0.64246696  1.3168776   1.0320425 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.22180796 -0.18839769 -0.36290097 -0.2057769   2.7437599 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.5611175  -1.6015449  -0.7997215  -0.37230211  1.248783  ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.25713152 -1.0948389  -0.3544342   1.1701956   3.397933  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.9160604 -0.7798491 -1.5856413  0.0536595  3.0639994]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.7685388 -1.099858  -1.1542227  0.4794194  2.1750016]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.51072925 -1.1783179  -1.5086462   2.6846726   2.0703006 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.9705226 -1.7047067 -1.7506663 -1.1098412  1.771829 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.68977666 -1.3588881  -0.62999654 -0.1804983   2.2404003 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.5550665  -0.90361416 -1.3780769   0.12957186  1.5978682 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.45881492 -0.8827839  -1.7530721  -0.72764385  1.6451476 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.1524477  -1.0928568  -0.8808977   0.6768347  -0.42297417]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.3111907  -0.37980264 -0.7307807  -0.75210446  2.6713438 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.08552271 -0.7862924  -0.40623942 -0.021007   -0.02159745]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.7102864   0.36682367 -1.1829648  -0.56853473  1.9983189 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.4110166  -0.40803856 -1.9191628  -0.15314221  2.015008  ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.25014094  0.5698333   0.71770227 -0.07571691  0.99761754]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.3229557  -0.41547495 -2.289988   -0.5374194   3.8663256 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.67896247  0.26633394 -0.44540396  1.362362    1.1451045 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.58874834 -1.4151895  -2.155796    0.230243    1.3187569 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.4467368   0.18704404  0.8273399  -0.7809509   2.2522595 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.614736   -0.8856894   1.2427907   0.08576655  0.40143383]]\n",
      "torch.Size([1, 768])\n",
      "[[-1.1888365  -0.6517862  -1.6726769  -0.1863113   0.11790869]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.9329157 -1.4354485 -1.3888346 -1.454856   2.7464812]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.21539357  0.19353107 -0.3561064  -0.67020607  2.3526652 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.3208334 -0.772766  -0.3280922 -0.8530589  2.1623845]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.8257766  -2.7141287  -1.769961   -0.61564994  0.90081394]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.45673493 -0.5919855  -0.47654578 -0.3692783   3.2641232 ]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.0165489  -0.5201651  -2.5520673  -0.14356697  0.99685526]]\n",
      "torch.Size([1, 768])\n",
      "[[-0.06110967 -2.463143   -0.8752372   1.514087    1.1939648 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.298265   -2.6253521  -0.41841543  1.9849263   3.3019104 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.18694386 -2.2086885  -0.4890774   0.53339815  1.7385726 ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.1013749 -1.6154352 -2.458239  -0.8619126  2.6202486]]\n",
      "torch.Size([1, 768])\n",
      "[[ 0.95002925  1.3246802  -2.005942   -0.52968824  0.12671   ]]\n",
      "torch.Size([1, 768])\n",
      "[[ 1.1053149  -0.22370335  0.4003247  -0.65588975  2.293155  ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(encoder([\"julia\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e89c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "class Embeddings:\n",
    "    #CLS is a special classification token and the last hidden state of BERT Embedding\n",
    "    def cls_pooling(self, model_output):\n",
    "        return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "    #BERT tokenizer of input text\n",
    "    def get_embeddings(self, text_list):\n",
    "        encoded_input = tokenizer(\n",
    "            text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        model_output = model(**encoded_input)\n",
    "        return self.cls_pooling(model_output).cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "    #convert dataset into embeddings dataset to run FAISS\n",
    "    def makeEmbeddings(self,dataset):\n",
    "        embeddings = []\n",
    "        for data in dataset:\n",
    "            embeddings.append(self.get_embeddings(data)[0])\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def getQueryEmbedding(self, query):\n",
    "        return self.get_embeddings([query])\n",
    "    \n",
    "class Faiss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def faiss(self,xb):\n",
    "        d = xb[0].size\n",
    "        M = 32\n",
    "        index = faiss.IndexHNSWFlat(d, M)            \n",
    "        index.hnsw.efConstruction = 40         # Setting the value for efConstruction.\n",
    "        index.hnsw.efSearch = 16               # Setting the value for efSearch.\n",
    "        index.add(xb)\n",
    "        return index\n",
    "    \n",
    "    def query(self,index,xq,k=3):\n",
    "        D, I = index.search(xq, k)   \n",
    "        return D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57189376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = encoder([\"julia is nice\", \"isabelle is planning\", \"julia plans too\"])\n",
    "xq = encoder([\"isabelle plans it\"])\n",
    "index = Faiss().faiss(xb)\n",
    "D,I = Faiss().query(index,xq)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7b59a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"SEC-CompanyTicker.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9ab44d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [233]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mcompanyName)\n\u001b[1;32m      2\u001b[0m q \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple Inc.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m xb,xq \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m Faiss()\u001b[38;5;241m.\u001b[39mfaiss(xb)\n\u001b[1;32m      5\u001b[0m D,I \u001b[38;5;241m=\u001b[39m Faiss()\u001b[38;5;241m.\u001b[39mquery(index,xb)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "x = list(df.head(20).companyName)\n",
    "q = [\"Apple Inc.\"]\n",
    "xb,xq = encoder(x,q)\n",
    "index = Faiss().faiss(xb)\n",
    "D,I = Faiss().query(index,xb)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c70c56e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple Inc.',\n",
       " 'Microsoft Corp',\n",
       " 'Alphabet Inc.',\n",
       " 'Amazon Com Inc',\n",
       " 'Nvidia Corp',\n",
       " 'Tesla, Inc.',\n",
       " 'Berkshire Hathaway Inc',\n",
       " 'Meta Platforms, Inc.',\n",
       " 'Eli Lilly & Co',\n",
       " 'Visa Inc.',\n",
       " 'Taiwan Semiconductor Manufacturing Co Ltd',\n",
       " 'Exxon Mobil Corp',\n",
       " 'Unitedhealth Group Inc',\n",
       " 'Walmart Inc.',\n",
       " 'Novo Nordisk A S',\n",
       " 'Jpmorgan Chase & Co',\n",
       " 'Spdr S&P 500 Etf Trust',\n",
       " 'Johnson & Johnson',\n",
       " 'Mastercard Inc',\n",
       " 'Lvmh Moet Hennessy Louis Vuitton']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "71a22b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.6796455 , -0.26035303,  0.939491  , -0.8765463 ,  1.2847595 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"am\"\n",
    "encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485038c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.261257,
   "end_time": "2023-11-07T12:12:13.753168",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-07T12:11:25.491911",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f54ff68288a47e4ab08fe1bbfc33d55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23e2a9ed016a4afcbce85d623df9d64e",
       "max": 28,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fa9b11bb2bba4257a632795bfe9f9b6e",
       "value": 28
      }
     },
     "0f6539d2bc1244888a6850ce92c31d4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ac3fe35df01454abc6b7a3bfdf4df94",
        "IPY_MODEL_dcad6910ef2b43e7a8001ab17ab10112",
        "IPY_MODEL_57bbb2ada9f0494aa4335a1418e1809e"
       ],
       "layout": "IPY_MODEL_79f1527694f74b80a5330cf6f99ea84e"
      }
     },
     "0fb39f643b2f4811be0d0c3409323d80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "17348793a7bb456bbb5ace7f13f537f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_98d3a93be64a4c01a2d1c98d00515f5a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4698dfa31864465ab5841f5b2d8c6e5c",
       "value": " 570/570 [00:00&lt;00:00, 25.3kB/s]"
      }
     },
     "1ac3fe35df01454abc6b7a3bfdf4df94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f5a281ae964e44528a7773b0213cb3a9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4c762ca4426949989ec3bf96ef853188",
       "value": "Downloading: 100%"
      }
     },
     "23e2a9ed016a4afcbce85d623df9d64e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c55c0d364b742ed8da1f4194e9d73db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2ecc0ca0282a4b51a9f47f299ddb0e25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "402bec0969c2439d9fe12534438cfd18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4698dfa31864465ab5841f5b2d8c6e5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4c762ca4426949989ec3bf96ef853188": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "508357fee53643ae9bbc41e2cbb0c6b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52ea49af22194ceeb7d0014756942b23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5653cd0054e34263946603a7d6c3e3b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "57bbb2ada9f0494aa4335a1418e1809e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_caf3de995ab1481fadb9cf72b9c573fc",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0fb39f643b2f4811be0d0c3409323d80",
       "value": " 226k/226k [00:00&lt;00:00, 4.39MB/s]"
      }
     },
     "5865b31e0bf24e4e80dd75fa252761e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58ec168261d746b2a2a803f6a87449c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "656311e69fa04ec7b6ddb361d470b11b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67c471286bcd4dcbb385a93887879a02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "694fd1a41ff147edb6eb32dd33fdb7c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b365053df97498e956e67bbe6b9ccfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2ecc0ca0282a4b51a9f47f299ddb0e25",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_5653cd0054e34263946603a7d6c3e3b8",
       "value": "Downloading: 100%"
      }
     },
     "6d448241463c48228efdfbe658d57acf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d59d2793acc4f30a0537f06d94c947b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "76a8d4885d704b68a49cc354bd98162d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_67c471286bcd4dcbb385a93887879a02",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_834207598a374281ab4b2457b33ac1f1",
       "value": "Downloading: 100%"
      }
     },
     "7909f11c3d9d4884bef998dc86c6c433": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_694fd1a41ff147edb6eb32dd33fdb7c7",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_402bec0969c2439d9fe12534438cfd18",
       "value": " 455k/455k [00:00&lt;00:00, 5.73MB/s]"
      }
     },
     "79f1527694f74b80a5330cf6f99ea84e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d3e823820b9404eb59a5fc22f235788": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7efc71dd48044f4182faee7196648d75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_656311e69fa04ec7b6ddb361d470b11b",
       "max": 570,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_52ea49af22194ceeb7d0014756942b23",
       "value": 570
      }
     },
     "834207598a374281ab4b2457b33ac1f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9444d2913a734d248a9741f3727b10f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98d3a93be64a4c01a2d1c98d00515f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c01269053814648ac62141686c8ff2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a28f0dbdde2448c4b1a3290a62c43eba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b365053df97498e956e67bbe6b9ccfd",
        "IPY_MODEL_7efc71dd48044f4182faee7196648d75",
        "IPY_MODEL_17348793a7bb456bbb5ace7f13f537f6"
       ],
       "layout": "IPY_MODEL_7d3e823820b9404eb59a5fc22f235788"
      }
     },
     "a80cfef718d84bc8967f53cdf83a5986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_508357fee53643ae9bbc41e2cbb0c6b0",
       "max": 466062,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6d59d2793acc4f30a0537f06d94c947b",
       "value": 466062
      }
     },
     "acf742bc1b9b4f71ae1f9ec994d85847": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8784b0f034242d3b9551a4b45615e53",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9c01269053814648ac62141686c8ff2a",
       "value": " 28.0/28.0 [00:00&lt;00:00, 1.15kB/s]"
      }
     },
     "b4e4504a77a3486b9f66e38cebc63182": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8784b0f034242d3b9551a4b45615e53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c067bf2601a84417bf052635a54ef790": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ce198efee07e4dd582a7c889f3b5f6d9",
        "IPY_MODEL_a80cfef718d84bc8967f53cdf83a5986",
        "IPY_MODEL_7909f11c3d9d4884bef998dc86c6c433"
       ],
       "layout": "IPY_MODEL_9444d2913a734d248a9741f3727b10f9"
      }
     },
     "c61426e6570245e6b1c07b09149e61bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76a8d4885d704b68a49cc354bd98162d",
        "IPY_MODEL_0f54ff68288a47e4ab08fe1bbfc33d55",
        "IPY_MODEL_acf742bc1b9b4f71ae1f9ec994d85847"
       ],
       "layout": "IPY_MODEL_5865b31e0bf24e4e80dd75fa252761e1"
      }
     },
     "caf3de995ab1481fadb9cf72b9c573fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce198efee07e4dd582a7c889f3b5f6d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c55c0d364b742ed8da1f4194e9d73db",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_6d448241463c48228efdfbe658d57acf",
       "value": "Downloading: 100%"
      }
     },
     "dcad6910ef2b43e7a8001ab17ab10112": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b4e4504a77a3486b9f66e38cebc63182",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_58ec168261d746b2a2a803f6a87449c5",
       "value": 231508
      }
     },
     "f5a281ae964e44528a7773b0213cb3a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa9b11bb2bba4257a632795bfe9f9b6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
