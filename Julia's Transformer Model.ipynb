{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a069a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "def sdp_attention(query, key, value):\n",
    "    dim_k = query.size(-1) # dimension component\n",
    "    sfact = sqrt(dim_k)     \n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sfact\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "'''\n",
    "\n",
    "Attention Class\n",
    "\n",
    "# nn.linear : apply linear transformation to incoming data\n",
    "#             y = x * A^T + b\n",
    "# Ax = b where x is input, b is output, A is weight\n",
    "\n",
    "# calculate scaled dot product attention matrix\n",
    "# Requires embedding dimension \n",
    "# Each attention head is made of different q,k,v vectors\n",
    "\n",
    "'''\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # initalisation \n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the three vectors\n",
    "        # input - embed_dim, output - head_dim\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # main class operation\n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # calculate scaled dot product given a \n",
    "        attn_outputs = sdp_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state))\n",
    "        \n",
    "        return attn_outputs\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "Multihead attention class\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class multiHeadAttention(nn.Module):\n",
    "    \n",
    "    # Config during initalisation\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # model params, read from config file\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # attention head (define only w/o hidden state)\n",
    "        # each attention head is initialised with embedd/heads head dimension\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # output uses whole embedding dimension for output\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # Given a hidden state (embeddings)\n",
    "    # Apply operation for multihead attention\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # for each head embed_size/heads, calculate attention\n",
    "        heads = [head(hidden_state) for head in self.heads] \n",
    "        x = torch.cat(heads, dim=-1) # merge/concat head data together\n",
    "    \n",
    "        # apply linear transformation to multihead attension scalar product\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class feedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # define layer operations input x\n",
    "        \n",
    "    def forward(self, x):    # note must be forward\n",
    "        x = self.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class encoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = multiHeadAttention(config)    # multihead attention layer \n",
    "        self.feed_forward = feedForward(config)        # feed forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer norm. to hidden state, copy input into query, key, value\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "Token + Position Embedding \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class tpEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding layer\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        \n",
    "        # positional embedding layer\n",
    "        # config.max_position_embeddings -> max number of positions in text 512 (tokens)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1) # number of tokens\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)[None,:] # range(0,9)\n",
    "        \n",
    "        # tensor([[ 1996, 11286,  1997,  1037,  5340,  3392,  2003,  2200,  5931]])\n",
    "        # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "        \n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Add normalisation & dropout layers\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    \n",
    "# full transformer encoder combining the `Embedding` with the ``Embedding` ` layers\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):       \n",
    "        super().__init__()\n",
    "        \n",
    "        # token & positional embedding layer\n",
    "        self.embeddings = tpEmbedding(config)\n",
    "        \n",
    "        # attention & forward feed layer \n",
    "        self.layers = nn.ModuleList([encoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embeddings layer output\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # cycle through all heads\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = x[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "72141ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 10\n",
      "epoch 20\n",
      "epoch 30\n",
      "epoch 40\n",
      "epoch 50\n",
      "epoch 60\n",
      "epoch 70\n",
      "epoch 80\n",
      "epoch 90\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "import torch\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TrainEmbeddings():\n",
    "    def __init__(self):\n",
    "        self.model = TransformerEncoder(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        \n",
    "    def getTensor(self,text):\n",
    "        inputs = self.tokenizer(text, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False,\n",
    "                          padding=True) # don't use pad, sep tokens\n",
    "        return inputs.input_ids\n",
    "    \n",
    "    def train(self,data,epochs=100):\n",
    "        self.model.train()\n",
    "        tensorData = self.getTensor(data)\n",
    "        for epoch in range(epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"epoch %s\" % epoch)\n",
    "            self.model.forward(tensorData)\n",
    "    \n",
    "    def getEmbeddings(self,q):\n",
    "        self.model.eval()\n",
    "        inputs = self.getTensor(q)\n",
    "        with torch.no_grad():\n",
    "            x = self.model(inputs).cpu().detach().numpy()\n",
    "            return x\n",
    "        \n",
    "\n",
    "data = list(pd.read_csv(\"SEC-CompanyTicker.csv\",index_col=0).companyName[:100])\n",
    "train = TrainEmbeddings()\n",
    "train.train(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "311d83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 10\n",
      "epoch 20\n",
      "epoch 30\n",
      "epoch 40\n",
      "epoch 50\n",
      "epoch 60\n",
      "epoch 70\n",
      "epoch 80\n",
      "epoch 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-6.98212534e-05,  3.07853334e-05,  6.64498657e-04,\n",
       "         1.17308239e-03, -1.21132156e-03, -9.26667824e-04,\n",
       "         8.40999011e-04,  1.16835779e-03, -6.53050549e-04,\n",
       "        -4.90022358e-04,  9.61003185e-04, -1.99670743e-04,\n",
       "        -5.90704847e-04,  8.53392121e-04, -6.32833398e-04,\n",
       "        -2.36460633e-04,  3.74554656e-04,  1.29150227e-04,\n",
       "        -1.07880402e-03, -1.23031484e-03,  9.52052884e-04,\n",
       "         6.60190359e-04,  8.79907981e-04,  9.93314534e-05,\n",
       "         8.26938835e-04, -4.43407014e-04, -1.23229343e-04,\n",
       "         7.51116313e-04, -9.79379867e-04, -5.12513507e-04,\n",
       "        -9.78070660e-04, -1.21099256e-04,  1.24194252e-03,\n",
       "        -9.53016512e-04, -3.03876121e-04, -2.52310041e-04,\n",
       "         1.05174957e-03, -7.72252039e-04,  5.88052580e-06,\n",
       "        -6.18975784e-04, -1.25046226e-03,  6.51991286e-04,\n",
       "        -1.14057108e-03, -5.71852259e-04, -4.57031047e-06,\n",
       "        -3.85652929e-05, -9.97557305e-04,  1.25191966e-03,\n",
       "         6.48705463e-04,  1.20223220e-03, -1.06222881e-03,\n",
       "         5.85390429e-04, -5.38681808e-04,  1.07361469e-04,\n",
       "         1.10659108e-03, -5.81012573e-04,  5.88216179e-04,\n",
       "        -8.83718778e-04, -4.62042779e-04,  1.22376403e-03,\n",
       "        -2.05423523e-04,  4.18452546e-05, -5.39144501e-04,\n",
       "        -1.00034999e-03, -1.96355235e-04,  3.21587868e-04,\n",
       "        -1.15628507e-04,  7.20528886e-04, -3.57158482e-04,\n",
       "         2.94279307e-04,  7.10389868e-04,  1.08671270e-03,\n",
       "        -1.89289145e-04, -1.19897688e-03,  5.69082331e-04,\n",
       "         7.44511708e-05,  9.68998473e-04, -1.05896186e-04,\n",
       "        -3.43543477e-04, -1.13971473e-03, -1.11530848e-04,\n",
       "         3.68042063e-04,  7.03311060e-04,  9.18314618e-04,\n",
       "        -7.42593955e-04,  2.42033813e-04,  7.92820763e-04,\n",
       "        -6.24746259e-04, -4.04591206e-04,  8.85108020e-04,\n",
       "         2.12431725e-04,  2.47287880e-05,  4.52296488e-04,\n",
       "         2.83564441e-05,  1.25245133e-03,  6.58932782e-04,\n",
       "        -1.16111850e-03, -9.16869845e-04,  1.17377065e-04,\n",
       "         8.32361169e-04, -1.12235511e-03,  4.77309659e-04,\n",
       "         6.75766089e-04,  7.47648242e-04,  9.72254958e-04,\n",
       "        -8.03082716e-04,  1.43960118e-04,  7.87406520e-04,\n",
       "        -3.69798247e-04, -8.03844072e-04, -5.34144529e-05,\n",
       "        -1.08970690e-03, -7.29168300e-04,  9.25070141e-04,\n",
       "         4.36528586e-04,  9.40842379e-04,  8.85448884e-04,\n",
       "         9.80565324e-04, -4.93379484e-04, -7.31518157e-05,\n",
       "         3.05778201e-04, -5.88415656e-04,  1.09228271e-03,\n",
       "        -1.28361501e-03,  8.80812586e-04,  3.79481353e-04,\n",
       "        -6.42295752e-04,  5.72680670e-04, -2.26507123e-04,\n",
       "         8.73878133e-04,  1.29750650e-03, -5.68026502e-04,\n",
       "        -7.80387782e-05, -7.41619442e-04,  5.01409173e-04,\n",
       "         3.62842024e-04,  8.97275575e-04,  7.94413500e-04,\n",
       "         1.24199176e-03,  1.20747613e-03,  1.02839433e-03,\n",
       "        -9.10091680e-04, -1.19216985e-03, -4.63219658e-05,\n",
       "        -4.03625105e-04,  1.02790585e-03,  7.73251813e-04,\n",
       "        -2.01258183e-04,  1.96740031e-04,  2.33078215e-04,\n",
       "         1.01791287e-03, -1.23830559e-03, -2.67618652e-05,\n",
       "         4.51718312e-04, -1.22262005e-04,  1.09137653e-03,\n",
       "         1.17327913e-03,  8.51107587e-04, -9.26589928e-05,\n",
       "         1.00395887e-03, -1.11124152e-03,  4.17592004e-04,\n",
       "        -6.03905879e-04, -6.62624370e-04,  4.67398204e-04,\n",
       "         6.99262891e-04,  1.01165555e-03, -7.50847161e-04,\n",
       "         9.67885542e-04,  8.62694811e-04, -4.83046897e-04,\n",
       "        -1.13875547e-03,  7.08003528e-04,  8.47624440e-04,\n",
       "        -1.02545600e-04, -8.73679121e-04, -9.22646548e-04,\n",
       "        -3.25138069e-04,  6.69694506e-04, -4.77244466e-04,\n",
       "        -1.22005993e-03,  4.98273410e-04,  6.35999895e-04,\n",
       "        -8.37052532e-04,  1.57364339e-04, -2.70166289e-04,\n",
       "         3.17751733e-06, -1.28691515e-03,  3.50521412e-04,\n",
       "        -6.18503429e-04,  1.41620636e-04, -2.05237578e-04,\n",
       "         2.86025141e-04, -1.02624681e-03, -3.53799987e-04,\n",
       "         3.46770656e-04,  6.96182542e-04, -3.11395153e-04,\n",
       "        -1.23829348e-03,  5.86702954e-04,  1.23130158e-05,\n",
       "         4.00692690e-04, -8.87063157e-04, -1.79097056e-04,\n",
       "         9.98513191e-04,  9.56563745e-04, -4.78293892e-04,\n",
       "         3.44101805e-04, -1.08295958e-03,  8.08005978e-04,\n",
       "        -6.03818044e-04, -4.11993038e-04,  1.21241622e-03,\n",
       "         1.13722090e-04,  9.75351955e-04, -7.90893566e-04,\n",
       "         6.71941030e-04,  1.29203417e-03, -1.10122282e-03,\n",
       "        -6.68709807e-04, -9.19900660e-04, -6.33157790e-04,\n",
       "        -4.92000487e-04, -1.11148425e-03,  1.03588623e-03,\n",
       "        -6.30721159e-04,  1.09682465e-03,  6.85230538e-04,\n",
       "        -8.52867961e-04,  5.15347812e-04,  7.12259032e-04,\n",
       "        -9.66996886e-04, -9.64286446e-04, -3.22295673e-04,\n",
       "        -1.12314138e-03, -2.05933888e-04, -5.25303185e-05,\n",
       "         4.29646432e-04,  1.87744852e-04, -1.14768431e-04,\n",
       "        -7.28392974e-04,  2.25308046e-04, -1.16845280e-04,\n",
       "         8.84595152e-04,  5.17394568e-04,  5.89774922e-04,\n",
       "         1.86758582e-04, -3.51543684e-04, -5.68595424e-04,\n",
       "        -1.34384725e-04,  1.87112950e-04, -3.44532396e-04,\n",
       "        -9.21065453e-04, -1.01631600e-03, -1.18773256e-03,\n",
       "        -7.72808504e-04, -2.40550071e-04, -5.63004054e-04,\n",
       "        -8.41233123e-04, -4.84026357e-04,  5.58484171e-04,\n",
       "        -4.86854609e-04,  1.09090831e-03,  1.99738730e-04,\n",
       "        -9.43010382e-04,  1.22835918e-03,  9.93647496e-04,\n",
       "         7.15271104e-04, -8.91776755e-04,  7.58161303e-04,\n",
       "         5.22017304e-04,  6.75178308e-04,  5.54153870e-04,\n",
       "         2.52572208e-04, -4.12781577e-04,  1.08774018e-03,\n",
       "         1.25158590e-03,  4.93828498e-04, -3.69400397e-04,\n",
       "         9.28062946e-07,  1.58700321e-04, -1.10134436e-03,\n",
       "        -1.07082620e-03, -3.00801676e-05,  1.61105141e-04,\n",
       "        -7.47836020e-04, -6.15270052e-04, -9.56520147e-04,\n",
       "         1.08445517e-03,  1.57939885e-05, -5.87161339e-04,\n",
       "         7.42409553e-04,  1.19531446e-03, -5.33837534e-04,\n",
       "         1.03706785e-03,  6.99926342e-04,  7.65510835e-04,\n",
       "         6.67435452e-05,  1.06941198e-03, -9.13937576e-04,\n",
       "        -1.07326533e-03,  1.21085346e-03, -2.57370993e-05,\n",
       "        -2.56155763e-04,  5.99431049e-04, -5.33244282e-04,\n",
       "         3.57176323e-04,  9.03641514e-04,  7.89768994e-04,\n",
       "        -9.77967982e-04,  1.22166018e-03,  6.08308415e-04,\n",
       "         5.16421918e-04, -8.12956423e-04,  1.10155984e-03,\n",
       "        -2.79969390e-04,  1.14911294e-03, -6.98177435e-04,\n",
       "        -1.05851807e-03,  8.88614450e-04,  2.17603214e-04,\n",
       "        -2.86264200e-04,  1.23875018e-03,  1.23617903e-03,\n",
       "        -1.27266243e-03,  3.26201640e-04,  8.01652670e-04,\n",
       "         5.04226133e-04,  2.63383758e-04,  5.60549088e-05,\n",
       "         8.77124257e-05, -4.97478701e-04, -9.29720060e-04,\n",
       "        -2.71988567e-04,  5.10924205e-04,  1.14826614e-03,\n",
       "         1.20561849e-03, -7.78116751e-04, -1.22430606e-03,\n",
       "         1.27140328e-03,  4.46586550e-04,  6.72671478e-04,\n",
       "         8.18013679e-04, -3.65138374e-04,  9.53477051e-04,\n",
       "         3.68524954e-04,  3.73828690e-04, -3.09944007e-04,\n",
       "        -4.07324173e-04, -3.08612216e-04,  5.56827697e-04,\n",
       "         9.90337412e-06, -1.24795304e-03, -1.25853438e-03,\n",
       "        -8.00546084e-04, -1.67408343e-05,  2.60080211e-04,\n",
       "         1.22812076e-03,  7.27128994e-04, -5.58684405e-04,\n",
       "         3.62391584e-05,  6.46400906e-04,  1.00238400e-03,\n",
       "        -1.48987398e-04,  5.62945381e-04, -7.57080677e-04,\n",
       "        -1.04712322e-04,  1.05469406e-03, -3.07300128e-04,\n",
       "        -1.25826243e-03,  7.52507884e-04, -5.11695631e-04,\n",
       "        -1.59228221e-04,  1.29954657e-03, -2.93795631e-04,\n",
       "        -6.19409431e-04, -6.93930604e-04,  9.08970076e-04,\n",
       "        -7.43342738e-04,  2.75216531e-04, -6.84330706e-04,\n",
       "         7.96967943e-04,  5.67357696e-04,  3.39369144e-04,\n",
       "        -1.94151420e-04, -3.57560348e-04,  1.17095525e-03,\n",
       "         6.79137360e-04, -2.81578075e-04, -1.23311335e-03,\n",
       "        -9.66933847e-04, -1.38507996e-04, -1.03508741e-04,\n",
       "        -3.33712116e-04,  1.26077095e-03, -5.97032085e-05,\n",
       "         7.64812634e-04, -9.69737943e-04, -3.26311681e-04,\n",
       "        -7.22638506e-04, -9.29559290e-04,  1.61592543e-04,\n",
       "        -9.34468291e-04, -2.92267971e-04,  4.84284334e-04,\n",
       "         7.59521325e-04,  1.56013455e-04,  2.73793150e-04,\n",
       "        -5.35207160e-04,  9.40798607e-04, -8.21229361e-04,\n",
       "         6.05106179e-04, -1.07030896e-03,  2.65165087e-04,\n",
       "        -6.48053654e-04, -5.53084363e-04, -4.04815655e-04,\n",
       "         7.36355316e-04,  7.55000103e-04, -6.47740730e-04,\n",
       "         1.00694131e-04, -1.10622111e-03,  1.01690181e-03,\n",
       "         1.20537647e-03, -3.57073877e-04,  1.04195751e-04,\n",
       "         9.72202979e-05,  7.13266258e-04, -1.12058315e-03,\n",
       "         7.61010087e-05,  8.94455996e-04,  2.90572178e-04,\n",
       "         1.46442253e-04, -1.21382240e-03,  1.10447488e-03,\n",
       "        -8.15641601e-04, -3.89631983e-04,  4.54920199e-04,\n",
       "        -1.00602549e-04,  1.83761891e-04,  2.32030172e-04,\n",
       "        -8.89179588e-04, -1.26625143e-03,  1.17715949e-03,\n",
       "         8.07038334e-04, -9.00120765e-04,  4.43161785e-04,\n",
       "         2.68312488e-05,  6.18977298e-04, -9.27075918e-04,\n",
       "         5.24342991e-04,  5.66072122e-04,  1.29653246e-03,\n",
       "        -5.82518231e-04, -1.80893738e-04, -9.52776230e-04,\n",
       "        -1.26273825e-03, -1.18232518e-03, -1.33171212e-04,\n",
       "        -8.46782525e-04,  6.31475064e-04, -8.02607567e-04,\n",
       "         3.28018941e-04,  9.62813720e-05, -4.41686716e-04,\n",
       "        -1.27503023e-04,  1.29936531e-03,  1.19087065e-03,\n",
       "        -5.80967404e-04,  1.18268572e-03, -7.34604604e-04,\n",
       "         7.72255473e-04, -4.03283630e-04,  4.46842663e-04,\n",
       "         3.92868038e-04,  8.98497470e-04, -3.09099443e-04,\n",
       "         1.14258286e-03,  9.88206826e-04, -1.24318304e-03,\n",
       "        -1.04273565e-03, -9.94517817e-04,  3.80632468e-04,\n",
       "        -3.63896135e-04, -9.02281317e-04, -1.05836766e-03,\n",
       "         1.08192442e-03,  2.59178225e-04, -1.21458562e-03,\n",
       "        -6.24051609e-04,  4.08429507e-04, -6.13698736e-04,\n",
       "         6.87609718e-04, -5.51229343e-04,  3.43983847e-04,\n",
       "        -1.04761554e-03,  8.08578916e-04,  6.27459493e-04,\n",
       "         1.02499034e-04,  3.92375980e-04, -1.13639096e-03,\n",
       "         2.77364772e-04, -1.13742739e-04, -1.21342298e-03,\n",
       "        -1.22762274e-03, -1.83687240e-04,  5.77136583e-04,\n",
       "         4.82300908e-04, -8.46184033e-04, -8.94930679e-04,\n",
       "        -6.50965143e-04, -2.97766179e-04, -9.44047875e-04,\n",
       "        -1.25043199e-03, -3.57243378e-04, -1.08891155e-03,\n",
       "        -7.86311924e-04, -7.38402188e-04, -3.05226218e-04,\n",
       "        -2.22265255e-04, -1.16627582e-03, -9.57290977e-05,\n",
       "         1.06152426e-03,  1.00135803e-03, -9.38296318e-04,\n",
       "        -4.77451977e-04,  4.06061445e-04, -1.24618784e-03,\n",
       "         1.92244697e-04,  8.49539880e-04,  7.48231716e-04,\n",
       "        -1.14102371e-03, -5.88169787e-04, -1.05991680e-03,\n",
       "         5.98390261e-06,  1.20620232e-03,  7.77774199e-04,\n",
       "         6.59805723e-04,  6.58992503e-04, -4.22254816e-04,\n",
       "         1.24377385e-03, -9.57867771e-04, -9.46665008e-04,\n",
       "        -2.94972531e-04, -1.01375081e-04, -4.18763462e-04,\n",
       "        -7.71596096e-05,  9.75107134e-04, -9.08227303e-05,\n",
       "        -2.11580817e-04,  3.57343670e-04, -1.08842459e-03,\n",
       "         1.02289114e-03,  1.11147191e-03, -1.24792801e-03,\n",
       "         3.18524282e-04,  1.28970982e-03, -9.98151489e-04,\n",
       "        -9.07150854e-04, -1.00735901e-03,  1.09321915e-03,\n",
       "        -8.87156129e-05,  1.19067822e-03, -1.06226839e-03,\n",
       "         4.87380807e-04,  3.43104504e-04,  9.67074520e-05,\n",
       "         3.03082779e-04, -9.72538255e-04, -1.21853827e-03,\n",
       "         3.06585483e-04,  8.00580077e-04,  1.03980314e-03,\n",
       "         7.46861275e-04, -1.01215672e-04,  1.08153187e-03,\n",
       "        -1.21566595e-03,  4.43506840e-04,  3.47335190e-05,\n",
       "         5.02245326e-04,  9.61690501e-04, -8.75672791e-04,\n",
       "         7.27145933e-04, -1.23987300e-03, -1.04747247e-04,\n",
       "        -1.13134598e-03, -6.63889747e-04,  1.20953470e-03,\n",
       "        -2.41961170e-04,  3.79482604e-04,  1.18115626e-03,\n",
       "         1.16381934e-03, -1.06880662e-03, -3.92228365e-04,\n",
       "         1.28731842e-03,  6.64639461e-04, -2.06782177e-04,\n",
       "        -1.13177358e-03,  3.85614112e-04, -8.69257550e-04,\n",
       "         1.05888955e-03, -5.80382068e-04, -1.39109019e-04,\n",
       "         1.31037086e-04, -2.48846281e-05,  1.49502259e-04,\n",
       "         7.96075619e-04, -2.63952347e-06, -4.22651734e-04,\n",
       "        -1.96709458e-04,  7.67877500e-04,  1.97148722e-04,\n",
       "        -9.43049454e-05,  1.21526653e-03, -6.40792132e-04,\n",
       "        -1.09167922e-04,  1.19471503e-03,  8.78831663e-04,\n",
       "         1.95684377e-04, -1.15658343e-03,  1.49576299e-04,\n",
       "        -2.97949970e-04,  1.21982256e-03,  1.57542687e-04,\n",
       "         1.94018707e-04,  3.13334633e-04, -2.39063360e-04,\n",
       "        -6.50994014e-04,  3.02642584e-05, -2.62263085e-04,\n",
       "         8.59496475e-04,  1.16407860e-03, -8.78586434e-05,\n",
       "         3.87632143e-04, -7.95267522e-04,  2.21266251e-04,\n",
       "        -9.01853200e-04, -1.13203470e-03, -7.68255617e-04,\n",
       "        -1.16620772e-03,  9.47603490e-04, -7.51566608e-04,\n",
       "         1.07765000e-03, -9.43169929e-04,  4.45530604e-04,\n",
       "         1.25976547e-03, -1.01373019e-03, -1.29492942e-03,\n",
       "        -5.63690905e-04, -3.49365961e-04, -3.53241339e-05,\n",
       "        -1.14994159e-03, -1.12207781e-03,  3.64610780e-04,\n",
       "        -1.06854260e-03, -1.18090317e-03, -3.04748159e-04,\n",
       "        -1.12393324e-03, -9.18834645e-04, -1.09389983e-03,\n",
       "        -3.92355323e-05, -5.94309706e-04,  8.62913381e-04,\n",
       "         1.98849011e-04, -4.35087975e-04,  7.95439060e-04,\n",
       "        -7.82979827e-04, -6.06272079e-04, -9.38477751e-04,\n",
       "        -5.64658840e-04, -2.35589847e-04,  8.45005561e-04,\n",
       "        -3.60728242e-04,  6.40490558e-04,  8.99015926e-04,\n",
       "        -9.71836678e-04,  5.94381534e-04,  7.97783665e-04,\n",
       "        -3.84697225e-04,  8.62633053e-04,  7.97640532e-04,\n",
       "        -8.38995446e-04, -8.80800944e-04,  3.30593582e-04,\n",
       "        -2.11434744e-04, -7.89730169e-04,  1.23687612e-03,\n",
       "        -6.67987857e-04, -8.53398058e-04, -1.56100523e-05,\n",
       "        -3.51748429e-04,  5.78646250e-05, -4.60606534e-04,\n",
       "        -5.46003394e-05, -9.22676772e-05,  1.07138105e-04,\n",
       "         1.06703350e-03, -7.46967096e-04, -2.16084372e-04,\n",
       "         7.25469727e-04,  1.06355734e-03, -5.78519888e-04,\n",
       "         1.16997829e-03,  1.07469596e-03, -5.77502884e-04,\n",
       "         3.94668132e-05,  5.56574378e-04, -5.11239574e-04,\n",
       "        -7.23953824e-04, -8.47958669e-04, -8.73357058e-05,\n",
       "        -3.85314524e-05,  5.81130851e-04, -3.22142470e-04,\n",
       "        -2.24751420e-05,  3.20556719e-04,  6.33801974e-04,\n",
       "        -4.01151692e-06, -8.25443945e-04, -1.20583421e-03,\n",
       "         3.47103924e-06,  8.67434137e-04,  1.90888371e-04,\n",
       "        -1.16751587e-03, -1.03367248e-03,  8.53112258e-04,\n",
       "        -4.92927153e-04,  8.14452127e-04, -8.69926065e-04,\n",
       "         1.10412261e-03, -8.48479744e-04,  4.28127591e-04,\n",
       "        -1.37628362e-04, -8.83792702e-04, -4.28072497e-04,\n",
       "        -1.51225526e-04, -7.12361943e-04, -1.57727554e-04,\n",
       "        -9.84806451e-04,  3.44617147e-04,  1.18100888e-03,\n",
       "        -3.09537776e-04, -1.27149746e-04,  4.57494985e-04,\n",
       "         1.12826657e-03, -7.71074556e-04, -8.96820042e-04,\n",
       "        -3.81899066e-04,  1.19110628e-03,  1.12795271e-04,\n",
       "        -1.13000011e-03, -1.88408725e-04,  1.23430544e-03,\n",
       "        -9.83006204e-04, -6.97669107e-04,  1.21309410e-03,\n",
       "        -1.16845395e-03,  4.98165085e-04,  8.66459086e-05,\n",
       "         8.67278781e-04,  1.08238973e-03, -3.71195987e-04,\n",
       "        -5.19832422e-04,  1.15858298e-03,  2.72089295e-04,\n",
       "         8.13664228e-04, -1.22991076e-03,  1.24871405e-03]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Word2VecEmbeddings():\n",
    "    #required to train on all data and queries, because use keys to find embeddings\n",
    "    def train(self, data,epochs=100):\n",
    "        data = [[x] for x in data]\n",
    "        self.model = Word2Vec(data, \n",
    "                 min_count = 1, vector_size = 768,\n",
    "                                             window = 5, sg = 1)\n",
    "        for epoch in range(epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"epoch %s\" % epoch)\n",
    "            self.model.train([[\"shell\"]], total_examples=self.model.corpus_count, epochs=100)\n",
    "            \n",
    "            \n",
    "    def getEmbeddings(self,q):\n",
    "        values = []\n",
    "        for val in q:\n",
    "            values.append(self.model.wv.get_vector(val))\n",
    "        return np.array(values)\n",
    "    \n",
    "    def getKeys(self):\n",
    "        words = list(self.model.wv.index_to_key)\n",
    "        return words\n",
    "\n",
    "data = list(pd.read_csv(\"SEC-CompanyTicker.csv\",index_col=0).companyName[:100])\n",
    "train = Word2VecEmbeddings()\n",
    "FullKeys = data + [\"shell\"]\n",
    "train.train(FullKeys)\n",
    "xq = train.getEmbeddings([\"shell\"])\n",
    "xb = train.getEmbeddings(data)\n",
    "xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8578d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03503197, -0.02060164, -0.01537573, ..., -0.01162699,\n",
       "        -0.00087646,  0.00465802]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "key = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "# Initialize OpenAI client (replace '...' with your API key)\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "\n",
    "class OpenAIEmbeddings:\n",
    "    def __init__(self,api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "    \n",
    "    def getEmbeddings(self, text_list):\n",
    "        data = self.client.embeddings.create(input=text_list, model='text-embedding-ada-002').data\n",
    "        embeddings = [embedding.embedding for embedding in data]\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    \n",
    "e = OpenAIEmbeddings(key)\n",
    "e.getEmbeddings([\"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dca62df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla, Inc.\n",
      "Salesforce, Inc. \n",
      "Qualcomm Inc/De\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "    \n",
    "class Faiss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def faiss(self,xb):\n",
    "        d = xb[0].size\n",
    "        M = 32\n",
    "        index = faiss.IndexHNSWFlat(d, M)            \n",
    "        index.hnsw.efConstruction = 40         # Setting the value for efConstruction.\n",
    "        index.hnsw.efSearch = 16               # Setting the value for efSearch.\n",
    "        index.add(xb)\n",
    "        return index\n",
    "    \n",
    "    def query(self,index,xq,k=3):\n",
    "        D, I = index.search(xq, k)   \n",
    "        return D, I\n",
    "\n",
    "xq = train.getEmbeddings([\"shell\"])\n",
    "xb = train.getEmbeddings(data)\n",
    "index = Faiss().faiss(xb)\n",
    "D,I = Faiss().query(index,xq)\n",
    "I = I[0]\n",
    "print(data[I[0]] + \"\\n\" + data[I[1]], \"\\n\" + data[I[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7c815e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "48410b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[x] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "76d5a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84328eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for epoch in range(epochs):\n",
    "            self.model.train(data, total_examples=self.model.corpus_count, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
